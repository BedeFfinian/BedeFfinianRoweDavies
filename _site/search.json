[
  {
    "objectID": "index_Desktop.html",
    "href": "index_Desktop.html",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "",
    "text": "Bede Ffinian Rowe Davies Post-Doctoral Researcher bedeffinian@gmail.com Laboratoire ISOMer, UFR Sciences et Techniques Nantes University, France."
  },
  {
    "objectID": "index_Desktop.html#projects",
    "href": "index_Desktop.html#projects",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "Projects",
    "text": "Projects\nI have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation."
  },
  {
    "objectID": "posts1/Baited.html",
    "href": "posts1/Baited.html",
    "title": "Ecosystem Approach to Fisheries Management works— How switching from Mbile to Static Fishing Gear Improves Populations of Fished and Non-Fished Species Inside a Marine Protected Area.",
    "section": "",
    "text": "Davies et al., 2021\nDesignated using a Statutory Instrument in 2008, Lyme Bay marine- protected area (MPA) is the UK’s first and largest example of an ambitious, whole-site approach to management, to recover and protect reef biodiversity. The whole-site approach applies consistent management, in this case excluding bottom towed fishing, across the full 206 km2 extent of the MPA, thus protecting a mosaic of reef-associated habitats from regular damage, while still allowing less destructive fishing methods, such as static gear, rod and line, and diving.2. To assess the effectiveness of this management strategy for mobile taxa and the sustainability for those taxa that continue to be targeted, Exploited and Non- Exploited species’ populations were compared inside the MPA, relative to open control sites spanning 11 of the 12 years of designation. baited remote underwater video systems (BRUVs) were deployed annually to assess mobile benthic and demersal fauna.3. Overall, the number of taxa significantly increased in the MPA relative to the open controls while total abundance increased in both treatments.4. Exploited fish showed increases in number of taxa (430%) and total abundance (370%) inside the MPA over 11 years.5. Likewise, but to a lesser degree in the open controls, number of taxa of commercially Exploited fish increased over time, potentially showing ‘spillover’ effects from the MPA.6. Non-Exploited fish did not show such changes. Regardless of constituting the majority of the fishery value, highly valuable Exploited invertebrates showed no significant changes over time.7. Synthesis and applications. The Lyme Bay marine- protected area shows importance of protecting a whole site, comprising mosaics of different benthic habitats, through protection of sessile organisms that contribute to essential fish habitats."
  },
  {
    "objectID": "posts1/SAC.html",
    "href": "posts1/SAC.html",
    "title": "Ecosystem benefits of adopting a whole‐site approach to MPA management.",
    "section": "",
    "text": "Davies et al., 2022\n\nAbstract\nGlobally, nations are designating Marine Protected Areas to recover and protect habitats and species. With targets to protect 30% of marine areas by 2030, effectiveness of MPAs to protect designated space is important. In Lyme Bay (South West UK) two co-located MPAs have each adopted different management styles to exclude mobile demersal fishing; a Special Area of Conservation (SAC) protecting the known extent of sensitive reef habitat and an area including a mosaic of reef and sedimentary habitats where the whole-site is protected from mobile demersal fishing under a Statutory Instrument (SI). Underwater videography, both towed (individuals m\\(^{-2}\\)) and baited (MaxN), was used to enumerate change over time of reef species (Number of Taxa, Total Abundance, Functional Richness and Functional Redundancy) in the MPAs and nearby control areas (2008-2019). Total abundance and functional redundancy of sessile taxa and functional richness of mobile taxa increased, while the number of sessile or mobile taxa, functional richness of sessile taxa, total abundance of mobile taxa or functional redundancy of mobile taxa did not differ from nearby control sites. Over time, both management styles did result in increases in sessile and sedentary taxa diversity relative to open controls, with increases in total abundance of 15% and 95% in the ‘feature based’ and whole-site MPAs respectively alongside increases in the number of sessile taxa of 44% over time in the ‘feature based’ MPA. However, the mobile taxa in the whole-site MPA showed levels of functional redundancy 7% higher than the ‘feature based’ MPA, indicative of a higher community resilience inside the whole-site MPA to perturbations, such as storms or biological invasions. Increases seen in the diversity of sessile taxa we expected only in areas where mobile demersal fishing was excluded (~46.8% of its’ areas). Therefore, if the whole ‘feature based’ MPA was consistently protected, we expected to see similar levels of increase in functional extent of reef. While the ‘feature based’ MPA showed similar results over time to that of the ‘whole site’, the ‘whole site’ showed higher levels of diversity, both taxonomical and functional."
  },
  {
    "objectID": "posts1/Acoustics.html",
    "href": "posts1/Acoustics.html",
    "title": "Acoustic Complexity Index to Assess Benthic Biodiversity of a Partially Protected Area in the Southwest of the UK",
    "section": "",
    "text": "Davies et al., 2020\nThe soundscape of the marine environment is a relatively understudied area of ecology that has the potential to provide large amounts of information on biodiversity, reproductive behaviour, habitat selection, spawning and predator–prey interactions. Biodiversity is often visually assessed and used as a proxy for ecosystem health. Visual assessment using divers or remote video methods can be expensive, and limited to times of good weather and water visibility. Previous studies have concluded that acoustic measures, such as the Acoustic Complexity Index (ACI), correlate with visual biodiversity estimates and offer an alternative to assess ecosystem health. Here, the ACI measured over 5 years in a Marine Protected Area (MPA) in the UK, Lyme Bay, was analysed alongside another monitoring method, Baited Remote Underwater Video Systems (BRUVs). Two treatments were sampled annually in the summer from 2014 until 2018 with sites inside the MPA, as well as Open Control sites outside of the MPA. Year by year correlations, which have been used elsewhere to test ACI, showed significant correlations with Number of Species and ACI. However, the sign of these correlations changed almost yearly, showing that more in-depth analyses are needed. Multivariate analysis of the benthic assemblage composition (from BRUVs) was carried out by Permutational Multivariate Analysis of Variance (PERMANOVA) using Distance Matrices. Although not consistently correlating with univariate measures, the ACI was significantly interacting with the changing benthic assemblage composition, as it changed over time and protection (Inside vs Outside the MPA). ACI showed potential to allude to shifting benthic communities, yet with no consistency when used alongside univariate measures of diversity. Although it is not without its own disadvantages, and thus should be developed further before implementation, the ACI could potentially reflect more complex changes to the benthos than simply the overall diversity."
  },
  {
    "objectID": "Tutorials.html",
    "href": "Tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Introduction\n\n\nR\n\n\n\nAn Introduction to R for Research Scientists: From Installation to Reading and Writing Data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\n\nAn Introduction to Data Manipulation in R for Research Scientists: From Data Creation to Data Wrangling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\n\nAn Introduction to Data Visualisation in R for Research Scientists: From Base Scatter Plots to Facetting GGPlots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\npatchwork\n\n\n\nA Introduction to Combining Plots in R for Research Scientists: From Facetting to Patchworks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nsf\n\n\nMapping\n\n\nGIS\n\n\n\nA Rapid Introduction to Mapping in R for Research Scientists: From Reading in Shape files and Rasters to plotting Shape files alongside Rasters in different Projections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nleaflet\n\n\nMapping\n\n\nGIS\n\n\n\nA Rapid Introduction to Interactive Mapping in R for Research Scientists: All/some things Leaflet.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tutorials.html#r-basics-tutorials-with-an-ecologist-bias",
    "href": "Tutorials.html#r-basics-tutorials-with-an-ecologist-bias",
    "title": "Tutorials",
    "section": "",
    "text": "Introduction\n\n\nR\n\n\n\nAn Introduction to R for Research Scientists: From Installation to Reading and Writing Data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\n\nAn Introduction to Data Manipulation in R for Research Scientists: From Data Creation to Data Wrangling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\n\nAn Introduction to Data Visualisation in R for Research Scientists: From Base Scatter Plots to Facetting GGPlots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\npatchwork\n\n\n\nA Introduction to Combining Plots in R for Research Scientists: From Facetting to Patchworks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nsf\n\n\nMapping\n\n\nGIS\n\n\n\nA Rapid Introduction to Mapping in R for Research Scientists: From Reading in Shape files and Rasters to plotting Shape files alongside Rasters in different Projections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nleaflet\n\n\nMapping\n\n\nGIS\n\n\n\nA Rapid Introduction to Interactive Mapping in R for Research Scientists: All/some things Leaflet.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tutorials.html#statistics-tutorials-with-an-ecologist-bias",
    "href": "Tutorials.html#statistics-tutorials-with-an-ecologist-bias",
    "title": "Tutorials",
    "section": "Statistics Tutorials with an Ecologist Bias:",
    "text": "Statistics Tutorials with an Ecologist Bias:\n\n\n\n\n\n\n\n\n\n\nIntroduction GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\n\nAn Introduction to General Linear Models (GLMs) in R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nGaussian\n\n\n\nExamples of Gaussian GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nPoisson\n\n\n\nExamples of Poisson GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nBinomial\n\n\n\nExamples of Binomial GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGamma GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nGamma\n\n\n\nExamples of Gamma GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeta GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nBeta\n\n\n\nExamples of Beta GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Inflated GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nZero Inflation\n\n\n\nExamples of Zero Inflated GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommon GLM Problems\n\n\n\n\n\n\nProblems\n\n\nR\n\n\nGLMs\n\n\n\nUnderstanding and Solving Common Problems of General Linear Models (GLMs) in R.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Categories\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBRUVs\n\n\nTUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFunctional Diversity\n\n\nFirst Author\n\n\n\nBede F. R. Davies, Luke Holmes, Anthony Bicknell, Martin J. Attrill & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcoustic\n\n\nBRUVs\n\n\nEcology\n\n\nPAM\n\n\nMarine Diversity\n\n\nFirst Author\n\n\n\nBede F.R. Davies, Martin J. Attrill, Luke Holmes, Adam Rees, Matthew J. Witt & Emma V. Sheehan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBRUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFirst Author\n\n\n\nBede F.R. Davies, Luke Holmes, Adam Rees, Martin J. Attrill, Amy Y. Cartwright & Emma V. Sheehan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBede F. R. Davies, Luke Holmes, Martin J. Attrill & Emma V. Sheehan\n\n\n\nMarine Management\n\n\nBRUVs\n\n\nTUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFunctional Diversity\n\n\nFirst Author\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBede F. R. Davies, Pierre Gernez, Andréa Geraud, Simon Oiry, Philippe Rosa, Maria Laura Zoffoli & Laurent Barillé\n\n\n\nRemote Sensing\n\n\nSpectral Library\n\n\nSatellite\n\n\nMachine Learning\n\n\nFirst Author\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Who am I and What have I done?",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#first-author-publications",
    "href": "Publications.html#first-author-publications",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Categories\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBRUVs\n\n\nTUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFunctional Diversity\n\n\nFirst Author\n\n\n\nBede F. R. Davies, Luke Holmes, Anthony Bicknell, Martin J. Attrill & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcoustic\n\n\nBRUVs\n\n\nEcology\n\n\nPAM\n\n\nMarine Diversity\n\n\nFirst Author\n\n\n\nBede F.R. Davies, Martin J. Attrill, Luke Holmes, Adam Rees, Matthew J. Witt & Emma V. Sheehan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBRUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFirst Author\n\n\n\nBede F.R. Davies, Luke Holmes, Adam Rees, Martin J. Attrill, Amy Y. Cartwright & Emma V. Sheehan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBede F. R. Davies, Luke Holmes, Martin J. Attrill & Emma V. Sheehan\n\n\n\nMarine Management\n\n\nBRUVs\n\n\nTUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFunctional Diversity\n\n\nFirst Author\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBede F. R. Davies, Pierre Gernez, Andréa Geraud, Simon Oiry, Philippe Rosa, Maria Laura Zoffoli & Laurent Barillé\n\n\n\nRemote Sensing\n\n\nSpectral Library\n\n\nSatellite\n\n\nMachine Learning\n\n\nFirst Author\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Who am I and What have I done?",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#co-author-publications",
    "href": "Publications.html#co-author-publications",
    "title": "Publications",
    "section": "Co-Author Publications:",
    "text": "Co-Author Publications:\n\n\n\n\n\n\n\n\n\n\nLessons from Lyme Bay (UK) to inform policy, management, and monitoring of Marine Protected Areas\n\n\n\n\n\n\nMPA\n\n\nEcology\n\n\nMarine Diversity\n\n\nClimatic Events\n\n\nBRUVs\n\n\nSocio-Economic\n\n\nTUVs\n\n\nCo-Author\n\n\n\nChloe Renn, Sian Rees, Adam Rees, Bede F R Davies, Amy Y Cartwright, Sam Fanshawe, Martin J Attrill, Luke A Holmes & Emma V Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping intertidal oyster farms using unmanned aerial vehicles (UAV) high-resolution multispectral data\n\n\n\n\n\n\nDrones\n\n\nEcology\n\n\nOyster\n\n\nMarine Diversity\n\n\nRemote Sensing\n\n\nCo-Author\n\n\n\nAlejandro Román, Hermansyah Prasyad, Simon Oiry, Bede F.R. Davies, Guillaume Brunier & Laurent Barillé́\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReaching New Heights in Plastic Pollution—Preliminary Findings of Microplastics on Mount Everest\n\n\n\n\n\n\nPlastics\n\n\nPollution\n\n\nTerrestrial Pollution\n\n\nCo-Author\n\n\n\nImogen E. Napper, Bede F.R. Davies, Heather Clifford, Sandra Elvin, Heather J.Koldewey, Paul A.Mayewski, Kimberley R.Miner, Mariusz Potocki, Aurora C. Elmore Ananta P. Gajurel & Richard C. Thompson.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote sensing in seagrass ecology: coupled dynamicsbetween migratory herbivorous birds and intertidalmeadows observed by satellite during four decades\n\n\n\n\n\n\nSatellite\n\n\nEcology\n\n\nSeagrass\n\n\nMarine Diversity\n\n\nClimatic Events\n\n\nRemote Sensing\n\n\nCo-Author\n\n\n\nMaria Laura Zoffoli, Pierre Gernez, Simon Oiry, Laurent Godet, Śebastien Dalloyau, Bede Ffinian Rowe Davies & Laurent Barilĺ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRewilding of Protected Areas Enhances Resilience of Marine Ecosystems to Extreme Climatic Events\n\n\n\n\n\n\nStorms\n\n\nEcology\n\n\nMarine Diversity\n\n\nClimatic Events\n\n\nTUVs\n\n\nCo-Author\n\n\n\nEmma V. Sheehan, Luke A. Holmes, Bede F. R. Davies, Amy Cartwright, Adam Rees & Martin J. Attrill.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Abundance and Characteristics of Microplastics in Surface Water in the Transboundary Ganges River\n\n\n\n\n\n\nPlastics\n\n\nPollution\n\n\nFreshwater Pollution\n\n\nCo-Author\n\n\n\nImogen E. Napper, Anju Baroth, Aaron C. Barrett, Sunanda Bhola. Gawsia W. Chowdhuryd, Bede F.R. Davies, Emily M. Duncan, Sumit Kumar, Sarah E. Nelms, Md Nazmul Hasan Niloy, Bushra Nishat, Taylor Maddalene, Richard C. Thompson & Heather Koldewey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Distribution and Characterisation of Microplastics in Air, Surface Water and Sediment within a Major River System.\n\n\n\n\n\n\nPlastics\n\n\nPollution\n\n\nFreshwater Pollution\n\n\nCo-Author\n\n\n\nImogen E. Napper, Anju Baroth, Aaron C. Barrett, Sunanda Bhola, Gawsia W. Chowdhury, Bede F.R. Davies, Emily M. Duncan, Sumit Kumar, Sarah E. Nelms, Md. Nazmul Hasan Niloy, Bushra Nishat, Taylor Maddalene, Natalie Smith, Richard C. Thompson & Heather Koldewey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe restoration potential of offshore mussel farming on degraded seabed habitat\n\n\n\n\n\n\nEcology\n\n\nMarine Diversity\n\n\nAquaculture\n\n\nTUVs\n\n\nCo-Author\n\n\n\nDanielle Bridger, Martin J. Attrill, Bede F. R. Davies, Luke A. Holmes, Amy Cartwright, Siân E. Rees, Llucia Mascorda Cabre & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Who am I and What have I done?",
      "Publications"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ZeroInflatedGLMs.html",
    "href": "StatisticsTutorials/ZeroInflatedGLMs.html",
    "title": "Zero Inflated GLMs",
    "section": "",
    "text": "So in the common issues with GLMs tutorial we talked about zero inflated data and how this type of data will cause a issues, often diagnosed by residuals plots. Here we will have some examples of discovering zero inflated data then how to model more appropriately using zero inflated distributions often called ZIP models. But first a little bit of theory and thought.\n\n\nAs we have mentioned throughout the tutorials, the cause of data allows us best to model it and discover or describe natural phenomena. This structure data allows us to effectively create models that capture the causal links between factors. Likewise, with high levels of zeros we want to know why there are lots of zeros? Are the Zeros True Zeros? or are they false zeros?\n\nA false zero is often called non-detection, as with the experimental fish example, if we have a net with holes bigger than a certain size of fish we will get lots of zeros for that fish, even if they were in the net but just swam through before we counted the sample.\nA true zero would be there were non of that fish there. This is almost impossible to know for sure but if we suspect the zeros are true zeros (we didn’t use a net, we used a bucket and nothing could escape if it was there) then we can model those zeros as well as the counts we do get in a very similar way.\n\nWe need to decide if we want to model the process of the zeros occuring or not. This will be case specific.\n\n\n\nWhat better way to explore poisson models than with fish data. We will use the remotes package to install a package from github called stats4nr. Within this package there is a fishing data set of fish caught in counts with livebait, whether they came in a camper, number of persons in the group and number of children in the group.\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"mbrussell/stats4nr\")\n\nlibrary(stats4nr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\ndata(fishing)\n\nsummary(fishing)\n\n     nofish         livebait         camper         persons     \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :1.000  \n 1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.000   1st Qu.:2.000  \n Median :0.000   Median :1.000   Median :1.000   Median :2.000  \n Mean   :0.296   Mean   :0.864   Mean   :0.588   Mean   :2.528  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:4.000  \n Max.   :1.000   Max.   :1.000   Max.   :1.000   Max.   :4.000  \n     child           count        \n Min.   :0.000   Min.   :  0.000  \n 1st Qu.:0.000   1st Qu.:  0.000  \n Median :0.000   Median :  0.000  \n Mean   :0.684   Mean   :  3.296  \n 3rd Qu.:1.000   3rd Qu.:  2.000  \n Max.   :3.000   Max.   :149.000  \n\nglm1&lt;-glm(count~persons + child + camper,family = \"poisson\", data = fishing)\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs expected this isn’t very good at all, with much of the residuals all around zero, maybe there are too many zeros? Lets look at the distribution of count data.\n\nggplot(fishing)+\n  geom_bar(aes(count),fill=\"darkcyan\")+\n  labs(x=\"Count of Fish Caught\", y=\"Count\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWell there are a lot of zeros there! But remember poisson will break down when the variance isn’t proportional to the expected value, so we cal look at the variance and the mean and see how similar they are.\n\nmean(fishing$count)\n\n[1] 3.296\n\nvar(fishing$count)\n\n[1] 135.3739\n\n\nDefinitely not. This shows extreme over dispersion. We have discussed using a Negative Binomial model for this situation. But as the name of tutorial shows we are looking at Zero-Inflated models. But lets try the negative binomial anyway..\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nglm2&lt;- glm.nb(count ~ persons + child + camper, \n              data = fishing)\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nIt does go some of the way, but there are still issues around the zero values in the Homogeneity of variance plot! Okay lets get on task then, we will use the zeroinfl() function from the pscl package. We will need to extract the fitted and residuals to plot ourselves as the check_model() function doesn’t support this glm type yet.\n\n#install.packages(\"pscl\")\nlibrary(pscl)\n\nClasses and Methods for R developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University\nSimon Jackman\nhurdle and zeroinfl functions by Achim Zeileis\n\nglm3 &lt;- zeroinfl(count ~ persons + child + camper|  persons+child+camper,\n                 data = fishing, dist = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm3),\n                  Residuals=resid(glm3))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nThis is definitely still not great.\n\n\n\nFor a full example we will actually make up some data. Lets pretend we have count of fish inside and outside of a Marine Protected Area (MPA). We will also pretend our MPA is unbelieveablly good at providing protection to the species of fish we are looking at. But we have some unknown situation that means that sometimes we have high levels of zero records.\nWe will first make our factors, Time and MPA, then we will create poisson data where lambda is influenced by both the MPA and Time factors. Lets visualise each factor so we can see how we have created the data and what our data look like. Don’t forget we still need to add in the zero-inflation. We will create lambda from a linear equation using b0, b1, b2 and b3 as the intercept and the effects of Time, MPA, and interaction of Time and MPA.\n\nlibrary(patchwork)\n\nn &lt;- 5000\n\nMPA &lt;- sample(c(0,1), size = n, replace = TRUE)\n\nTime &lt;- c(1:100)\n\nb0&lt;-log(2) # Intercept\n\nb1&lt;-log(1.1) # Effect of Time\n\nb2&lt;-log(1.2) # Effect of MPA\n\nb3&lt;-log(1.3) # Effect of interaction between Time and MPA\n\nlambda&lt;-exp(b0 + b1 * log(Time) + b2 * (MPA==1) + b3 * (MPA==1) * log(Time))\n\ny_sim &lt;- rpois(n = n, lambda = lambda) \n\ndf&lt;-data.frame(MPA=as.factor(MPA),\n               Time=Time,\n               Count=y_sim)\n\np6&lt;-ggplot(df)+\n    geom_bar(aes(x=MPA),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"MPA\")\n\np7&lt;-ggplot(df)+\n    geom_bar(aes(x=Time),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"Time\")\n\np8&lt;-ggplot(df)+\n    geom_bar(aes(x=Count),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"Fish Count\")\n\np6+p7+p8\n\n\n\n\n\n\n\nggplot(df)+\n    geom_point(aes(x=Time,y=Count,colour=MPA))+\n    theme_classic()+\n    scale_colour_manual(values=c(\"darkgoldenrod\",\"darkcyan\"))+\n    labs(y=\"Fish Count\",x=\"Time\")\n\n\n\n\n\n\n\n\nNow lets create a high number of zeros, which are going to more likely to occur (with a probability of 0.1) if it is inside the MPA but not difference with time.\n\ndf1&lt;-df %&gt;% \n  mutate(Count_Zeros=Count*rbinom(MPA, size = 1, prob=0.7))\n  \n\nggplot(df1)+\n    geom_point(aes(x=Time,y=Count_Zeros,colour=MPA))+\n    theme_classic()+\n    scale_colour_manual(values=c(\"darkgoldenrod\",\"darkcyan\"))+\n    labs(y=\"Fish Count\",x=\"Time\")\n\n\n\n\n\n\n\n\nNow lets try model this.\n\nglm4 &lt;- glm(Count_Zeros ~ MPA*Time,\n                 data = df1, family = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm4),\n                  Residuals=resid(glm4))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nWell this model looks pretty bad, not surprisingly.\n\nglm5 &lt;- zeroinfl(Count_Zeros ~ MPA*Time|MPA,\n                 data = df1, dist = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm5),\n                  Residuals=resid(glm5))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nThis looks better, again not perfect but pretty good. A more comprehensive example will come when we do GLMMs.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Zero Inflated GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ZeroInflatedGLMs.html#cause-of-the-zeros",
    "href": "StatisticsTutorials/ZeroInflatedGLMs.html#cause-of-the-zeros",
    "title": "Zero Inflated GLMs",
    "section": "",
    "text": "As we have mentioned throughout the tutorials, the cause of data allows us best to model it and discover or describe natural phenomena. This structure data allows us to effectively create models that capture the causal links between factors. Likewise, with high levels of zeros we want to know why there are lots of zeros? Are the Zeros True Zeros? or are they false zeros?\n\nA false zero is often called non-detection, as with the experimental fish example, if we have a net with holes bigger than a certain size of fish we will get lots of zeros for that fish, even if they were in the net but just swam through before we counted the sample.\nA true zero would be there were non of that fish there. This is almost impossible to know for sure but if we suspect the zeros are true zeros (we didn’t use a net, we used a bucket and nothing could escape if it was there) then we can model those zeros as well as the counts we do get in a very similar way.\n\nWe need to decide if we want to model the process of the zeros occuring or not. This will be case specific.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Zero Inflated GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ZeroInflatedGLMs.html#zero-inflated-poisson-data",
    "href": "StatisticsTutorials/ZeroInflatedGLMs.html#zero-inflated-poisson-data",
    "title": "Zero Inflated GLMs",
    "section": "",
    "text": "What better way to explore poisson models than with fish data. We will use the remotes package to install a package from github called stats4nr. Within this package there is a fishing data set of fish caught in counts with livebait, whether they came in a camper, number of persons in the group and number of children in the group.\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"mbrussell/stats4nr\")\n\nlibrary(stats4nr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\ndata(fishing)\n\nsummary(fishing)\n\n     nofish         livebait         camper         persons     \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :1.000  \n 1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.000   1st Qu.:2.000  \n Median :0.000   Median :1.000   Median :1.000   Median :2.000  \n Mean   :0.296   Mean   :0.864   Mean   :0.588   Mean   :2.528  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:4.000  \n Max.   :1.000   Max.   :1.000   Max.   :1.000   Max.   :4.000  \n     child           count        \n Min.   :0.000   Min.   :  0.000  \n 1st Qu.:0.000   1st Qu.:  0.000  \n Median :0.000   Median :  0.000  \n Mean   :0.684   Mean   :  3.296  \n 3rd Qu.:1.000   3rd Qu.:  2.000  \n Max.   :3.000   Max.   :149.000  \n\nglm1&lt;-glm(count~persons + child + camper,family = \"poisson\", data = fishing)\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs expected this isn’t very good at all, with much of the residuals all around zero, maybe there are too many zeros? Lets look at the distribution of count data.\n\nggplot(fishing)+\n  geom_bar(aes(count),fill=\"darkcyan\")+\n  labs(x=\"Count of Fish Caught\", y=\"Count\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWell there are a lot of zeros there! But remember poisson will break down when the variance isn’t proportional to the expected value, so we cal look at the variance and the mean and see how similar they are.\n\nmean(fishing$count)\n\n[1] 3.296\n\nvar(fishing$count)\n\n[1] 135.3739\n\n\nDefinitely not. This shows extreme over dispersion. We have discussed using a Negative Binomial model for this situation. But as the name of tutorial shows we are looking at Zero-Inflated models. But lets try the negative binomial anyway..\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nglm2&lt;- glm.nb(count ~ persons + child + camper, \n              data = fishing)\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nIt does go some of the way, but there are still issues around the zero values in the Homogeneity of variance plot! Okay lets get on task then, we will use the zeroinfl() function from the pscl package. We will need to extract the fitted and residuals to plot ourselves as the check_model() function doesn’t support this glm type yet.\n\n#install.packages(\"pscl\")\nlibrary(pscl)\n\nClasses and Methods for R developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University\nSimon Jackman\nhurdle and zeroinfl functions by Achim Zeileis\n\nglm3 &lt;- zeroinfl(count ~ persons + child + camper|  persons+child+camper,\n                 data = fishing, dist = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm3),\n                  Residuals=resid(glm3))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nThis is definitely still not great.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Zero Inflated GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ZeroInflatedGLMs.html#zero-inflated-simulated-data",
    "href": "StatisticsTutorials/ZeroInflatedGLMs.html#zero-inflated-simulated-data",
    "title": "Zero Inflated GLMs",
    "section": "",
    "text": "For a full example we will actually make up some data. Lets pretend we have count of fish inside and outside of a Marine Protected Area (MPA). We will also pretend our MPA is unbelieveablly good at providing protection to the species of fish we are looking at. But we have some unknown situation that means that sometimes we have high levels of zero records.\nWe will first make our factors, Time and MPA, then we will create poisson data where lambda is influenced by both the MPA and Time factors. Lets visualise each factor so we can see how we have created the data and what our data look like. Don’t forget we still need to add in the zero-inflation. We will create lambda from a linear equation using b0, b1, b2 and b3 as the intercept and the effects of Time, MPA, and interaction of Time and MPA.\n\nlibrary(patchwork)\n\nn &lt;- 5000\n\nMPA &lt;- sample(c(0,1), size = n, replace = TRUE)\n\nTime &lt;- c(1:100)\n\nb0&lt;-log(2) # Intercept\n\nb1&lt;-log(1.1) # Effect of Time\n\nb2&lt;-log(1.2) # Effect of MPA\n\nb3&lt;-log(1.3) # Effect of interaction between Time and MPA\n\nlambda&lt;-exp(b0 + b1 * log(Time) + b2 * (MPA==1) + b3 * (MPA==1) * log(Time))\n\ny_sim &lt;- rpois(n = n, lambda = lambda) \n\ndf&lt;-data.frame(MPA=as.factor(MPA),\n               Time=Time,\n               Count=y_sim)\n\np6&lt;-ggplot(df)+\n    geom_bar(aes(x=MPA),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"MPA\")\n\np7&lt;-ggplot(df)+\n    geom_bar(aes(x=Time),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"Time\")\n\np8&lt;-ggplot(df)+\n    geom_bar(aes(x=Count),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"Fish Count\")\n\np6+p7+p8\n\n\n\n\n\n\n\nggplot(df)+\n    geom_point(aes(x=Time,y=Count,colour=MPA))+\n    theme_classic()+\n    scale_colour_manual(values=c(\"darkgoldenrod\",\"darkcyan\"))+\n    labs(y=\"Fish Count\",x=\"Time\")\n\n\n\n\n\n\n\n\nNow lets create a high number of zeros, which are going to more likely to occur (with a probability of 0.1) if it is inside the MPA but not difference with time.\n\ndf1&lt;-df %&gt;% \n  mutate(Count_Zeros=Count*rbinom(MPA, size = 1, prob=0.7))\n  \n\nggplot(df1)+\n    geom_point(aes(x=Time,y=Count_Zeros,colour=MPA))+\n    theme_classic()+\n    scale_colour_manual(values=c(\"darkgoldenrod\",\"darkcyan\"))+\n    labs(y=\"Fish Count\",x=\"Time\")\n\n\n\n\n\n\n\n\nNow lets try model this.\n\nglm4 &lt;- glm(Count_Zeros ~ MPA*Time,\n                 data = df1, family = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm4),\n                  Residuals=resid(glm4))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nWell this model looks pretty bad, not surprisingly.\n\nglm5 &lt;- zeroinfl(Count_Zeros ~ MPA*Time|MPA,\n                 data = df1, dist = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm5),\n                  Residuals=resid(glm5))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nThis looks better, again not perfect but pretty good. A more comprehensive example will come when we do GLMMs.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Zero Inflated GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GaussianGLMs.html",
    "href": "StatisticsTutorials/GaussianGLMs.html",
    "title": "Gaussian GLMs",
    "section": "",
    "text": "Linear Model? or General Linear Model with Gaussian Distribution? or ANOVA? or ANCOVA? There are many names for this type of model, they all effectively mean the same thing. I am going to stick to calling it a Gaussian GLM because then there isn’t a new name for every different test. If you don’t like that, use whatever term you like, but the code and interpretation will be the same.\n\n\nAs before lets use the Palmer penguins dataset and remove the NAs (as before NAs should never be remove without considering why there are NAs but here we will remove them for ease).\n\n#install.packages(\"palmerpenguins\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_noNAs&lt;-penguins%&gt;% \n  drop_na()\n\n\n\n\n\nSo now we will try prove the obvious\nDoes the flipper length of penguins change between species and between sexes?\n\n\nWhether we use an interaction or not depends on if our scientific thought believes the relationship of Species to flipper length is different between sexes (sexual dimorphism may not be consistent across species). We shall use an interaction here as we might expect some sexual dimorphism in some species while less, no or opposite sexual dimorphism in other species. If we had good reason to expect the same sexual dimorphism across these species we would not use an interaction term.\n\n\n\nAs you probably guessed from the title we will be using a Gaussian Distribution. However, flipper length would more technically be a Gamma distribution. It is Numeric, Continuous but cannot be Zero or Negative! Using a Gaussian distribution in this situation is okay as flipper length will not approach 0, so issues of modelling near the zero will not be a problem. This is due to our sampling and just the fact that you won’t ever be able to measure a penguins foot that is 1 mm. or probably less than 25 mm! Our lowest value is 174 in the data. One method could be to centre and scale the flipper_length_mm and model it with a Gamma distribution but this would be a lot of converting back and forth, especially when a Gaussian model will perform equally as well.\n\n\n\nThankfully both sex and species are already factors in the dataset so we don’t have any organising to do!\nSo lets apply our model. lm() is a function in base r that allows us to create a Gaussian GLM object. There is also a glm() function where we would need to define the distribution but lm() is easier to use for gaussian models. We will create a model object then we can inspect and use this model.\n\nlm2.1&lt;-lm(flipper_length_mm~species*sex,data=penguins_noNAs)\n\n\n\n\nWe could apply a linear model to almost all data but often it will not meet our assumptions.\nWe can now check visually the residuals from our model.\nBy using the base plot function in 4 we get 4 plots. The first two are the ones we are most interested in generally. The next two plots are less important generally but can be used to find out what is wrong if the first two plots are not as we want them.\nFor the Residuals vs Fitted plot we want the data to be evenly spread from right to left, meaning the difference between the model and the data (residuals) are not generally larger or smaller at higher or lower values of the model.\nThe next important plot is the qq plot, this is best if the points follow line of x=y, which is the dotted line behind the points.\n\nplot(lm2.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is annoying as we have to press enter in the console to see all the plots.\nWe will code the residuals and qqnorm plots. This is relatively simple code, there are helper functions such as part of the performance package but they are quite unstable from my experience.\n\n#install.packages(\"patchwork\")\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm2.1),\n                  Residuals=resid(lm2.1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs we only have factors in our model we don’t see a ‘cloud’ of points, but the residuals are evenly spread above and below 0 so this is good.\nAs the diagnostics are good we can look at the results\n\nsummary(lm2.1)\n\n\nCall:\nlm(formula = flipper_length_mm ~ species * sex, data = penguins_noNAs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7945  -3.4110   0.0882   3.4590  17.5890 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              187.7945     0.6619 283.721  &lt; 2e-16 ***\nspeciesChinstrap           3.9408     1.1742   3.356 0.000884 ***\nspeciesGentoo             24.9124     0.9947  25.044  &lt; 2e-16 ***\nsexmale                    4.6164     0.9361   4.932  1.3e-06 ***\nspeciesChinstrap:sexmale   3.5600     1.6606   2.144 0.032782 *  \nspeciesGentoo:sexmale      4.2176     1.3971   3.019 0.002737 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.655 on 327 degrees of freedom\nMultiple R-squared:  0.8396,    Adjusted R-squared:  0.8372 \nF-statistic: 342.4 on 5 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nI find the best way to interpret a model output is to plot the model results.\nFirst lets re-plot the raw data, boxplots are probably the best for categorical factors but we could also use a half eye distribution plot. Lets look at both.\nThe best way to do this (in my opinion) is using the patchwork package that can combine the plots\nWe create a plot for each, then plot them side by side. Patchwork is a very simple way of combining ggplots together.\nWe can re-use some of our code from the intro for appearance and colours\n\n#install.packages(\"ggdist\")\nlibrary(ggdist)\n\np_box&lt;-ggplot(penguins_noNAs)+\n  geom_boxplot(aes(x=species,\n                   y=flipper_length_mm,\n                   fill=sex))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\np_cloud&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,colour=sex))+ \n  stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\np_box+p_cloud\n\n\n\n\n\n\n\n\nNow we can also see what the model believes about our data\nThe model has estimated parameters of a linear model.\nIf we wanted to we could write out our model as this:\n\\[FlipperLength = Gaussian(y',\\sigma)\\]\n\\[y'=y\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} sex(female-male):species(Adelie-Chinstrap) \\\\\n+  \\\\\n\\beta_{2} sex(female-male):species(Adelie-Gentoo) \\\\\n+  \\\\\n\\beta_{3} sex(female-male) \\\\\n+  \\\\\n\\beta_{4} species(Adelie-Chinstrap) \\\\\n+  \\\\\n\\beta_{5} species(Adelie-Gentoo) + Intercept\n\\end{aligned}\n\\]\nAs the Gaussian distribution has no data restrictions for the mean value the link function is nothing. Seems silly to include here, and many people wouldn’t, but it will keep consistency for later when we do have a link function.\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just sex and species the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Flipper length in mm based on those species and sexes.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval)\n\nNewData_1&lt;-expand.grid(sex=c(\"female\",\"male\"),\n                     species=c(\"Adelie\",\"Chinstrap\",\"Gentoo\"))\n\nPred&lt;-predict(lm2.1,NewData_1,se.fit=TRUE)\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_point(aes(x=species,\n                 y=response,\n                 colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nLets look at both of these plots next to each other,\nFirst we save both the raw data boxplot as one object and the predicted plot as another then we plot them side by side\n\nPlot1&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,\n                    colour=sex))+ \n  ggdist::stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()+\n  theme(legend.position = \"none\")\n\nPlot2&lt;-ggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Modelled Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\nPlot1+Plot2\n\n\n\n\n\n\n\n\nAt first look this is quite good, but maybe some polishing is needed, mostly the y axis range\nThere are multiple ways to change this for example setting them both to the same range with scale_y_continuous()\n\nPlot1&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,colour=sex))+ \n  ggdist::stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()+\n  scale_y_continuous(limits=c(170,240))+\n  theme(legend.position = \"none\")\n\nPlot2&lt;-ggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Modelled Response Variable (Flipper Length (mm))\")+\n    scale_y_continuous(limits=c(170,240))+\n  theme_classic()\n\nPlot1+Plot2\n\n\n\n\n\n\n\n\nThis is better, although we could actually plot both the modelled and raw data on one plot.\nWe can use a position=position_jitterdodge() to have the raw data not all in one line above their species\n\nggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  geom_point(data=penguins_noNAs,aes(x=species,\n                                      y=flipper_length_mm,\n                                      colour=sex),\n                position=position_jitterdodge(jitter.width = 0.4,\n                                              dodge.width = 0.8),\n              alpha=0.3,\n             size=0.5)+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nOkay that is what we do with linear models of categorical factors.\nBut what if we want to see the relationship between flipper_length_mm and bill_length_mm.\n\n\nWe know there are species differences and sexual differences in flipper length.\nAs males always tend to be larger lets just assess species differences in their flipper to bill relationship.\nflipper_length_mm~species*bill_length_mm\nLets plot the raw data first\n\nggplot(penguins_noNAs)+\n  geom_point(aes(x=bill_length_mm,y=flipper_length_mm,colour=species))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Flipper Length (mm)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can see from the raw data that we will expect to find some interesting relationships\n\n\n\nNothing has changed from above, so lets stick to Gaussian\n\n\n\nThe fixed effects again need little to no prep so lets apply the model.\n\nlm3.1&lt;-lm(flipper_length_mm~species*bill_length_mm,data=penguins_noNAs)\n\n\n\n\nLets check the plots and then the model summary.\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm3.1),\n                  Residuals=resid(lm3.1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\nsummary(lm3.1)\n\n\nCall:\nlm(formula = flipper_length_mm ~ species * bill_length_mm, data = penguins_noNAs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.0561  -3.2927  -0.1646   3.5212  16.2890 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     158.5047     7.0383  22.520  &lt; 2e-16 ***\nspeciesChinstrap                -11.8689    12.5448  -0.946   0.3448    \nspeciesGentoo                    -8.2555    10.8008  -0.764   0.4452    \nbill_length_mm                    0.8139     0.1809   4.500 9.46e-06 ***\nspeciesChinstrap:bill_length_mm   0.1934     0.2788   0.694   0.4884    \nspeciesGentoo:bill_length_mm      0.5943     0.2495   2.382   0.0178 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.799 on 327 degrees of freedom\nMultiple R-squared:  0.8314,    Adjusted R-squared:  0.8288 \nF-statistic: 322.5 on 5 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nAs we hypothesised before modelling that there would be different bill to flipper relationships between species\nThe interaction model follows our scientific assumptions.\nTherefore, it would be incorrect to use lower complexity models (without the interaction for example)\n\n\n\nTo predict again we want to create lines for each species.\nTo do this we want to create fake bill length data over the same range for each species\nHere we will use the seq() function again that creates a sequence of values from your first number to your last number\nAnd you can chose the length of the vector it creates or the distance between each individual value\n\nNewData_&lt;-expand.grid(bill_length_mm=seq(from=min(penguins_noNAs$bill_length_mm),\n                                        to=max(penguins_noNAs$bill_length_mm),\n                                        length.out=1000),\n                     species=c(\"Adelie\",\"Chinstrap\",\"Gentoo\"))\n\nAs the different species won’t be across all of these bill length ranges\nWe should also remove values outside of each species range\nThere would be many ways to do it, here we will use multiple dplyr functions together\nThis is where having the pip function helps keep the order of functions that are applied clear\nFirst we create a df for each Species with their max and min bill lengths\nThen we use case_when (a more sophisticated version of if_else()) to create a new column in our new df that either says Good or it will have NAs\nWe then filter all rows that have NAs in them, thus removing bill lengths outside of each species’ range.\n\nGentoo_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Gentoo\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\nAdelie_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Adelie\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\nChinstrap_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Chinstrap\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\n\nNewData_3&lt;-NewData_ %&gt;% \n  mutate(Range=case_when(species==\"Gentoo\" &\n                              bill_length_mm&gt;=Gentoo_Range$min &\n                              bill_length_mm&lt;=Gentoo_Range$max~\"Good\",\n                         species==\"Adelie\" &\n                           bill_length_mm&gt;=Adelie_Range$min &\n                           bill_length_mm&lt;=Adelie_Range$max~\"Good\",\n                         species==\"Chinstrap\" &\n                           bill_length_mm&gt;=Chinstrap_Range$min &\n                           bill_length_mm&lt;=Chinstrap_Range$max~\"Good\"\n  )) %&gt;% \n  filter(!Range%in%NA) %&gt;% \n  select(-Range)\n\nAfter bad range values are filtered out we use the select function to remove the “Range” column, we do this with the - operator.\n\nPred_2&lt;-predict(lm3.1,NewData_3,se.fit=TRUE)\n\nNewData_2&lt;-NewData_3 %&gt;% \n  mutate(response=Pred_2$fit,\n         se.fit=Pred_2$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nSo now we have many data points that can be used to draw the linear model outputs\n\nggplot()+\n  geom_ribbon(data=NewData_2,mapping=aes(x=bill_length_mm,ymax=Upr,\n                                                 ymin=Lwr,fill=species),\n              alpha=0.4)+\n  geom_line(data=NewData_2,mapping=aes(x=bill_length_mm,y=response,colour=species),\n             alpha=0.4)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks good but lets maybe add the raw data values onto the same figure as the model outputs as before\n\nggplot()+\n  geom_point(data=penguins_noNAs,mapping = aes(x=bill_length_mm,\n                                               y=flipper_length_mm,\n                                               colour=species),\n             alpha=0.4,size=0.8)+\n  geom_ribbon(data=NewData_2,mapping=aes(x=bill_length_mm,ymax=Upr,\n                                         ymin=Lwr,fill=species),\n              alpha=0.4)+\n  geom_line(data=NewData_2,mapping=aes(x=bill_length_mm,y=response,colour=species),\n            alpha=0.4)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Gaussian GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GaussianGLMs.html#data-loading---palmer-penguins",
    "href": "StatisticsTutorials/GaussianGLMs.html#data-loading---palmer-penguins",
    "title": "Gaussian GLMs",
    "section": "",
    "text": "As before lets use the Palmer penguins dataset and remove the NAs (as before NAs should never be remove without considering why there are NAs but here we will remove them for ease).\n\n#install.packages(\"palmerpenguins\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_noNAs&lt;-penguins%&gt;% \n  drop_na()",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Gaussian GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GaussianGLMs.html#modelling-with-categorical-predictor-variables",
    "href": "StatisticsTutorials/GaussianGLMs.html#modelling-with-categorical-predictor-variables",
    "title": "Gaussian GLMs",
    "section": "",
    "text": "So now we will try prove the obvious\nDoes the flipper length of penguins change between species and between sexes?\n\n\nWhether we use an interaction or not depends on if our scientific thought believes the relationship of Species to flipper length is different between sexes (sexual dimorphism may not be consistent across species). We shall use an interaction here as we might expect some sexual dimorphism in some species while less, no or opposite sexual dimorphism in other species. If we had good reason to expect the same sexual dimorphism across these species we would not use an interaction term.\n\n\n\nAs you probably guessed from the title we will be using a Gaussian Distribution. However, flipper length would more technically be a Gamma distribution. It is Numeric, Continuous but cannot be Zero or Negative! Using a Gaussian distribution in this situation is okay as flipper length will not approach 0, so issues of modelling near the zero will not be a problem. This is due to our sampling and just the fact that you won’t ever be able to measure a penguins foot that is 1 mm. or probably less than 25 mm! Our lowest value is 174 in the data. One method could be to centre and scale the flipper_length_mm and model it with a Gamma distribution but this would be a lot of converting back and forth, especially when a Gaussian model will perform equally as well.\n\n\n\nThankfully both sex and species are already factors in the dataset so we don’t have any organising to do!\nSo lets apply our model. lm() is a function in base r that allows us to create a Gaussian GLM object. There is also a glm() function where we would need to define the distribution but lm() is easier to use for gaussian models. We will create a model object then we can inspect and use this model.\n\nlm2.1&lt;-lm(flipper_length_mm~species*sex,data=penguins_noNAs)\n\n\n\n\nWe could apply a linear model to almost all data but often it will not meet our assumptions.\nWe can now check visually the residuals from our model.\nBy using the base plot function in 4 we get 4 plots. The first two are the ones we are most interested in generally. The next two plots are less important generally but can be used to find out what is wrong if the first two plots are not as we want them.\nFor the Residuals vs Fitted plot we want the data to be evenly spread from right to left, meaning the difference between the model and the data (residuals) are not generally larger or smaller at higher or lower values of the model.\nThe next important plot is the qq plot, this is best if the points follow line of x=y, which is the dotted line behind the points.\n\nplot(lm2.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is annoying as we have to press enter in the console to see all the plots.\nWe will code the residuals and qqnorm plots. This is relatively simple code, there are helper functions such as part of the performance package but they are quite unstable from my experience.\n\n#install.packages(\"patchwork\")\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm2.1),\n                  Residuals=resid(lm2.1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs we only have factors in our model we don’t see a ‘cloud’ of points, but the residuals are evenly spread above and below 0 so this is good.\nAs the diagnostics are good we can look at the results\n\nsummary(lm2.1)\n\n\nCall:\nlm(formula = flipper_length_mm ~ species * sex, data = penguins_noNAs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7945  -3.4110   0.0882   3.4590  17.5890 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              187.7945     0.6619 283.721  &lt; 2e-16 ***\nspeciesChinstrap           3.9408     1.1742   3.356 0.000884 ***\nspeciesGentoo             24.9124     0.9947  25.044  &lt; 2e-16 ***\nsexmale                    4.6164     0.9361   4.932  1.3e-06 ***\nspeciesChinstrap:sexmale   3.5600     1.6606   2.144 0.032782 *  \nspeciesGentoo:sexmale      4.2176     1.3971   3.019 0.002737 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.655 on 327 degrees of freedom\nMultiple R-squared:  0.8396,    Adjusted R-squared:  0.8372 \nF-statistic: 342.4 on 5 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nI find the best way to interpret a model output is to plot the model results.\nFirst lets re-plot the raw data, boxplots are probably the best for categorical factors but we could also use a half eye distribution plot. Lets look at both.\nThe best way to do this (in my opinion) is using the patchwork package that can combine the plots\nWe create a plot for each, then plot them side by side. Patchwork is a very simple way of combining ggplots together.\nWe can re-use some of our code from the intro for appearance and colours\n\n#install.packages(\"ggdist\")\nlibrary(ggdist)\n\np_box&lt;-ggplot(penguins_noNAs)+\n  geom_boxplot(aes(x=species,\n                   y=flipper_length_mm,\n                   fill=sex))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\np_cloud&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,colour=sex))+ \n  stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\np_box+p_cloud\n\n\n\n\n\n\n\n\nNow we can also see what the model believes about our data\nThe model has estimated parameters of a linear model.\nIf we wanted to we could write out our model as this:\n\\[FlipperLength = Gaussian(y',\\sigma)\\]\n\\[y'=y\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} sex(female-male):species(Adelie-Chinstrap) \\\\\n+  \\\\\n\\beta_{2} sex(female-male):species(Adelie-Gentoo) \\\\\n+  \\\\\n\\beta_{3} sex(female-male) \\\\\n+  \\\\\n\\beta_{4} species(Adelie-Chinstrap) \\\\\n+  \\\\\n\\beta_{5} species(Adelie-Gentoo) + Intercept\n\\end{aligned}\n\\]\nAs the Gaussian distribution has no data restrictions for the mean value the link function is nothing. Seems silly to include here, and many people wouldn’t, but it will keep consistency for later when we do have a link function.\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just sex and species the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Flipper length in mm based on those species and sexes.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval)\n\nNewData_1&lt;-expand.grid(sex=c(\"female\",\"male\"),\n                     species=c(\"Adelie\",\"Chinstrap\",\"Gentoo\"))\n\nPred&lt;-predict(lm2.1,NewData_1,se.fit=TRUE)\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_point(aes(x=species,\n                 y=response,\n                 colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nLets look at both of these plots next to each other,\nFirst we save both the raw data boxplot as one object and the predicted plot as another then we plot them side by side\n\nPlot1&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,\n                    colour=sex))+ \n  ggdist::stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()+\n  theme(legend.position = \"none\")\n\nPlot2&lt;-ggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Modelled Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\nPlot1+Plot2\n\n\n\n\n\n\n\n\nAt first look this is quite good, but maybe some polishing is needed, mostly the y axis range\nThere are multiple ways to change this for example setting them both to the same range with scale_y_continuous()\n\nPlot1&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,colour=sex))+ \n  ggdist::stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()+\n  scale_y_continuous(limits=c(170,240))+\n  theme(legend.position = \"none\")\n\nPlot2&lt;-ggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Modelled Response Variable (Flipper Length (mm))\")+\n    scale_y_continuous(limits=c(170,240))+\n  theme_classic()\n\nPlot1+Plot2\n\n\n\n\n\n\n\n\nThis is better, although we could actually plot both the modelled and raw data on one plot.\nWe can use a position=position_jitterdodge() to have the raw data not all in one line above their species\n\nggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  geom_point(data=penguins_noNAs,aes(x=species,\n                                      y=flipper_length_mm,\n                                      colour=sex),\n                position=position_jitterdodge(jitter.width = 0.4,\n                                              dodge.width = 0.8),\n              alpha=0.3,\n             size=0.5)+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Gaussian GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GaussianGLMs.html#modelling-continuous-predictor-variables",
    "href": "StatisticsTutorials/GaussianGLMs.html#modelling-continuous-predictor-variables",
    "title": "Gaussian GLMs",
    "section": "",
    "text": "Okay that is what we do with linear models of categorical factors.\nBut what if we want to see the relationship between flipper_length_mm and bill_length_mm.\n\n\nWe know there are species differences and sexual differences in flipper length.\nAs males always tend to be larger lets just assess species differences in their flipper to bill relationship.\nflipper_length_mm~species*bill_length_mm\nLets plot the raw data first\n\nggplot(penguins_noNAs)+\n  geom_point(aes(x=bill_length_mm,y=flipper_length_mm,colour=species))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Flipper Length (mm)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can see from the raw data that we will expect to find some interesting relationships\n\n\n\nNothing has changed from above, so lets stick to Gaussian\n\n\n\nThe fixed effects again need little to no prep so lets apply the model.\n\nlm3.1&lt;-lm(flipper_length_mm~species*bill_length_mm,data=penguins_noNAs)\n\n\n\n\nLets check the plots and then the model summary.\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm3.1),\n                  Residuals=resid(lm3.1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\nsummary(lm3.1)\n\n\nCall:\nlm(formula = flipper_length_mm ~ species * bill_length_mm, data = penguins_noNAs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.0561  -3.2927  -0.1646   3.5212  16.2890 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     158.5047     7.0383  22.520  &lt; 2e-16 ***\nspeciesChinstrap                -11.8689    12.5448  -0.946   0.3448    \nspeciesGentoo                    -8.2555    10.8008  -0.764   0.4452    \nbill_length_mm                    0.8139     0.1809   4.500 9.46e-06 ***\nspeciesChinstrap:bill_length_mm   0.1934     0.2788   0.694   0.4884    \nspeciesGentoo:bill_length_mm      0.5943     0.2495   2.382   0.0178 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.799 on 327 degrees of freedom\nMultiple R-squared:  0.8314,    Adjusted R-squared:  0.8288 \nF-statistic: 322.5 on 5 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nAs we hypothesised before modelling that there would be different bill to flipper relationships between species\nThe interaction model follows our scientific assumptions.\nTherefore, it would be incorrect to use lower complexity models (without the interaction for example)\n\n\n\nTo predict again we want to create lines for each species.\nTo do this we want to create fake bill length data over the same range for each species\nHere we will use the seq() function again that creates a sequence of values from your first number to your last number\nAnd you can chose the length of the vector it creates or the distance between each individual value\n\nNewData_&lt;-expand.grid(bill_length_mm=seq(from=min(penguins_noNAs$bill_length_mm),\n                                        to=max(penguins_noNAs$bill_length_mm),\n                                        length.out=1000),\n                     species=c(\"Adelie\",\"Chinstrap\",\"Gentoo\"))\n\nAs the different species won’t be across all of these bill length ranges\nWe should also remove values outside of each species range\nThere would be many ways to do it, here we will use multiple dplyr functions together\nThis is where having the pip function helps keep the order of functions that are applied clear\nFirst we create a df for each Species with their max and min bill lengths\nThen we use case_when (a more sophisticated version of if_else()) to create a new column in our new df that either says Good or it will have NAs\nWe then filter all rows that have NAs in them, thus removing bill lengths outside of each species’ range.\n\nGentoo_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Gentoo\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\nAdelie_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Adelie\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\nChinstrap_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Chinstrap\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\n\nNewData_3&lt;-NewData_ %&gt;% \n  mutate(Range=case_when(species==\"Gentoo\" &\n                              bill_length_mm&gt;=Gentoo_Range$min &\n                              bill_length_mm&lt;=Gentoo_Range$max~\"Good\",\n                         species==\"Adelie\" &\n                           bill_length_mm&gt;=Adelie_Range$min &\n                           bill_length_mm&lt;=Adelie_Range$max~\"Good\",\n                         species==\"Chinstrap\" &\n                           bill_length_mm&gt;=Chinstrap_Range$min &\n                           bill_length_mm&lt;=Chinstrap_Range$max~\"Good\"\n  )) %&gt;% \n  filter(!Range%in%NA) %&gt;% \n  select(-Range)\n\nAfter bad range values are filtered out we use the select function to remove the “Range” column, we do this with the - operator.\n\nPred_2&lt;-predict(lm3.1,NewData_3,se.fit=TRUE)\n\nNewData_2&lt;-NewData_3 %&gt;% \n  mutate(response=Pred_2$fit,\n         se.fit=Pred_2$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nSo now we have many data points that can be used to draw the linear model outputs\n\nggplot()+\n  geom_ribbon(data=NewData_2,mapping=aes(x=bill_length_mm,ymax=Upr,\n                                                 ymin=Lwr,fill=species),\n              alpha=0.4)+\n  geom_line(data=NewData_2,mapping=aes(x=bill_length_mm,y=response,colour=species),\n             alpha=0.4)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks good but lets maybe add the raw data values onto the same figure as the model outputs as before\n\nggplot()+\n  geom_point(data=penguins_noNAs,mapping = aes(x=bill_length_mm,\n                                               y=flipper_length_mm,\n                                               colour=species),\n             alpha=0.4,size=0.8)+\n  geom_ribbon(data=NewData_2,mapping=aes(x=bill_length_mm,ymax=Upr,\n                                         ymin=Lwr,fill=species),\n              alpha=0.4)+\n  geom_line(data=NewData_2,mapping=aes(x=bill_length_mm,y=response,colour=species),\n            alpha=0.4)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Gaussian GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/PoissonGLMs.html",
    "href": "StatisticsTutorials/PoissonGLMs.html",
    "title": "Poisson GLMs",
    "section": "",
    "text": "Lets use a real-world dataset. This data set is the number of Plant species on different islands in the Galapagos Islands, how many of those species are endemic, the area of the island, its max elevation, the distance to the nearest island, its distance to santa cruz (the most populace island) and the area of the nearest island.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(faraway)\nlibrary(patchwork)\n\ndata(\"gala\")\n\nglimpse(gala)\n\nRows: 30\nColumns: 7\n$ Species   &lt;dbl&gt; 58, 31, 3, 25, 2, 18, 24, 10, 8, 2, 97, 93, 58, 5, 40, 347, …\n$ Endemics  &lt;dbl&gt; 23, 21, 3, 9, 1, 11, 0, 7, 4, 2, 26, 35, 17, 4, 19, 89, 23, …\n$ Area      &lt;dbl&gt; 25.09, 1.24, 0.21, 0.10, 0.05, 0.34, 0.08, 2.33, 0.03, 0.18,…\n$ Elevation &lt;dbl&gt; 346, 109, 114, 46, 77, 119, 93, 168, 71, 112, 198, 1494, 49,…\n$ Nearest   &lt;dbl&gt; 0.6, 0.6, 2.8, 1.9, 1.9, 8.0, 6.0, 34.1, 0.4, 2.6, 1.1, 4.3,…\n$ Scruz     &lt;dbl&gt; 0.6, 26.3, 58.7, 47.4, 1.9, 8.0, 12.0, 290.2, 0.4, 50.2, 88.…\n$ Adjacent  &lt;dbl&gt; 1.84, 572.33, 0.78, 0.18, 903.82, 1.84, 0.34, 2.85, 17.95, 0…\n\n\n\n\n\n\nAs an archipelago of volcanic islands the Galapagos were formed by geological processes, these geological processes such as tectonic movement and volcanic activity will have implications for the amount of endemic plant species on an island. Therefore, lets explore the relationship of plant endemism and physical features of the islands. For this example we will assess the effect of elevation of an island on the number of endemic species on that island.\nThis is a relatively simple model with just one fixed effect and can be written as:\nCount of Endemic Plants ~ Elevation\n\n\n\nThe number of endemic species is a count response where there is no theoretical limit (although one probably exists). Therefore the values can range from 0 upwards. This tells us that is most likely a Poisson distribution.\n\n\n\nOften with highly variable numeric values, such as Elevation or Area or Population, we might need to transform our fixed effect with a log or a square root. We can assess the distribution of our Elevation variables to see if there is a a lot of variance across our islands.\n\np1&lt;-ggplot(gala,aes(x=Elevation))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(gala,aes(x=log(Elevation)))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np1+p2\n\n\n\n\n\n\n\n\nWe seem to have a good spread of values across its range with some very large values so we will use a log transformation for modelling. We can then convert back to its native scale after modelling. Lets fit the model using the glm function, we add our statistical formula with the log transformed Elevation, our data and then we specify that the family or distribution we want to use is poisson.\n\ndf&lt;-gala %&gt;% \n  mutate(Elevation_log=log(Elevation))\n\nglm1&lt;-glm(Endemics~Elevation_log,data=df, family= \"poisson\")\n\n\n\n\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nWe see some fairly mixed results here. The normality of residuals is good with almost all points following the line, whereas our homogenerity of variance is not amazing, which is likely being driven by a low number of high elevation values. However, the general patterns are quite spread but with very large values showing some overdispersion. This over dispersion might make us want to use a distribution that is more able to deal with over dispersion, such as Negative Binomial. We will accept the amount of over dispersion here as it is quite minor and only at the largest values, which are being driven by our highest value of Elevation and highest value of Endemism: Fernandina and Santa Cruz\n\nsummary(glm1)\n\n\nCall:\nglm(formula = Endemics ~ Elevation_log, family = \"poisson\", data = df)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.32549    0.23027  -5.756  8.6e-09 ***\nElevation_log  0.79088    0.03657  21.626  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 743.55  on 29  degrees of freedom\nResidual deviance: 223.50  on 28  degrees of freedom\nAIC: 360.45\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Count of Endemic Species = Poisson(\\lambda)\\]\n\\[\\lambda=log(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} log Elevation + Intercept\n\\end{aligned}\n\\]\nAs the Poisson distribution requires only one shape parameter (\\(\\lambda\\)) and this must be zero or above, we must convert out linear equation results (\\(y\\)) so that it is non-negative. This means we use the link function, which for poisson models is by default a log. We can use a different link function if we want, or even check to check which link was used.\n\nglm1$family$link\n\n[1] \"log\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just Elevation (on the log scale) the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Count of Endemic Species based on those log Elevations.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-data.frame(Elevation_log=seq(min(df$Elevation_log),max(df$Elevation_log),length.out=50))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Elevation=exp(Elevation_log))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Elevation,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=Elevation,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Elevation\",y=\"Response Variable (Count of Endemic Plant Species)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=df,aes(x=Elevation,\n                         y=Endemics),\n              alpha=0.3,\n             size=0.8,\n             colour=\"darkcyan\")+\n  geom_ribbon(aes(x=Elevation,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=Elevation,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Elevation\",y=\"Response Variable (Count of Endemic Plant Species)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good, with more uncertainty at higher values where there are less values to influence the prediction. This is a very simplified model that is not taking into consideration many different factors. For example, Age of an island is highly influential on its plant communities as well as the volcanic activity. So from a science point of view this is not the whole story, infact the Elevation may be just a proxy for the amount of available habitat space and the potential for habitat niches that are influential on endemism. Other factors such as human occupation and the influence that has caused in Galapagos on the local plant populations should not be ignored: invasive species, agricultural practices etc.\n\n\n\nLets create a more complex poisson model. This data set is the number of epileptic seizures from 59 patients in a clinical trial of a treatment. Patients were given a a treatment of Placebo or a drug called Progabide. There is a base number of seizures for the 8 weeks before the trial and then a seizure rate for every 2 week period (up to 8 weeks) after being given a treatment. Patients Ages are also available. We will summarise all seizures had by a patient in the 4 periods post treatment to make this a simpler assessment. Although, we could have assessed an effect over time post treatment as well. (Another day perhaps)\n\n# install.packages(\"HSAUR2\")\n\nlibrary(HSAUR2)\n\nLoading required package: tools\n\n\n\nAttaching package: 'HSAUR2'\n\n\nThe following objects are masked from 'package:faraway':\n\n    epilepsy, toenail\n\n\nThe following object is masked from 'package:tidyr':\n\n    household\n\ndata(\"epilepsy\")\n\nglimpse(epilepsy)\n\nRows: 236\nColumns: 6\n$ treatment    &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ base         &lt;int&gt; 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 8, 8, 8, 8, 6…\n$ age          &lt;int&gt; 31, 31, 31, 31, 30, 30, 30, 30, 25, 25, 25, 25, 36, 36, 3…\n$ seizure.rate &lt;int&gt; 5, 3, 3, 3, 3, 5, 3, 3, 2, 4, 0, 5, 4, 4, 1, 4, 7, 18, 9,…\n$ period       &lt;ord&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, …\n$ subject      &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, …\n\ndf_epilepsy&lt;-epilepsy %&gt;% \n  group_by(age,base,subject,treatment) %&gt;% \n  summarise(seizures=sum(seizure.rate))\n\n`summarise()` has grouped output by 'age', 'base', 'subject'. You can override\nusing the `.groups` argument.\n\n\n\n\nWe will assess the number of seizures in the 8 weeks after treatment for patients and assess whether this pattern changes with age and the number of seizures they had before treatment.\nThis is a bit more complex model with two interacting fixed effect and one additional fixed effect and can be written as:\nCount of Seizures After Treatment ~ Treatment*Age + Base Number of Seizures\n\n\n\nAgain, the number of seizures can only be a non-negative integer.\n\n\n\nLets check all our fixed effects. For numeric values we can assess their distribution, categorical we can see the number of samples is relatively even.\n\np1&lt;-ggplot(df_epilepsy,aes(x=age))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(df_epilepsy,aes(x=treatment))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np3&lt;-ggplot(df_epilepsy,aes(x=base))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n  \np4&lt;-ggplot(df_epilepsy,aes(x=sqrt(base)))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\nOur factors age and treatment seem fine, but maybe we should square root the base effect so we don’t have really big base values influencing our model as much. We could log here or centre and scale but we shall use square root for now.\n\ndf&lt;-df_epilepsy %&gt;% \n  mutate(base_sqrt=sqrt(base))\n\nglm2&lt;-glm(seizures~treatment*age+base_sqrt,data=df, family= \"poisson\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs earlier, we see some fairly mixed results here. The normality of residuals is good with almost all points following the line, whereas our homogenerity of variance is again not amazing, which is likely being driven by a low number of high base seizure values values. However, the general patterns are quite spread but with very large values showing some overdispersion. This over dispersion might make us want to use a distribution that is more able to deal with over dispersion, such as Negative Binomial. We will accept the amount of over dispersion here as it is quite minor and only at the largest values.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = seizures ~ treatment * age + base_sqrt, family = \"poisson\", \n    data = df)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.483382   0.182272   2.652   0.0080 ** \ntreatmentProgabide      0.317571   0.226665   1.401   0.1612    \nage                     0.029911   0.005555   5.385 7.25e-08 ***\nbase_sqrt               0.357709   0.008901  40.187  &lt; 2e-16 ***\ntreatmentProgabide:age -0.013838   0.007903  -1.751   0.0799 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2122.73  on 58  degrees of freedom\nResidual deviance:  490.77  on 54  degrees of freedom\nAIC: 784.04\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Count of Seizures Post Treatment = Poisson(\\lambda)\\]\n\\[\\lambda=log(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} treatment:age\\\\\n+ \\beta_{2} \\sqrt{base} \\\\\n+ \\beta_{3} age\\\\\n+ \\beta_{4} treatment\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Poisson distribution requires only one shape parameter (\\(\\lambda\\)) and this must be zero or above, we must convert out linear equation results (\\(y\\)) so that it is non-negative. This means we use the link function, which for poisson models is by default a log. We can use a different link function if we want, or even check to check which link was used.\n\nglm2$family$link\n\n[1] \"log\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age, treatment and base level the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nWe will choose a low, middle and high base level\nThe model then predicts the average Count of Seizures based on those ages, treatments and base levels.\nAs we have two continuous fixed effects we could plot as heatmap style if we wanted. But then it is difficult or impossible to display confidence levels well.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(base_sqrt=seq(min(df$base_sqrt),max(df$base_sqrt),length.out=100),\n                       age=seq(min(df$age),max(df$age),length.out=50),\n                       treatment=c(\"placebo\",\"Progabide\"))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         base=base_sqrt^2,\n         treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData)+\n  geom_tile(aes(x=age,y=base,fill=response),\n            alpha=0.9)+\n  scale_y_sqrt()+\n  scale_fill_viridis_c(direction=-1)+\n  facet_wrap(~treatment)+\n  labs(x=\"Age\",y=\"Base Number of Seizures\",fill=\"Predicted Number\\nof Seizures\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\ndf_1&lt;-df %&gt;% \n  mutate(treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData)+\n  geom_tile(aes(x=age,y=base,fill=response),\n            alpha=0.9)+\n  geom_point(data=df_1,aes(x=age,y=base,fill=seizures),shape=21,colour=\"#FFFFFF50\")+\n  scale_y_sqrt()+\n  scale_fill_viridis_c(direction=-1,limits=c(0,470))+\n  scale_colour_viridis_c(direction=-1,limits=c(0,470))+\n  facet_wrap(~treatment)+\n  labs(x=\"Age\",y=\"Base Number of Seizures\",fill=\"Number\\nof Seizures\",colour=\"Number\\nof Seizures\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThe patterns of colour from points to back ground do seem to generalise well. However, from this plot we can see clearly that the model is predicting in the age/base space that is not in the original data, so perhaps it would be better to plot the model assuming an average base level of seizures then compare with the raw data. We can also display the models confidence then too.\n\nNewData_2&lt;-expand.grid(base_sqrt=sqrt(mean(df$base)),\n                       age=seq(min(df$age),max(df$age),length.out=50),\n                       treatment=c(\"placebo\",\"Progabide\"))\n\nPred_3&lt;-predict(glm2,NewData_2,se.fit=TRUE,type=\"response\")\n\nNewData_MeanBase&lt;-NewData_2 %&gt;% \n  mutate(response=Pred_3$fit,\n         se.fit=Pred_3$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         base=base_sqrt^2,\n         treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData_MeanBase)+\n  geom_point(data=df_1,aes(x=age,y=seizures,colour=treatment))+\n   geom_ribbon(aes(x=age,\n                     ymax=Upr,\n                     ymin=Lwr,\n                   fill=treatment),\n               alpha=0.7)+\n   geom_line(aes(x=age,\n                  y=response,\n                   colour=treatment))+\n   scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n   scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n   scale_y_continuous(limits=c(0,100))+\n   labs(x=\"Age\",y=\"Response Variable (Count of Seizures)\",colour=\"Treatment\",fill=\"Treatment\")+\n   theme_classic()\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nI have set the y axis to ignore very large values (above 100) so we can see clearly what the model is telling us.\nThis shows a clearer picture and helps us understand that the model has seen some differences between the treatments and that this difference becomes more distinct with age. Although the differences are very minimal between treatments. But a clear increase in Seizures with increasing age regardless of treatment. This model (which is simplistic and probably not fully adequate at addressing this question) would not give us evidence that the drug is significantly different from placebo but we might infer there is some lessening of the effect of age in the Progabide treatment.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Poisson GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/PoissonGLMs.html#data-loading-complex---epilepsy",
    "href": "StatisticsTutorials/PoissonGLMs.html#data-loading-complex---epilepsy",
    "title": "Poisson GLMs",
    "section": "",
    "text": "Lets create a more complex poisson model. This data set is the number of epileptic seizures from 59 patients in a clinical trial of a treatment. Patients were given a a treatment of Placebo or a drug called Progabide. There is a base number of seizures for the 8 weeks before the trial and then a seizure rate for every 2 week period (up to 8 weeks) after being given a treatment. Patients Ages are also available. We will summarise all seizures had by a patient in the 4 periods post treatment to make this a simpler assessment. Although, we could have assessed an effect over time post treatment as well. (Another day perhaps)\n\n# install.packages(\"HSAUR2\")\n\nlibrary(HSAUR2)\n\nLoading required package: tools\n\n\n\nAttaching package: 'HSAUR2'\n\n\nThe following objects are masked from 'package:faraway':\n\n    epilepsy, toenail\n\n\nThe following object is masked from 'package:tidyr':\n\n    household\n\ndata(\"epilepsy\")\n\nglimpse(epilepsy)\n\nRows: 236\nColumns: 6\n$ treatment    &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ base         &lt;int&gt; 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 8, 8, 8, 8, 6…\n$ age          &lt;int&gt; 31, 31, 31, 31, 30, 30, 30, 30, 25, 25, 25, 25, 36, 36, 3…\n$ seizure.rate &lt;int&gt; 5, 3, 3, 3, 3, 5, 3, 3, 2, 4, 0, 5, 4, 4, 1, 4, 7, 18, 9,…\n$ period       &lt;ord&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, …\n$ subject      &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, …\n\ndf_epilepsy&lt;-epilepsy %&gt;% \n  group_by(age,base,subject,treatment) %&gt;% \n  summarise(seizures=sum(seizure.rate))\n\n`summarise()` has grouped output by 'age', 'base', 'subject'. You can override\nusing the `.groups` argument.\n\n\n\n\nWe will assess the number of seizures in the 8 weeks after treatment for patients and assess whether this pattern changes with age and the number of seizures they had before treatment.\nThis is a bit more complex model with two interacting fixed effect and one additional fixed effect and can be written as:\nCount of Seizures After Treatment ~ Treatment*Age + Base Number of Seizures\n\n\n\nAgain, the number of seizures can only be a non-negative integer.\n\n\n\nLets check all our fixed effects. For numeric values we can assess their distribution, categorical we can see the number of samples is relatively even.\n\np1&lt;-ggplot(df_epilepsy,aes(x=age))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(df_epilepsy,aes(x=treatment))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np3&lt;-ggplot(df_epilepsy,aes(x=base))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n  \np4&lt;-ggplot(df_epilepsy,aes(x=sqrt(base)))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\nOur factors age and treatment seem fine, but maybe we should square root the base effect so we don’t have really big base values influencing our model as much. We could log here or centre and scale but we shall use square root for now.\n\ndf&lt;-df_epilepsy %&gt;% \n  mutate(base_sqrt=sqrt(base))\n\nglm2&lt;-glm(seizures~treatment*age+base_sqrt,data=df, family= \"poisson\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs earlier, we see some fairly mixed results here. The normality of residuals is good with almost all points following the line, whereas our homogenerity of variance is again not amazing, which is likely being driven by a low number of high base seizure values values. However, the general patterns are quite spread but with very large values showing some overdispersion. This over dispersion might make us want to use a distribution that is more able to deal with over dispersion, such as Negative Binomial. We will accept the amount of over dispersion here as it is quite minor and only at the largest values.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = seizures ~ treatment * age + base_sqrt, family = \"poisson\", \n    data = df)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.483382   0.182272   2.652   0.0080 ** \ntreatmentProgabide      0.317571   0.226665   1.401   0.1612    \nage                     0.029911   0.005555   5.385 7.25e-08 ***\nbase_sqrt               0.357709   0.008901  40.187  &lt; 2e-16 ***\ntreatmentProgabide:age -0.013838   0.007903  -1.751   0.0799 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2122.73  on 58  degrees of freedom\nResidual deviance:  490.77  on 54  degrees of freedom\nAIC: 784.04\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Count of Seizures Post Treatment = Poisson(\\lambda)\\]\n\\[\\lambda=log(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} treatment:age\\\\\n+ \\beta_{2} \\sqrt{base} \\\\\n+ \\beta_{3} age\\\\\n+ \\beta_{4} treatment\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Poisson distribution requires only one shape parameter (\\(\\lambda\\)) and this must be zero or above, we must convert out linear equation results (\\(y\\)) so that it is non-negative. This means we use the link function, which for poisson models is by default a log. We can use a different link function if we want, or even check to check which link was used.\n\nglm2$family$link\n\n[1] \"log\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age, treatment and base level the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nWe will choose a low, middle and high base level\nThe model then predicts the average Count of Seizures based on those ages, treatments and base levels.\nAs we have two continuous fixed effects we could plot as heatmap style if we wanted. But then it is difficult or impossible to display confidence levels well.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(base_sqrt=seq(min(df$base_sqrt),max(df$base_sqrt),length.out=100),\n                       age=seq(min(df$age),max(df$age),length.out=50),\n                       treatment=c(\"placebo\",\"Progabide\"))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         base=base_sqrt^2,\n         treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData)+\n  geom_tile(aes(x=age,y=base,fill=response),\n            alpha=0.9)+\n  scale_y_sqrt()+\n  scale_fill_viridis_c(direction=-1)+\n  facet_wrap(~treatment)+\n  labs(x=\"Age\",y=\"Base Number of Seizures\",fill=\"Predicted Number\\nof Seizures\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\ndf_1&lt;-df %&gt;% \n  mutate(treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData)+\n  geom_tile(aes(x=age,y=base,fill=response),\n            alpha=0.9)+\n  geom_point(data=df_1,aes(x=age,y=base,fill=seizures),shape=21,colour=\"#FFFFFF50\")+\n  scale_y_sqrt()+\n  scale_fill_viridis_c(direction=-1,limits=c(0,470))+\n  scale_colour_viridis_c(direction=-1,limits=c(0,470))+\n  facet_wrap(~treatment)+\n  labs(x=\"Age\",y=\"Base Number of Seizures\",fill=\"Number\\nof Seizures\",colour=\"Number\\nof Seizures\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThe patterns of colour from points to back ground do seem to generalise well. However, from this plot we can see clearly that the model is predicting in the age/base space that is not in the original data, so perhaps it would be better to plot the model assuming an average base level of seizures then compare with the raw data. We can also display the models confidence then too.\n\nNewData_2&lt;-expand.grid(base_sqrt=sqrt(mean(df$base)),\n                       age=seq(min(df$age),max(df$age),length.out=50),\n                       treatment=c(\"placebo\",\"Progabide\"))\n\nPred_3&lt;-predict(glm2,NewData_2,se.fit=TRUE,type=\"response\")\n\nNewData_MeanBase&lt;-NewData_2 %&gt;% \n  mutate(response=Pred_3$fit,\n         se.fit=Pred_3$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         base=base_sqrt^2,\n         treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData_MeanBase)+\n  geom_point(data=df_1,aes(x=age,y=seizures,colour=treatment))+\n   geom_ribbon(aes(x=age,\n                     ymax=Upr,\n                     ymin=Lwr,\n                   fill=treatment),\n               alpha=0.7)+\n   geom_line(aes(x=age,\n                  y=response,\n                   colour=treatment))+\n   scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n   scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n   scale_y_continuous(limits=c(0,100))+\n   labs(x=\"Age\",y=\"Response Variable (Count of Seizures)\",colour=\"Treatment\",fill=\"Treatment\")+\n   theme_classic()\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nI have set the y axis to ignore very large values (above 100) so we can see clearly what the model is telling us.\nThis shows a clearer picture and helps us understand that the model has seen some differences between the treatments and that this difference becomes more distinct with age. Although the differences are very minimal between treatments. But a clear increase in Seizures with increasing age regardless of treatment. This model (which is simplistic and probably not fully adequate at addressing this question) would not give us evidence that the drug is significantly different from placebo but we might infer there is some lessening of the effect of age in the Progabide treatment.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Poisson GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GammaGLMs.html",
    "href": "StatisticsTutorials/GammaGLMs.html",
    "title": "Gamma GLMs",
    "section": "",
    "text": "Lets use the real-world dataset Loblolly Pine trees, which contains the Height of Pine trees (in feet) over Ages (in years) for different individuals (Seeds).\nThis will be quite a simple example assessing change with age of pine trees. Here we will not be telling the model that are data are repeat measures. This means our data are not independent. To take this into consideration we could use a General Linear Mixed Effects Model with Seed as a random factor. However, for this example we shall pretend that there isn’t this structure in our data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(performance)\nlibrary(patchwork)\n\ndata(\"Loblolly\")\n\nglimpse(Loblolly)\n\nRows: 84\nColumns: 3\n$ height &lt;dbl&gt; 4.51, 10.89, 28.72, 41.74, 52.70, 60.92, 4.55, 10.92, 29.07, 42…\n$ age    &lt;dbl&gt; 3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 2…\n$ Seed   &lt;ord&gt; 301, 301, 301, 301, 301, 301, 303, 303, 303, 303, 303, 303, 305…\n\n\n\n\n\nAs we mentioned above we are simplifying this example into the change in Height of Pine trees with age.\nThis is a very simple model with just one fixed effect and can be written as:\nHeight of Pine Tree ~ Age\n\n\n\nThe height of the tree must be positive and is continuous, therefore we should technically use a Gamma Model. Again as with most Gamma examples we could use a Gaussian distribution, which is simpler mathematically, but when using a Gaussian model on a transformed response variable we will get incorrect estimates of effects meaning our inference will be incorrect.\n\n\n\nAs a fully experimental data set we actually have the same count of each age category: 3, 5, 10, 15, 20, 25. This means we should be able to use the raw fixed effect value with little issue.\n\nggplot(Loblolly,aes(x=age))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\n\n\n\n\n\n\nAs we are scientists so should use logical units we shall convert our height column to metres before modelling. We can do this by multiplying our feet by 0.3048.\n\ndf&lt;-Loblolly %&gt;% \n  mutate(Height=height*0.3048)\n\nglm1&lt;-glm(Height~age,data=df, family=Gamma(link = \"identity\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nWe see some fairly mixed results here. We can see out Homogeneity of Variance isn’t flat and horizontal, but there are no clear patterns at high or low fitted values. The only pattern we do see is vertical banding, this is because we have actually got repeat measurements of the same trees over time. We shall ignore this for this example but we really should have run a GLMM to take into account all the hierarchy of the data.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = Height ~ age, family = Gamma(link = \"identity\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.32432    0.05556  -23.84   &lt;2e-16 ***\nage          0.88058    0.01246   70.68   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.008982799)\n\n    Null deviance: 57.46161  on 83  degrees of freedom\nResidual deviance:  0.73641  on 82  degrees of freedom\nAIC: 173.7\n\nNumber of Fisher Scoring iterations: 3\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Height \\;of \\;Pine \\;Tree = Gamma(y',\\alpha)\\]\n\\[y'=y\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Age + Intercept\n\\end{aligned}\n\\]\nAs the Gamma distribution requires two shape parameters (\\(y'\\) and \\(\\alpha\\)), where \\(y'\\) must be above zero, we must convert out linear equation results (\\(y\\)) so that it is positive. This means we use the link function, which for Gamma models is by default a inverse. We can use a different link function if we want, and here we did, we used the identity link function. The identity function does nothing: it just uses the raw data, this is simpler mathematically and computationally but may be the incorrect decision depending on the situation.\n\nglm1$family$link\n\n[1] \"identity\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nAs always we then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just age, the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Height of Pine Tree based on those ages.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\n\nNewData_1&lt;-data.frame(age=seq(min(df$age),max(df$age),length.out=50))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=age,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=age,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Age\",y=\"Response Variable (Height of Pine Tree)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=df,aes(x=age,\n                         y=Height),\n              alpha=0.3,\n             size=0.8,\n             colour=\"darkcyan\")+\n  geom_ribbon(aes(x=age,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=age,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Age\",y=\"Response Variable (Height of Pine Tree)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good. But we have been honest about making sure we model the full hierarchy of our data set, which we did not do here! However, we can see the model got generally the correct pattern with an almost 1:1 relationship with meters and years. We could have seen that from the summary table, where the estimate for age was 0.881 while the estimate for the intercept was -1.324. Fed into our equation that gives use a line of Height = 0.881 * Age + -1.324.\n\n\n\n\nThis dataset is an experimental dataset where weights (g) of chicks were measured from birth until day 21 (Time) based on 4 different dietary regimes. Again as above there is correlation element of each chick being correlated with its previous weight but again as above we will ignore this issue. This type of hierarchy or repeat measurement is highly important and as researchers faced with this structure of data we should really use a GLMM (sometimes called hierarchy model or multilevel model). This should be fine for our example but again: always model the structure you know or understand about your data.\n\n\ndata(\"ChickWeight\")\n\nsummary(ChickWeight)\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n\n\nWe will assess the Weight over time of Chicks depending on different diet types. We will assess if the change in weight over time is different across the different diets.\nThis is a bit more complex model with two interacting fixed effect, and can be written as:\nChick Weight ~ Age*Diet\n\n\n\nAgain, our value is a measurement that is always positive and continuous, thus we will use the Gamma distribution. While, the values in our response variable do not have decimal points the weight of a chick in grams could feasibly have 0.5 of a gram etc. This data not having decimals doesn’t matter and is more related to the measurement style (precision of the scale used), the data still come from a Gamma distribution.\n\n\n\nLets check all our fixed effects.\n\np1&lt;-ggplot(ChickWeight,aes(x=Time))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(ChickWeight,aes(x=Diet))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\n(p1+p2)\n\n\n\n\n\n\n\n\nOur factors are all evenly grouped as this is more of a traditional experimental set up. There are more chicks on diet 1 but still high values for the other diet types so it should be fine. We have less and less chicks at higher times of the experiment, I don’t know why but I would guess at some mortality issues.\n\nglm2&lt;-glm(weight~Time*Diet,data=ChickWeight, family=Gamma())\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs earlier, we see some fairly mixed results here. The normality of residuals is not perfect with many small and high value points not following the line, whereas our homogenerity of variance is pretty good apart from less variation at lower values than larger values.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = weight ~ Time * Diet, family = Gamma(), data = ChickWeight)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.892e-02  4.949e-04  38.234  &lt; 2e-16 ***\nTime        -6.796e-04  2.882e-05 -23.576  &lt; 2e-16 ***\nDiet2       -1.857e-03  7.830e-04  -2.371 0.018065 *  \nDiet3       -2.703e-03  7.457e-04  -3.625 0.000315 ***\nDiet4       -3.463e-03  7.391e-04  -4.685  3.5e-06 ***\nTime:Diet2   4.940e-05  4.484e-05   1.102 0.271037    \nTime:Diet3   4.714e-05  4.208e-05   1.120 0.263050    \nTime:Diet4   1.031e-04  4.249e-05   2.426 0.015570 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.07498304)\n\n    Null deviance: 193.022  on 577  degrees of freedom\nResidual deviance:  42.305  on 570  degrees of freedom\nAIC: 5512.8\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Chick \\;Weight = Gamma(y',\\alpha)\\]\n\\[y'=y^{-1}\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Time:Diet \\;1 \\;vs \\;4\\\\\n+ \\beta_{2} Time:Diet \\;1 \\;vs \\;3\\\\\n+ \\beta_{3} Time:Diet \\;1 \\;vs \\;2\\\\\n+ \\beta_{4} Diet \\;1 \\;vs \\;4\\\\\n+ \\beta_{5} Diet \\;1 \\;vs \\;3\\\\\n+ \\beta_{6} Diet \\;1 \\;vs \\;2\\\\\n+ \\beta_{7} Time\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Gamma distribution requires two shape parameters (\\(y'\\) and \\(\\alpha\\)), where \\(y'\\) must be above zero, we must convert out linear equation results (\\(y\\)) so that it is positive. This means we use the link function, which for Gamma models is by default a inverse. We can use a different link function if we want, for this example we used this default.\n\nglm2$family$link\n\n[1] \"inverse\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age and diet the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average weight based on those ages and diets.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(Time=seq(min(ChickWeight$Time),max(ChickWeight$Time),length.out=100),\n                       Diet=as.factor(c(1:4)))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Time,ymax=Upr,ymin=Lwr,fill=Diet),\n              alpha=0.6)+\n  geom_line(aes(x=Time,y=response,colour=Diet,linetype=Diet))+\n  labs(x=\"Age\",y=\"Predicted Chick Weight (g)\",\n       fill=\"Diet\",colour=\"Diet\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=ChickWeight,aes(x=Time,y=weight,colour=Diet),\n             alpha=0.5,\n             size=0.6)+\n  geom_ribbon(aes(x=Time,ymax=Upr,ymin=Lwr,fill=Diet),\n              alpha=0.6)+\n  geom_line(aes(x=Time,y=response,colour=Diet,linetype=Diet))+\n  labs(x=\"Age\",y=\"Predicted Chick Weight (g)\",\n       fill=\"Diet\",colour=\"Diet\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo again our model seems pretty good, with minimal differences between treatments and strong increase in weight with Age of Chick. But some of this isn’t great, such as very low values being over predicted, plus some clear lines of data well outside the models.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Gamma GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GammaGLMs.html#data-loading-simple---loblolly-pine-trees",
    "href": "StatisticsTutorials/GammaGLMs.html#data-loading-simple---loblolly-pine-trees",
    "title": "Gamma GLMs",
    "section": "",
    "text": "Lets use the real-world dataset Loblolly Pine trees, which contains the Height of Pine trees (in feet) over Ages (in years) for different individuals (Seeds).\nThis will be quite a simple example assessing change with age of pine trees. Here we will not be telling the model that are data are repeat measures. This means our data are not independent. To take this into consideration we could use a General Linear Mixed Effects Model with Seed as a random factor. However, for this example we shall pretend that there isn’t this structure in our data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(performance)\nlibrary(patchwork)\n\ndata(\"Loblolly\")\n\nglimpse(Loblolly)\n\nRows: 84\nColumns: 3\n$ height &lt;dbl&gt; 4.51, 10.89, 28.72, 41.74, 52.70, 60.92, 4.55, 10.92, 29.07, 42…\n$ age    &lt;dbl&gt; 3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 2…\n$ Seed   &lt;ord&gt; 301, 301, 301, 301, 301, 301, 303, 303, 303, 303, 303, 303, 305…\n\n\n\n\n\nAs we mentioned above we are simplifying this example into the change in Height of Pine trees with age.\nThis is a very simple model with just one fixed effect and can be written as:\nHeight of Pine Tree ~ Age\n\n\n\nThe height of the tree must be positive and is continuous, therefore we should technically use a Gamma Model. Again as with most Gamma examples we could use a Gaussian distribution, which is simpler mathematically, but when using a Gaussian model on a transformed response variable we will get incorrect estimates of effects meaning our inference will be incorrect.\n\n\n\nAs a fully experimental data set we actually have the same count of each age category: 3, 5, 10, 15, 20, 25. This means we should be able to use the raw fixed effect value with little issue.\n\nggplot(Loblolly,aes(x=age))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\n\n\n\n\n\n\nAs we are scientists so should use logical units we shall convert our height column to metres before modelling. We can do this by multiplying our feet by 0.3048.\n\ndf&lt;-Loblolly %&gt;% \n  mutate(Height=height*0.3048)\n\nglm1&lt;-glm(Height~age,data=df, family=Gamma(link = \"identity\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nWe see some fairly mixed results here. We can see out Homogeneity of Variance isn’t flat and horizontal, but there are no clear patterns at high or low fitted values. The only pattern we do see is vertical banding, this is because we have actually got repeat measurements of the same trees over time. We shall ignore this for this example but we really should have run a GLMM to take into account all the hierarchy of the data.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = Height ~ age, family = Gamma(link = \"identity\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.32432    0.05556  -23.84   &lt;2e-16 ***\nage          0.88058    0.01246   70.68   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.008982799)\n\n    Null deviance: 57.46161  on 83  degrees of freedom\nResidual deviance:  0.73641  on 82  degrees of freedom\nAIC: 173.7\n\nNumber of Fisher Scoring iterations: 3\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Height \\;of \\;Pine \\;Tree = Gamma(y',\\alpha)\\]\n\\[y'=y\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Age + Intercept\n\\end{aligned}\n\\]\nAs the Gamma distribution requires two shape parameters (\\(y'\\) and \\(\\alpha\\)), where \\(y'\\) must be above zero, we must convert out linear equation results (\\(y\\)) so that it is positive. This means we use the link function, which for Gamma models is by default a inverse. We can use a different link function if we want, and here we did, we used the identity link function. The identity function does nothing: it just uses the raw data, this is simpler mathematically and computationally but may be the incorrect decision depending on the situation.\n\nglm1$family$link\n\n[1] \"identity\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nAs always we then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just age, the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Height of Pine Tree based on those ages.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\n\nNewData_1&lt;-data.frame(age=seq(min(df$age),max(df$age),length.out=50))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=age,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=age,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Age\",y=\"Response Variable (Height of Pine Tree)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=df,aes(x=age,\n                         y=Height),\n              alpha=0.3,\n             size=0.8,\n             colour=\"darkcyan\")+\n  geom_ribbon(aes(x=age,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=age,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Age\",y=\"Response Variable (Height of Pine Tree)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good. But we have been honest about making sure we model the full hierarchy of our data set, which we did not do here! However, we can see the model got generally the correct pattern with an almost 1:1 relationship with meters and years. We could have seen that from the summary table, where the estimate for age was 0.881 while the estimate for the intercept was -1.324. Fed into our equation that gives use a line of Height = 0.881 * Age + -1.324.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Gamma GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GammaGLMs.html#data-loading-complex---chicks",
    "href": "StatisticsTutorials/GammaGLMs.html#data-loading-complex---chicks",
    "title": "Gamma GLMs",
    "section": "",
    "text": "This dataset is an experimental dataset where weights (g) of chicks were measured from birth until day 21 (Time) based on 4 different dietary regimes. Again as above there is correlation element of each chick being correlated with its previous weight but again as above we will ignore this issue. This type of hierarchy or repeat measurement is highly important and as researchers faced with this structure of data we should really use a GLMM (sometimes called hierarchy model or multilevel model). This should be fine for our example but again: always model the structure you know or understand about your data.\n\n\ndata(\"ChickWeight\")\n\nsummary(ChickWeight)\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n\n\nWe will assess the Weight over time of Chicks depending on different diet types. We will assess if the change in weight over time is different across the different diets.\nThis is a bit more complex model with two interacting fixed effect, and can be written as:\nChick Weight ~ Age*Diet\n\n\n\nAgain, our value is a measurement that is always positive and continuous, thus we will use the Gamma distribution. While, the values in our response variable do not have decimal points the weight of a chick in grams could feasibly have 0.5 of a gram etc. This data not having decimals doesn’t matter and is more related to the measurement style (precision of the scale used), the data still come from a Gamma distribution.\n\n\n\nLets check all our fixed effects.\n\np1&lt;-ggplot(ChickWeight,aes(x=Time))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(ChickWeight,aes(x=Diet))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\n(p1+p2)\n\n\n\n\n\n\n\n\nOur factors are all evenly grouped as this is more of a traditional experimental set up. There are more chicks on diet 1 but still high values for the other diet types so it should be fine. We have less and less chicks at higher times of the experiment, I don’t know why but I would guess at some mortality issues.\n\nglm2&lt;-glm(weight~Time*Diet,data=ChickWeight, family=Gamma())\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs earlier, we see some fairly mixed results here. The normality of residuals is not perfect with many small and high value points not following the line, whereas our homogenerity of variance is pretty good apart from less variation at lower values than larger values.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = weight ~ Time * Diet, family = Gamma(), data = ChickWeight)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.892e-02  4.949e-04  38.234  &lt; 2e-16 ***\nTime        -6.796e-04  2.882e-05 -23.576  &lt; 2e-16 ***\nDiet2       -1.857e-03  7.830e-04  -2.371 0.018065 *  \nDiet3       -2.703e-03  7.457e-04  -3.625 0.000315 ***\nDiet4       -3.463e-03  7.391e-04  -4.685  3.5e-06 ***\nTime:Diet2   4.940e-05  4.484e-05   1.102 0.271037    \nTime:Diet3   4.714e-05  4.208e-05   1.120 0.263050    \nTime:Diet4   1.031e-04  4.249e-05   2.426 0.015570 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.07498304)\n\n    Null deviance: 193.022  on 577  degrees of freedom\nResidual deviance:  42.305  on 570  degrees of freedom\nAIC: 5512.8\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Chick \\;Weight = Gamma(y',\\alpha)\\]\n\\[y'=y^{-1}\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Time:Diet \\;1 \\;vs \\;4\\\\\n+ \\beta_{2} Time:Diet \\;1 \\;vs \\;3\\\\\n+ \\beta_{3} Time:Diet \\;1 \\;vs \\;2\\\\\n+ \\beta_{4} Diet \\;1 \\;vs \\;4\\\\\n+ \\beta_{5} Diet \\;1 \\;vs \\;3\\\\\n+ \\beta_{6} Diet \\;1 \\;vs \\;2\\\\\n+ \\beta_{7} Time\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Gamma distribution requires two shape parameters (\\(y'\\) and \\(\\alpha\\)), where \\(y'\\) must be above zero, we must convert out linear equation results (\\(y\\)) so that it is positive. This means we use the link function, which for Gamma models is by default a inverse. We can use a different link function if we want, for this example we used this default.\n\nglm2$family$link\n\n[1] \"inverse\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age and diet the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average weight based on those ages and diets.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(Time=seq(min(ChickWeight$Time),max(ChickWeight$Time),length.out=100),\n                       Diet=as.factor(c(1:4)))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Time,ymax=Upr,ymin=Lwr,fill=Diet),\n              alpha=0.6)+\n  geom_line(aes(x=Time,y=response,colour=Diet,linetype=Diet))+\n  labs(x=\"Age\",y=\"Predicted Chick Weight (g)\",\n       fill=\"Diet\",colour=\"Diet\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=ChickWeight,aes(x=Time,y=weight,colour=Diet),\n             alpha=0.5,\n             size=0.6)+\n  geom_ribbon(aes(x=Time,ymax=Upr,ymin=Lwr,fill=Diet),\n              alpha=0.6)+\n  geom_line(aes(x=Time,y=response,colour=Diet,linetype=Diet))+\n  labs(x=\"Age\",y=\"Predicted Chick Weight (g)\",\n       fill=\"Diet\",colour=\"Diet\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo again our model seems pretty good, with minimal differences between treatments and strong increase in weight with Age of Chick. But some of this isn’t great, such as very low values being over predicted, plus some clear lines of data well outside the models.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Gamma GLMs"
    ]
  },
  {
    "objectID": "BasicRTutorials/CombiningPlots.html",
    "href": "BasicRTutorials/CombiningPlots.html",
    "title": "Combining Multiple Plots in R",
    "section": "",
    "text": "We have two options for plotting many plots together. The first would be using facetting, which is where you use the same response variables and split your plots across some grouping factor within your data. This can be very useful but is in specific gridded formats of data with each sub plot being the same size. Sometimes we won’t want that, if we are making a selection of plots and combining with images or maps or maybe just not related plots. To do this we can use a wide selection of packages such as cowplot, ggarrange, grid or Patchwork. My personal favourite is patchwork for its simplicity, integration with ggplot2 and its flexibility.\n\n\nFor facetting we will normally be using at least one of the same axes across the plots. For example, we might look at the height to weight association across Starwars characters.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"starwars\")\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"The Empire Strikes Back\", \"Revenge of the Sith\", \"Return…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nLet’s assess height against mass of all the characters in this dplyr dataset.\n\nstarwars %&gt;%\n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\nHmmm I wonder who that heavy thing is? Perhaps we want to look at the different species as different colours?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=species))+\n  theme_classic()\n\n\n\n\n\n\n\n\nHmm this is not that easy to see, and even facetting may not be that great but lets see.\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=species))+\n  theme_classic()+\n  facet_wrap(~species)\n\n\n\n\n\n\n\n\nMaybe we could compare hair colour?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color)\n\n\n\n\n\n\n\n\nAlot of characters without hair, okay lets allow each facet (individual subplot) to have a different y scale.\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color,scales = \"free_y\")\n\n\n\n\n\n\n\n\nThat is better but we could also allow different scales for the x axis too?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color,scales = \"free\")\n\n\n\n\n\n\n\n\nWhat does this look like for eye colour?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=eye_color))+\n  theme_classic()+\n  facet_wrap(~eye_color,scales = \"free\")\n\n\n\n\n\n\n\n\nOkay so none of these plots are very nice as the starwars characters are very well spread in their physical characteristics. Maybe we can group some of these lesser filled groups into “Other”? Then we can do a facet grid with Eye (rows) and Hair (columns) Colours grouped.\n\nstarwars %&gt;% \n  mutate(eye_group=case_when(eye_color%in%c(\"black\",\"brown\",\"dark\",\"red\")~\"Dark Eyes\",\n                             eye_color%in%c(\"blue\",\"blue-gray\",\"gold\",\"green, yellow\",\"hazel\",\"orange\",\"pink\",\"red, blue\",\n                                            \"white\",\"yellow\")~\"Light Eyes\",\n                             TRUE~\"Other\"),\n         hair_group=case_when(hair_color%in%c(\"brown\",\"brown, grey\",\"black\")~\"Dark Hair\",\n                             hair_color%in%c(\"blond\",\"auburn, white\", \"auburn, grey\",\n                                             \"white\",\"grey\",\"auburn\",\"blonde\",\"unknown\")~\"Light Hair\",\n                             TRUE~\"Other\")) %&gt;% \n  drop_na()%&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_group,shape=eye_group))+\n  theme_classic()+\n  facet_grid(eye_group~hair_group,scales = \"free\")\n\n\n\n\n\n\n\n\nNot amazingly illuminating but shows the use of facets. When using facet grid it automatically removes repeated axes.\nLets maybe use a slightly different data set, next will be some mpg data from ggplot2. This data is to do with car mile per gallon and different elements of the engine.\nWe can use facet_wrap() or facet_grid(). we have to put a dot after the ‘~’ if we are only facetting by one column. We will look at the type of drive, which is front wheel drive (f), 4x4 (4) or rear-wheel drive (r).\n\ndata(\"mpg\")\n\nmpg2 &lt;- mpg %&gt;% \n  filter(cyl != 5 & class != \"2seater\")\n\n\n mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy, colour=drv)) + \n  geom_point()+\n   facet_grid(~drv)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n\n\n\n\n\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n   facet_grid(drv~.)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n\n\n\n\n\n\n\nLets do some lines on all these points. For this we can use geom_smooth() that creates a loess model around our points. We can also define the number of columns or rows if we use facet_wrap rather than facet_grid().\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,nrow=2)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,ncol=1)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,ncol=3)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nOkay so maybe we want to look at a couple different associations in our data but without having related axes across the plots. To do this with patchwork we can save each plot as an object then print them. We can use + to add other objects to out ‘patchwork’ and build up layouts with () and /, or for more complex layouts we can use a few methods using the function plot_layout() from patchwork.\n\nlibrary(patchwork)\n\n\np1&lt;-mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\np2&lt;-mpg2 %&gt;% \n  ggplot(aes(x=as.factor(year),fill=class)) + \n  geom_bar(position = \"dodge2\")+\n   labs(x=\"Year\",y=\"Number of Models\")+\n   theme_classic()\n\n\n\n\np1+p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\np1/p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can also reuse plots as you like.\n\n(p1+p1+p2)/p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo to make some fairly complicated plot layouts we can use brackets (), slashes / and pluses +. With a slash denoting a new line.\n\n(p1+p2+p2)/p2/(p2+p1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also use some helper functions to tidy our plots up. If for example we have repeated legends across our plots we can collect our legends or ‘guides’.\n\n(p1+p2+p2)/p2/(p2+p1)+plot_layout(guides=\"collect\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs the plots were made with ggplot2 we can also edit the theme of all of them together. To do this we use an ampersand & in our patchwork layout.\n\n(p1+p2+p2)/p2/(p2+p1)+\n  plot_layout(guides=\"collect\")& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use some more advanced layout options for if for example we don’t want to fill the whole grid space. We create a layout object that has a grid spacing, which we can check by plotting, then we apply that layout to a basic patchwork. We need to use the area function for all the plots we want. We will try plot the same lay out as just above, but without stretching plots that are on a row on their own. In the area() function from patchwork (be careful with other packages with the same name function - you can make sure it is correct by using patchwork::area() ) we have four arguments for the top (t), the left (l), the bottom (b) and the right (r). We can put any non-negative numbers in these to create any array of plots.\n\nlayout&lt;-c(\n  area(t=1,l=1,b=1,r=1),\n  area(t=1,l=2,b=1,r=2),\n  area(t=1,l=3,b=1,r=3),\n  area(t=2,l=2,b=2,r=2),\n  area(t=3,l=1,b=3,r=1),\n  area(t=3,l=3,b=3,r=3)\n)\n\nplot(layout)\n\n\n\n\n\n\n\n\nWe can now apply this layout with plot_layout() to a basic list of added up plots.\n\np1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout)& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs patchwork is happy with a ggplot2 object, we could even combine patchworks if we save one patchwork as a global object and added another ggplot to a new patchwork.\n\npatch&lt;-p1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout)& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n\np1/patch\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nUsing layout and its area function can allow us to create more and more complex arrangements, with and without overlaps. We can also add plot labels to help us refer to the plots in the legend. We can use plot_annotation() with tag_levels=“a” for a, b, c etc or “1” for numbers.\n\nlayout2&lt;-c(\n  area(t=1,l=1,b=5,r=5),\n  area(t=1,l=2,b=1,r=2),\n  area(t=1,l=3,b=1,r=3),\n  area(t=2,l=2,b=2,r=2),\n  area(t=3,l=1,b=3,r=1),\n  area(t=3,l=6,b=5,r=9)\n)\n\nplot(layout2)\n\n\n\n\n\n\n\np1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout2)+\n  plot_annotation(tag_levels = \"a\")& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThese don’t look great now but the concept can be really useful for displaying lots of information and especially when making maps or plots where want to zoom in to a certain region to highlight some element of it. Lets get the lakers data from the lubridate package (it is already loaded in tidyverse). We will look at basketball shots on the court with their x and y cordinates, and whether they were missed or made. Let’s make our first plot and build it up slowly.\n\ndata(\"lakers\")\n\n## Default ggplot facet plot\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;% \nggplot()+\n  geom_point(aes(x=x,y=y),\n             alpha=0.4)+\n  facet_wrap(~result)\n\n\n\n\n\n\n\n## Lets fix coordinates and remove axis info\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;% \nggplot()+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  facet_wrap(~result)+\n  coord_fixed()+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Lets add a Hoop and Make the Legend a bit nicer plus different colours\n## We shall also add a square to show where our zoomed in plot will be.\n## To do this we select only the one shots made facet.\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"), ## We can use a dot to show that we are using the data already in the ggplot\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## we will also add an arrow into the plot to show where the new zoom plot will be.\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(data=. %&gt;% filter(result==\"Shot Made\"), ## We use the same trick from above to only put arrow on one facet.\n               aes(x = 25, y = 45, xend = 25, yend = 50),\n                  arrow = arrow(length = unit(0.5, \"cm\")),\n               colour=\"darkorange\")+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"),\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Okay Lets save this one to our global environment\n\nBigPlot&lt;-lakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(data=. %&gt;% filter(result==\"Shot Made\"), \n               aes(x = 25, y = 45, xend = 25, yend = 50),\n                  arrow = arrow(length = unit(0.5, \"cm\")),\n               colour=\"darkorange\")+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"),\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nSo we have our background plot showing all the data and information, now lets zoom in to the area where shots are being made. Lots of points are overlaid here so we could try look at the density of points spatially to see if there was a pattern in the made shots. We use geom_hex() and apply a log transformation to get a heat map.\n\nlakers %&gt;% \n  filter(etype==\"shot\" & result==\"made\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_hex(aes(x=x,y=y,fill = after_stat(log(count))),bins=30)+\n  scale_fill_viridis_c()+\n  labs(fill=\"Number of Made\\nShots: log(count)\")+\n  coord_fixed(ylim=c(0,NA))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Again lets save it as a global object\n\nZoomedInHex&lt;-lakers %&gt;% \n  filter(etype==\"shot\" & result==\"made\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_hex(aes(x=x,y=y,fill = after_stat(log(count))),bins=30)+\n  scale_fill_viridis_c()+\n  labs(fill=\"Number of Made\\nShots: log(count)\")+\n  coord_fixed(ylim=c(-2,NA))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nFinally lets sort out the layout we want to then combine these two saved plots.\n\nlayout3&lt;-c(\n  area(t=1,l=1,b=20,r=20),\n  area(t=2,l=3,b=8,r=9)\n)\n\nplot(layout3)\n\n\n\n\n\n\n\nBigPlot+ZoomedInHex+\n  plot_layout(design = layout3)+\n  plot_annotation(tag_levels = \"a\")\n\n\n\n\n\n\n\n\nThis is okay, still not great. We shall see if we can make a nice in the next tutorial.",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Combining Multiple Plots in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/CombiningPlots.html#facetting",
    "href": "BasicRTutorials/CombiningPlots.html#facetting",
    "title": "Combining Multiple Plots in R",
    "section": "",
    "text": "For facetting we will normally be using at least one of the same axes across the plots. For example, we might look at the height to weight association across Starwars characters.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"starwars\")\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"The Empire Strikes Back\", \"Revenge of the Sith\", \"Return…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nLet’s assess height against mass of all the characters in this dplyr dataset.\n\nstarwars %&gt;%\n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\nHmmm I wonder who that heavy thing is? Perhaps we want to look at the different species as different colours?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=species))+\n  theme_classic()\n\n\n\n\n\n\n\n\nHmm this is not that easy to see, and even facetting may not be that great but lets see.\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=species))+\n  theme_classic()+\n  facet_wrap(~species)\n\n\n\n\n\n\n\n\nMaybe we could compare hair colour?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color)\n\n\n\n\n\n\n\n\nAlot of characters without hair, okay lets allow each facet (individual subplot) to have a different y scale.\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color,scales = \"free_y\")\n\n\n\n\n\n\n\n\nThat is better but we could also allow different scales for the x axis too?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color,scales = \"free\")\n\n\n\n\n\n\n\n\nWhat does this look like for eye colour?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=eye_color))+\n  theme_classic()+\n  facet_wrap(~eye_color,scales = \"free\")\n\n\n\n\n\n\n\n\nOkay so none of these plots are very nice as the starwars characters are very well spread in their physical characteristics. Maybe we can group some of these lesser filled groups into “Other”? Then we can do a facet grid with Eye (rows) and Hair (columns) Colours grouped.\n\nstarwars %&gt;% \n  mutate(eye_group=case_when(eye_color%in%c(\"black\",\"brown\",\"dark\",\"red\")~\"Dark Eyes\",\n                             eye_color%in%c(\"blue\",\"blue-gray\",\"gold\",\"green, yellow\",\"hazel\",\"orange\",\"pink\",\"red, blue\",\n                                            \"white\",\"yellow\")~\"Light Eyes\",\n                             TRUE~\"Other\"),\n         hair_group=case_when(hair_color%in%c(\"brown\",\"brown, grey\",\"black\")~\"Dark Hair\",\n                             hair_color%in%c(\"blond\",\"auburn, white\", \"auburn, grey\",\n                                             \"white\",\"grey\",\"auburn\",\"blonde\",\"unknown\")~\"Light Hair\",\n                             TRUE~\"Other\")) %&gt;% \n  drop_na()%&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_group,shape=eye_group))+\n  theme_classic()+\n  facet_grid(eye_group~hair_group,scales = \"free\")\n\n\n\n\n\n\n\n\nNot amazingly illuminating but shows the use of facets. When using facet grid it automatically removes repeated axes.\nLets maybe use a slightly different data set, next will be some mpg data from ggplot2. This data is to do with car mile per gallon and different elements of the engine.\nWe can use facet_wrap() or facet_grid(). we have to put a dot after the ‘~’ if we are only facetting by one column. We will look at the type of drive, which is front wheel drive (f), 4x4 (4) or rear-wheel drive (r).\n\ndata(\"mpg\")\n\nmpg2 &lt;- mpg %&gt;% \n  filter(cyl != 5 & class != \"2seater\")\n\n\n mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy, colour=drv)) + \n  geom_point()+\n   facet_grid(~drv)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n\n\n\n\n\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n   facet_grid(drv~.)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n\n\n\n\n\n\n\nLets do some lines on all these points. For this we can use geom_smooth() that creates a loess model around our points. We can also define the number of columns or rows if we use facet_wrap rather than facet_grid().\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,nrow=2)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,ncol=1)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,ncol=3)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Combining Multiple Plots in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/CombiningPlots.html#patchwork",
    "href": "BasicRTutorials/CombiningPlots.html#patchwork",
    "title": "Combining Multiple Plots in R",
    "section": "",
    "text": "Okay so maybe we want to look at a couple different associations in our data but without having related axes across the plots. To do this with patchwork we can save each plot as an object then print them. We can use + to add other objects to out ‘patchwork’ and build up layouts with () and /, or for more complex layouts we can use a few methods using the function plot_layout() from patchwork.\n\nlibrary(patchwork)\n\n\np1&lt;-mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\np2&lt;-mpg2 %&gt;% \n  ggplot(aes(x=as.factor(year),fill=class)) + \n  geom_bar(position = \"dodge2\")+\n   labs(x=\"Year\",y=\"Number of Models\")+\n   theme_classic()\n\n\n\n\np1+p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\np1/p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can also reuse plots as you like.\n\n(p1+p1+p2)/p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo to make some fairly complicated plot layouts we can use brackets (), slashes / and pluses +. With a slash denoting a new line.\n\n(p1+p2+p2)/p2/(p2+p1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also use some helper functions to tidy our plots up. If for example we have repeated legends across our plots we can collect our legends or ‘guides’.\n\n(p1+p2+p2)/p2/(p2+p1)+plot_layout(guides=\"collect\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs the plots were made with ggplot2 we can also edit the theme of all of them together. To do this we use an ampersand & in our patchwork layout.\n\n(p1+p2+p2)/p2/(p2+p1)+\n  plot_layout(guides=\"collect\")& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use some more advanced layout options for if for example we don’t want to fill the whole grid space. We create a layout object that has a grid spacing, which we can check by plotting, then we apply that layout to a basic patchwork. We need to use the area function for all the plots we want. We will try plot the same lay out as just above, but without stretching plots that are on a row on their own. In the area() function from patchwork (be careful with other packages with the same name function - you can make sure it is correct by using patchwork::area() ) we have four arguments for the top (t), the left (l), the bottom (b) and the right (r). We can put any non-negative numbers in these to create any array of plots.\n\nlayout&lt;-c(\n  area(t=1,l=1,b=1,r=1),\n  area(t=1,l=2,b=1,r=2),\n  area(t=1,l=3,b=1,r=3),\n  area(t=2,l=2,b=2,r=2),\n  area(t=3,l=1,b=3,r=1),\n  area(t=3,l=3,b=3,r=3)\n)\n\nplot(layout)\n\n\n\n\n\n\n\n\nWe can now apply this layout with plot_layout() to a basic list of added up plots.\n\np1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout)& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs patchwork is happy with a ggplot2 object, we could even combine patchworks if we save one patchwork as a global object and added another ggplot to a new patchwork.\n\npatch&lt;-p1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout)& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n\np1/patch\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nUsing layout and its area function can allow us to create more and more complex arrangements, with and without overlaps. We can also add plot labels to help us refer to the plots in the legend. We can use plot_annotation() with tag_levels=“a” for a, b, c etc or “1” for numbers.\n\nlayout2&lt;-c(\n  area(t=1,l=1,b=5,r=5),\n  area(t=1,l=2,b=1,r=2),\n  area(t=1,l=3,b=1,r=3),\n  area(t=2,l=2,b=2,r=2),\n  area(t=3,l=1,b=3,r=1),\n  area(t=3,l=6,b=5,r=9)\n)\n\nplot(layout2)\n\n\n\n\n\n\n\np1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout2)+\n  plot_annotation(tag_levels = \"a\")& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThese don’t look great now but the concept can be really useful for displaying lots of information and especially when making maps or plots where want to zoom in to a certain region to highlight some element of it. Lets get the lakers data from the lubridate package (it is already loaded in tidyverse). We will look at basketball shots on the court with their x and y cordinates, and whether they were missed or made. Let’s make our first plot and build it up slowly.\n\ndata(\"lakers\")\n\n## Default ggplot facet plot\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;% \nggplot()+\n  geom_point(aes(x=x,y=y),\n             alpha=0.4)+\n  facet_wrap(~result)\n\n\n\n\n\n\n\n## Lets fix coordinates and remove axis info\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;% \nggplot()+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  facet_wrap(~result)+\n  coord_fixed()+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Lets add a Hoop and Make the Legend a bit nicer plus different colours\n## We shall also add a square to show where our zoomed in plot will be.\n## To do this we select only the one shots made facet.\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"), ## We can use a dot to show that we are using the data already in the ggplot\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## we will also add an arrow into the plot to show where the new zoom plot will be.\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(data=. %&gt;% filter(result==\"Shot Made\"), ## We use the same trick from above to only put arrow on one facet.\n               aes(x = 25, y = 45, xend = 25, yend = 50),\n                  arrow = arrow(length = unit(0.5, \"cm\")),\n               colour=\"darkorange\")+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"),\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Okay Lets save this one to our global environment\n\nBigPlot&lt;-lakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(data=. %&gt;% filter(result==\"Shot Made\"), \n               aes(x = 25, y = 45, xend = 25, yend = 50),\n                  arrow = arrow(length = unit(0.5, \"cm\")),\n               colour=\"darkorange\")+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"),\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nSo we have our background plot showing all the data and information, now lets zoom in to the area where shots are being made. Lots of points are overlaid here so we could try look at the density of points spatially to see if there was a pattern in the made shots. We use geom_hex() and apply a log transformation to get a heat map.\n\nlakers %&gt;% \n  filter(etype==\"shot\" & result==\"made\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_hex(aes(x=x,y=y,fill = after_stat(log(count))),bins=30)+\n  scale_fill_viridis_c()+\n  labs(fill=\"Number of Made\\nShots: log(count)\")+\n  coord_fixed(ylim=c(0,NA))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Again lets save it as a global object\n\nZoomedInHex&lt;-lakers %&gt;% \n  filter(etype==\"shot\" & result==\"made\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_hex(aes(x=x,y=y,fill = after_stat(log(count))),bins=30)+\n  scale_fill_viridis_c()+\n  labs(fill=\"Number of Made\\nShots: log(count)\")+\n  coord_fixed(ylim=c(-2,NA))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nFinally lets sort out the layout we want to then combine these two saved plots.\n\nlayout3&lt;-c(\n  area(t=1,l=1,b=20,r=20),\n  area(t=2,l=3,b=8,r=9)\n)\n\nplot(layout3)\n\n\n\n\n\n\n\nBigPlot+ZoomedInHex+\n  plot_layout(design = layout3)+\n  plot_annotation(tag_levels = \"a\")\n\n\n\n\n\n\n\n\nThis is okay, still not great. We shall see if we can make a nice in the next tutorial.",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Combining Multiple Plots in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/MakingInteractiveMaps.html",
    "href": "BasicRTutorials/MakingInteractiveMaps.html",
    "title": "Making Interactive Maps in R",
    "section": "",
    "text": "One of the main issues with doing mapping in R over other software is our ability to zoom in and out of maps to assess different scales, from whole ocean basin to small harbour? Likewise, when planning fieldwork or even just assessing whether the coordinates you recorded are in the right location with no missing minus signs or wrong decimal location, having a tool you can quickly upload points onto a map that then allows you to zoom in and out on is really handy.",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Making Interactive Maps in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/MakingInteractiveMaps.html#layering",
    "href": "BasicRTutorials/MakingInteractiveMaps.html#layering",
    "title": "Making Interactive Maps in R",
    "section": "Layering",
    "text": "Layering\n\nBase Map\nLike with ggplot2 the first blank arguement creates a blank layer, we can then layer preloaded and local data into the map. Unlike ggplot2 we use the pipe ( %&gt;% ) rather than the plus (+).\n\nleaflet() %&gt;% \n  addTiles()\n\n\n\n\n\nThe base map is zoomed to 2 ish globes and with the standard Open Street Map. We can change these element by choosing some other background tile and setting the zoom. You can find a whole list of all the available basemaps here\n\nleaflet() %&gt;%\n  addProviderTiles(\n    \"OpenTopoMap\"\n  )%&gt;%\n    setView(lng = 6.8652, lat = 45.8326, zoom = 6) ## Location of Mont Blanc\n\n\n\n\n\nWe can set one we like, or we can allow our user to choose. Here we will save the map as m but we also want to plot it too so we can put brackets around the whole thing, this means we can just run the line to save and plot.\n\n(m&lt;-leaflet()%&gt;%\n  addProviderTiles(\n    \"Esri.WorldImagery\",\n    group = \"Esri.WorldImagery\"\n  ) %&gt;%\n  # add different provider tiles\n  addProviderTiles(\n    \"OpenStreetMap\",\n    # give the layer a name\n    group = \"OpenStreetMap\"\n  ) %&gt;%\n  addProviderTiles(\n    \"Stamen.Toner\",\n    group = \"Stamen.Toner\"\n  ) %&gt;%\n  addProviderTiles(\n    \"Stamen.Terrain\",\n    group = \"Stamen.Terrain\"\n  ) %&gt;%\n  addProviderTiles(\n    \"Esri.WorldStreetMap\",\n    group = \"Esri.WorldStreetMap\"\n  ) %&gt;%\n  addProviderTiles(\n    \"Wikimedia\",\n    group = \"Wikimedia\"\n  ) %&gt;%\n  addProviderTiles(\n    \"CartoDB.Positron\",\n    group = \"CartoDB.Positron\"\n  ) %&gt;%\n# add a layers control\n  addLayersControl(\n    baseGroups = c(\n      \"Esri.WorldImagery\",\"OpenStreetMap\", \"Stamen.Toner\",\n      \"Stamen.Terrain\", \"Esri.WorldStreetMap\",\n      \"Wikimedia\", \"CartoDB.Positron\" \n    ),\n    # position it on the topleft\n    position = \"topleft\"\n  ))\n\n\n\n\n\n\n\nLocal Data\n\nPoint Data\nSo lets assume we are interested in the tallest mountains in each continent. Lets make a dataframe then add it to the leaflet map.\n\nMountains&lt;-data.frame(\n  Mountain=c(\"Everest\", \"Aconcagua\", \"Denali\", \"Kilimanjaro\", \"Vinson\", \"Mont Blanc\", \"Mount Wilhelm\"),\n  Longitude=c(86.9250,-70.0109,-151.0070, 37.3556,-85.2135, 6.8652,145.0297),\n  Latitude=c(27.9881,-32.6532,63.0692,-3.0674,-78.6341,45.8326,-5.7800),\n  Height=c(8848,6961,6194,5895,4892,4810,4509),\n  Continent=c(\"Asia\",\"South America\",\"North America\",\"Africa\",\"Antarctica\",\"Europe\",\"Oceania\")\n)\n\nm %&gt;% \n  addMarkers(data=Mountains,lng=~Longitude,lat=~Latitude)\n\n\n\n\n\nThe base markers aren’t very nice so lets add some colour and style plus some info.\n\npal1 &lt;- colorFactor(c(\"navy\", \"red\", \"green\"),\n                   domain = unique(Mountains$Continent))\n\nm %&gt;% \n  addCircleMarkers(data=Mountains,\n                    lng=~Longitude,\n                    lat=~Latitude,\n                    popup=~paste0(Mountain,\" - \",Height),\n                    label=~as.character(Mountain),\n                   color=~pal1(Continent),\n                   fillOpacity = 0.8\n                   )\n\n\n\n\n\n\n\nSpatial Data\nLets download and plot some bathymetry data for the Caribbean and Easter Tropical seas. We need to convert our bathy object to be a SpatRaster from the Terra package (also accepts raster objects but the raster package is being deprecated to switch to terra!!!!) Then we can add a colour palette along the bathymetry data we have. This is then just a couple arguments to make an alright legend.\n\nlibrary(marmap)\n\nRegistered S3 methods overwritten by 'adehabitatMA':\n  method                       from\n  print.SpatialPixelsDataFrame sp  \n  print.SpatialPixels          sp  \n\n\n\nAttaching package: 'marmap'\n\n\nThe following object is masked from 'package:grDevices':\n\n    as.raster\n\nlibrary(terra)\n\nterra 1.7.55\n\nlibrary(tidyterra)\n\n\nAttaching package: 'tidyterra'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nbat_panama &lt;- getNOAA.bathy(lon1=-100,lon2=-54,lat1=32,lat2=-10, res = 1)\n\nQuerying NOAA database ...\n\n\nThis may take seconds to minutes, depending on grid size\n\n\nBuilding bathy matrix ...\n\nBathy_panama &lt;- as.xyz(bat_panama) %&gt;% \n  rename(Longitude=V1,Latitude=V2,Depth=V3) %&gt;% \n  filter(Depth&lt;0) %&gt;% \n  rast()\n\ncrs(Bathy_panama)&lt;-\"+proj=longlat\" \n\npal2 &lt;- colorNumeric(c(\"#0C2C84\", \"#41B6C4\", \"#FFFFCC\"), values(Bathy_panama),\n  na.color = \"transparent\")\n\nleaflet()%&gt;%\n  addTiles()%&gt;%\n  addRasterImage(Bathy_panama,colors=pal2,opacity=0.7) %&gt;%\n  addLegend(pal = pal2, values = values(Bathy_panama),\n    title = \"Depth (m)\")",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Making Interactive Maps in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/MakingMaps.html",
    "href": "BasicRTutorials/MakingMaps.html",
    "title": "Making Static Maps in R",
    "section": "",
    "text": "Okay so I have strong and potentially controversial opinions on using GIS software. While very useful, user friendly tools, just like Excel, I strongly argue against their use for scientific analysis (I have caveats to this). The main pillars of science, in my opinion, are increasing societies knowledge in an open, repeatable, replicate-able and critique-able way. Therefore, using software that is paid for or not open access is a big no for me. Likewise, software that is just lots of clicking buttons, without a full transcript of the process being carried out, doesn’t allow full repeatablility. This may be repetition by other researchers to check your results, emulate your analysis method on another dataset or it may be repetition by future you or me! Another major element of using a coding language to carry out tasks is scaleability. If I make a map in a GIS software, to make the same map with a few changes will always take a similar amount of time, doing so in r (or another coding language) will decrease the time per map through looping or functional programming. I am also a lazy scientist, I can use R to carry out all elements of my research, so why learn another program? (I do use other languages from time to time actually but still)\n\n\n\n\n\nSpatial data is basically the same as any other data type but it has some information about its location, and generally it is stored in two major types: Raster or Shape. Raster data are gridded with repeating units or pixels, with each pixel having x and y coordinates and some data value(s). Shape data are generally types of irregular polygons or points.\nThankfully for us we can use ggplot2 and its functionality to plot a wide range of shapes and rasters, as well as combinations of them.\n\n\n\n\n\n\n\nSometimes this is all we want and can be many different types: satellite image, coastline, road map. Generally, it gives us the spatial context for other elements we want to display.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondly we want the elements we are going to display inside our geographical context, this could be bathymetry, sample sites, labels etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we want to add other info elements like scale bars, north arrows, data sources and generally improve the appearance.\n\n\n\n\n\n\n\n\n\n\nWith ggplot we can layer all these elements on top of each other in the order we want.",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Making Static Maps in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/MakingMaps.html#step-1---background-map-1",
    "href": "BasicRTutorials/MakingMaps.html#step-1---background-map-1",
    "title": "Making Static Maps in R",
    "section": "Step 1 - Background Map",
    "text": "Step 1 - Background Map\nHere we need the data itself, then we want to plot it in a projection that makes sense for our use. Atlantic centric? Pacific centric? But depending on some projections we may need to do some extra work for it to look nice.\n\nData Download\nOften we will have our own shape or raster files locally, but there are also a wide range of easily accessible data for different forms of data.\nIn this tutorial we will be using readily available open access data, specifically the rnaturalearth shape files of countries to begin with.\nTo organise and edit we will use the sf package for shape files and the terra package for rasters. There are many others, such as sp, raster and maptools etc. However, they are being deprecated soon and recommend using sf or terra. There are also many supplements to these packages, when we need them we will install and use them.\n\n\nWorld Coastlines\nThe rnaturalearth packages have preloaded data that we can access for country polygons at different spatial scales, lets look at the middle scale. If we load the basic ne_countries() data set and tell the function we want it as an ‘sf’ object we can plot it using the base plot. However, as there are many columns other than the spatial information the base plot() function plots each separately.\n\nlibrary(tidyverse) \nlibrary(sf) \nlibrary(rnaturalearth) \nlibrary(rnaturalearthdata) \n#remotes::install_github(\"ropensci/rnaturalearthhires\")\nlibrary(rnaturalearthhires) \n\nworld_map &lt;- ne_countries(returnclass = \"sf\",scale = 50) \n\nplot(world_map)\n\nWarning: plotting the first 9 out of 63 attributes; use max.plot = 63 to plot\nall\n\n\n\n\n\n\n\n\n\nHere we have a world_map object in our global environment, and generally it seems just like a normal dataframe, with columns and rows of data. An sf object has extra information in a column called geometry. This is information on the type of shape (Point, Line or Polygon) for each row in the dataframe. Here we have a row with a polygon or group of polygons for each country, alongside lots of info on its administration and population etc. As a dataframe we can easily inspect the data we have and use the same techniques of data manipulation we have used before such as mutating, selecting, summarising, grouping etc.\nLets take a glance at the top of the first 6 columns:\n\nhead(world_map[c(1:6)])\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -70.06611 ymin: -18.01973 xmax: 74.89131 ymax: 60.40581\nGeodetic CRS:  +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n  scalerank      featurecla labelrank     sovereignt sov_a3 adm0_dif\n0         3 Admin-0 country         5    Netherlands    NL1        1\n1         1 Admin-0 country         3    Afghanistan    AFG        0\n2         1 Admin-0 country         3         Angola    AGO        0\n3         1 Admin-0 country         6 United Kingdom    GB1        1\n4         1 Admin-0 country         6        Albania    ALB        0\n5         3 Admin-0 country         6        Finland    FI1        1\n                        geometry\n0 MULTIPOLYGON (((-69.89912 1...\n1 MULTIPOLYGON (((74.89131 37...\n2 MULTIPOLYGON (((14.19082 -5...\n3 MULTIPOLYGON (((-63.00122 1...\n4 MULTIPOLYGON (((20.06396 42...\n5 MULTIPOLYGON (((20.61133 60...\n\n\n\nEnter ggplot2\nLets plot it using ggplot instead. Thankfully the sf package has its own geom (geom_sf()), this means we don’t have to give it too much information for it to do something pretty good.\n\nggplot(world_map)+\n  geom_sf()\n\n\n\n\n\n\n\n\nThere are some obvious issues with this map but generally it is a pretty good starting point.\n\n\n\nCoordinate Reference Systems and Projections\nHere we are looking at a map of a three dimensional sphere projected onto a flat surface. To do this we use different map projections, which can lead to interesting effects by distorting apparent size, such as Antarctica being long and thin at the base of this map, when really it is a circle shape around the south pole. We can easily change between CRS using sf functions. While previously CRS was denoted with quite a long string, now there are easy to use 4 digit EPSG codes. You can find loads here: https://spatialreference.org/ref/epsg/\nEPSG codes relate to preset or defined CRS strings. So we can use either EPSG codes or we can be more specific with a CRS we want to use e.g.: “+proj=laea +lat_0=0 +lon_0=-30 +x_0=43210000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs”. This string states the projection, the centre lat, centre lon, the centre x, centre y, the ellipsoid (type of sphere the globe is assumed to be), the conversion from WGS 84, the units, and other info. For the standard latlong projection that is often used we would have: “+proj=longlat +datum=WGS84 +no_defs”\nFor example here is the same map reprojected to Mercator:\n\nworld_map %&gt;%\nst_transform(crs=3857) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nBut we can see a BIG issue with this projection. It makes greenland look like the size of africa, and it also makes antarctica the size of the sun (not quite)! As it is a ggplot we can crop out the bottom of the graph instead.\n\nworld_map %&gt;%\nst_transform(crs=3857) %&gt;% \nggplot()+\n  geom_sf()+\n  coord_sf(ylim = c(-20000000,NA))+\n  theme_classic()\n\n\n\n\n\n\n\n\nThe mercator uses metres as its units so we have to use those units for setting limits. Lets see some other systems.\n\nworld_map %&gt;%\nst_transform(crs=3978) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\nworld_map %&gt;%\nst_transform(crs=32190) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\nworld_map %&gt;%\nst_transform(crs=26917) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nAs you can see, all of these are horrible. By reprojecting the sf file we are causing the polygons to become glitchy and distorted. It seems some issue is being caused by Antarctica. Perhaps if we remove it it will look better?\n\nworld_map %&gt;%\n  filter(!sovereignt==\"Antarctica\") %&gt;% \nst_transform(crs=3978) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay no it does not.\n\n\nDistortions\nThese projections have inherent edges to their projection (the far left or far right of the plotted map) which will be latitude or longitude values, they will also have the centre position. The areas of the map that are furthest from the centre, around the edges will become distorted when plotting on a 2D plane: think about looking at a 3D globe and trying to see the countries on the other side from you. When transforming and then plotting, a polygon of a country (the shape that country makes) will potentially get divided byt the edge of the projection area. Thus, we get these weird glitchy distortions. There are ways around having these distortions but it involves creating a really thin polygon along the edges of the original map, then deleting this thin polygon. This splits the polygons that will be split when changing the transformation. We can visualise this if we want.\nOriginal Map and transformed into desired CRS.\n\nworld_map %&gt;%\nst_transform(crs=4326) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\nworld_map %&gt;%\nst_transform(crs=st_crs(\"+proj=cea +lon_0=130 +x_0=0 +y_0=0 +lat_ts=45 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo in our desired projection we can see the the central longitude is 130. We therefore need to make a slim polygon opposite 130 on the globe! (180-130)\n\nworld&lt;-world_map %&gt;%\nst_transform(crs=4326)%&gt;%\n  st_make_valid()\n\n\n# define a long & slim polygon that overlaps the meridian line & set its CRS to match\n# that of world\n\n# Centered in lon 130 on this example\n\noffset &lt;- 180 - 130\n\n\npolygon &lt;- st_polygon(x = list(rbind(\n  c(-0.0001 - offset, 90),\n  c(0 - offset, 90),\n  c(0 - offset, -90),\n  c(-0.0001 - offset, -90),\n  c(-0.0001 - offset, 90)\n))) %&gt;%\n  st_sfc() %&gt;%\n  st_set_crs(4326)\n\nLets plot the slim polygon on the original map.\n\nworld_map %&gt;%\nst_transform(crs=4326) %&gt;% \nggplot()+\n  geom_sf()+\n  geom_sf(data=polygon,fill=\"red\",colour=\"red\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow we have this thin polygon we can remove this slim area from the original world map.\n\nworld_fixed &lt;- world %&gt;% \n  st_difference(polygon)\n\ntarget_crs&lt;-st_crs(\"+proj=cea +lon_0=130 +x_0=0 +y_0=0 +lat_ts=45 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n\nworld_fixed &lt;- world_fixed %&gt;% \n  st_transform(target_crs)\n\n\nggplot(data = world_fixed) +\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nHere there are still some odd looking shapes e.g. Papua New Guinea. Maybe we can find a better projection. Maybe we can use one that looks a little 3D even when in 2D?\n\nworld_map %&gt;% \n  st_transform(\"+proj=laea +lat_0=0 +lon_0=-30 +x_0=43210000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\") %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis projection is a favourite of mine, I used it above for the map of Europe. The choice of mapping projection will be dictated by the scale you want to show and on your own personal preference.",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Making Static Maps in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/MakingMaps.html#step-2---spatially-explicit-details-1",
    "href": "BasicRTutorials/MakingMaps.html#step-2---spatially-explicit-details-1",
    "title": "Making Static Maps in R",
    "section": "Step 2 - Spatially Explicit Details",
    "text": "Step 2 - Spatially Explicit Details\nOkay so we have a global map we will use to be our Base Map, it isn’t perfect by any means but it is fine for now. Now lets plot some spatially explicit data. As a Marine Ecologist with a dark Oceanography past, I absolutely LOVE bathymetry plots so lets get some bathymetry and try make a nice Bathymetry plot of the Atlantic. We may have our own bathymetry data, or we could go to some website, download it and read into r, or we can download straight into r using the marmap package. This gets bathymetry data from the NOAA database. (Or GEBCO if you prefer)\nTo do this we use the getNOAA.bathy() and tell the function the longitude limits, then latitude limits, then the resolution. If our desired data cross the antimeridian (the longitude line 180 and -180) we can tell the function this too. This function creates a bathy object, but we can convert this to a data frame with as.xyz(). Remember when we set a big area and a fine resolution value the file gets bigger so will take longer to download and to plot. We shall also remove Depth values that are positive so we only get Depth. (This may not be ideal for inland areas that are below 0, such as the eastern coast of the UK).\n\n#install.packages(\"marmap\")\n\nlibrary(marmap)\n\n\n# Get bathymetric data\nbat &lt;- getNOAA.bathy(lon1=-90,lon2=40,lat1=70,lat2=-60, res = 4)\n\nbat_xyz &lt;- as.xyz(bat) %&gt;% \n  rename(Longitude=V1,Latitude=V2,Depth=V3) %&gt;% \n  filter(Depth&lt;1)\n\n\nggplot(bat_xyz)+\n  geom_tile(aes(x=Longitude,y=Latitude,fill=Depth))+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay so now we have some regular gridded bathymetry data but it isn’t the same projection we have our base map in, so if we try plot together it will be rather wrong. Infact on the global scale our bathymetry data which range from -90 to 40 in longitude and 70 to -60 in latitude are not even visible on a map using this projection.\n\nProjection3D&lt;-\"+proj=laea +lat_0=0 +lon_0=-30 +x_0=43210000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\"\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_sf()+\n  geom_tile(data=bat_xyz,aes(x=Longitude,y=Latitude,fill=Depth))+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo what we will do is convert the dataframe to a spatial object. We could use sf to do this. Then we can easily transform it to the projection we want! To do this we can use the st_as_sf() function. One issue is we will need to tell sf what projection our latitude and longitude data are. Thankfully getNOAAA.bathy() uses EPSG:4326.\n\nBathy_sf&lt;-st_as_sf(bat_xyz,coords = c(\"Longitude\",\"Latitude\"),crs=4326) %&gt;%  \n  st_transform(Projection3D)\n  \n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_sf()+\n  geom_sf(data=Bathy_sf,aes(fill=Depth))+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis doesn’t look like it has worked, what is happening is that each row of the sf is thought of as a point, ggplot is then plotting those points. It would be better for speed of plotting and how the plot looks if we use a raster format, as this is how the data are laid out. We can use the terra package to do this.\n\n#install.packages(\"terra\")\n#install.packages(\"tidyterra\")\n\nlibrary(terra)\nlibrary(tidyterra)\n\nBathy_terra&lt;-as_spatraster(bat_xyz,xycols = c(1:2),crs=4326) %&gt;%  \n  project(Projection3D)\n  \n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_sf()+\n  geom_spatraster(data=Bathy_terra,aes(fill=Depth))+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay, this is good, the projection is correct and the detail is enough for our current zoom level. The main issue now seems to be the NA values in the spat raster being plotted in a grey colour. So lets plot this with a nicer colour palette, and maybe we want our map lines to be on top of the bathymetry?\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_spatraster(data=Bathy_terra,aes(fill=Depth))+\n  geom_sf()+\n  scale_fill_viridis_c(na.value = NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nHmmm maybe it would be nicer with a full global bathymetry? Or we could zoom into the area we have bathymetry for? Or both?\n\nbat_whole &lt;- getNOAA.bathy(lon1=-180,lon2=180,lat1=90,lat2=-90, res = 10)\n\nBathy_Whole_World &lt;- as.xyz(bat_whole) %&gt;% \n  rename(Longitude=V1,Latitude=V2,Depth=V3) %&gt;% \n  filter(Depth&lt;1) %&gt;% \n  as_spatraster(xycols = c(1:2),crs=4326) %&gt;%  \n  project(Projection3D)\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_spatraster(data=Bathy_Whole_World,aes(fill=Depth))+\n  geom_sf()+\n  scale_fill_viridis_c(na.value = NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nLets also make some labels of points of interest, and lets zoom in on the mid atlantic ridge because it is very cool! Always have to refer to Marie Tharp for her magnificent, literally world changing work on mapping and effectively discovering this region of the ocean!\nAs our projection works in metres it would be a lot of trial and error to get the correct values, but we can create an sf object with the latitude and longitude we want then transform it and use that to create our limits! I will just look up some islands and archipelagoes and their Latitudes and Longitudes and add them as labels with the ggforce package function geom_mark_ellipse().\n\n#install.packages(\"sfheaders\")\n\nlibrary(sfheaders)\nlibrary(ggforce)\n\n\nCrop_MAR&lt;-data.frame(\n  lon = c(-80,50,-80,50),  \n  lat = c(-45,-45,45,45)\n) %&gt;% \n  st_as_sf(coords=c(\"lon\",\"lat\"), crs = 4326) %&gt;% \n  st_transform(Projection3D) %&gt;% \n  sf_to_df(fill=T)\n\nInterestingPoints&lt;-data.frame(\n  lon = c(-27.862,-14.3737,-15.7315,-16.751,-64.7896,\n          -23.777,-32.425,6.739,-5.705,-12.2821,-37.05,\n          -59.364,51.741,-59.5463),  \n  lat = c(38.723,-7.9481,28.620,32.860,32.352,15.9495,\n          -3.8543,0.4535,-15.9697,-37.115,-54.403,-51.772,\n          -46.4046,13.102),\n  Islands= c(\"Azores\",\"Ascension\",\"Canaries\",\"Madeira\",\n             \"Bermuda\",\"Cabo Verde\",\"Fernando de Noronha\",\n             \"São Tomé and Príncipe\",\"Saint Helena\",\"Tristan da Cunha\",\n             \"South Georgia\",\"Islas Malvinas\",\"Possession\",\"Barbados\")\n) %&gt;% \n  st_as_sf(coords=c(\"lon\",\"lat\"), crs = 4326) %&gt;% \n  st_transform(Projection3D) %&gt;% \n  sf_to_df(fill=T)\n\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_spatraster(data=Bathy_Whole_World,aes(fill=Depth))+\n  geom_sf()+\ngeom_mark_ellipse(data=InterestingPoints,\n               aes(x=x,\n                   y=y,\n                   label = Islands,\n                   group=Islands))+\n  scale_fill_viridis_c(na.value = NA)+\n  coord_sf(xlim=c(min(Crop_MAR$x),max(Crop_MAR$x)),\n           ylim=c(min(Crop_MAR$y),max(Crop_MAR$y)))+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Making Static Maps in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/MakingMaps.html#step-3---appearance-1",
    "href": "BasicRTutorials/MakingMaps.html#step-3---appearance-1",
    "title": "Making Static Maps in R",
    "section": "Step 3 - Appearance",
    "text": "Step 3 - Appearance\nAppearance is very down to personal preference but is all just using ggplot theme and scale elements. For a map this big i think a scale bar is unneccesary (but we will put a small one in anyway) and in my opinion a North arrow is never useful (except for navigational maps), as having Latitude on the y axis and Longitude on the x axis removes its utility.\nHere I add nicer legend labels. Again it is personal preference but Depth in my opinion should be positive going deeper. I also add a scale bar, although with this projection it will change how big it should be for different areas of the map. I change the colour scheme of the bathymetry to be a blue palette. Then I add Labels for x and y, and a caption of data source. I add some grid lines to show latitude and longitude lines with theme().\n\n#install.packages(\"ggsn\")\n\nlibrary(ggsn)\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_spatraster(data=Bathy_Whole_World,aes(fill=Depth), maxcell = 5005560)+\n  geom_sf(linewidth=0.1,alpha=0.9,\n          fill=\"palegreen4\",colour=\"grey30\")+\ngeom_mark_ellipse(data=InterestingPoints,\n               aes(x=x,\n                   y=y,\n                   label = Islands,\n                   group=Islands) ,\n               show.legend=F,\n               alpha=0.8,\n               label.buffer = unit(1, \"mm\"),\n               label.fill = \"grey80\")+\n  scale_fill_gradientn(colours=c(\"#5e24d6\",\"#22496d\",\"#042f66\",\"#054780\",\"#1074a6\",\n                                \"#218eb7\",\"#48b5d2\",\"#72d0e1\",\"#9ddee7\",\"#c6edec\"),\n                       breaks=c(0,-2500,-5000,-7500),\n                       labels=c(\"0\",\"2,500\",\"5,000\",\"7,500\"),\n                       na.value = NA)+\n  coord_sf(xlim=c(min(Crop_MAR$x),max(Crop_MAR$x)),\n           ylim=c(min(Crop_MAR$y),max(Crop_MAR$y)))+\n  scalebar(world_map %&gt;%  st_transform(Projection3D),\n           dist = 1500, dist_unit = \"km\", \n                 transform=FALSE, \n                 st.dist=0.008,\n                 height=0.01,\n                 location=\"bottomright\",\n                 anchor=c(x=min(Crop_MAR$x)*1.07,\n                          y=max(Crop_MAR$y)*0.25),\n                 border.size = 0.8)+\n  labs(x=\"Longitude\",y=\"Latitude\", \n       caption = \"Data Source: National Oceanic and Atmospheric Administration (NOAA)\",\n       title = \"Bathymetry of the Mid-Atlantic Ridge\",fill=\"Depth (m)\")+\n  theme(panel.background = element_blank(), # bg of the panel\n    panel.grid.major = element_line(linetype = \"dotted\",\n                                             colour=\"grey30\",\n                                             linewidth=0.25),\n    panel.ontop = TRUE,\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(colour = \"black\", fill=NA, linewidth=1))",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Making Static Maps in R"
    ]
  },
  {
    "objectID": "consultancy_Desktop.html",
    "href": "consultancy_Desktop.html",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\n\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects."
  },
  {
    "objectID": "consultancy_Desktop.html#statistical-experience",
    "href": "consultancy_Desktop.html#statistical-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\n\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects."
  },
  {
    "objectID": "consultancy_Desktop.html#consultancy-fieldwork-experience",
    "href": "consultancy_Desktop.html#consultancy-fieldwork-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "Consultancy Fieldwork Experience",
    "text": "Consultancy Fieldwork Experience\n\n\nI have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp."
  },
  {
    "objectID": "posts2/Everest.html",
    "href": "posts2/Everest.html",
    "title": "Reaching New Heights in Plastic Pollution—Preliminary Findings of Microplastics on Mount Everest",
    "section": "",
    "text": "Napper et al., 2020\nMount Everest was once a pristine environment. However, due to increased tourism, waste is accumulating on the mountain, with a large proportion being made of plastic. This research aimed to identify and characterize microplastic (MP) pollution near the top of highest mountain on Earth and could illustrate the implications for the environment and the people living below. Stream water and snow were collected from multiple locations leading up to, and including, the Balcony (8,440 m.a.s.l). MPs were detected at an ~30 MP L\\(^−1\\) in snow and ~1 MP L\\(^−1\\) in stream water, and the majority were fibrous. Therefore, with increased tourism, deposition of MP near Mt. Everest is expected to rise. At a pivotal point in the exploration of remote areas, environmental stewardship should focus on technological and other advances toward minimizing sources of MP pollution."
  },
  {
    "objectID": "posts2/BrentGoose.html",
    "href": "posts2/BrentGoose.html",
    "title": "Remote sensing in seagrass ecology: coupled dynamicsbetween migratory herbivorous birds and intertidalmeadows observed by satellite during four decades",
    "section": "",
    "text": "Zoffoli et al., 2021\nTaking into account trophic relationships in seagrass meadows is crucial toexplain and predict seagrass temporal trajectories, as well as for implementingand evaluating seagrass conservation policies. However, this type of interactionhas been rarely investigated over the long term and at the scale of the wholeseagrass habitat. In this work, reciprocal links between an intertidal seagrassspecies,Zostera noltei, and a herbivorous bird feeding on this seagrass species,the migratory gooseBranta bernicla bernicla, were investigated using an originalcombination of long-term Earth Observation (EO) and bird census data. Sea-grass Essential Biodiversity Variables (EBVs) such as seagrass abundance andphenology were measured from 1985 to 2020 using high-resolution satelliteremote sensing over Bourgneuf Bay (France), and cross-analysed within situmeasurements of bird population size during the goose wintering season. Ourresults showed a mutual relationship between seagrass and Brent geese over thefour last decades, suggesting that the relationship between the two speciesextends beyond a simple grass—herbivore consumptive effect. We provided evi-dence of two types of interactions: (i) a bottom-up control where the late-summer seagrass abundance drives the wintering population of herbivorousgeese and (ii) an indirect top-down effect of Brent goose on seagrass habitat,where seagrass development is positively influenced by the bird population dur-ing the previous wintering season. Such a mutualistic relationship has strongimplications for biodiversity conservation because protecting one species isbeneficial to the other one, as demonstrated here by the positive trajectoriesobserved from 1985 to 2020 in both seagrass and bird populations. Importantly,we also demonstrated here that exploring the synergy between EO andin situbird data can benefit seagrass ecology and ecosystem management."
  },
  {
    "objectID": "posts2/Ganges_2.html",
    "href": "posts2/Ganges_2.html",
    "title": "The Distribution and Characterisation of Microplastics in Air, Surface Water and Sediment within a Major River System.",
    "section": "",
    "text": "Napper et al., 2023\nRivers are key pathways for the transfer of microplastics (MP) to marine environments. However, there are considerable uncertainties about the amount of microplastics transported by rivers to the ocean; this results in inaccuracies in our understanding of microplastic quantity and transport by freshwater systems. Additionally, it has been suggested that rivers may represent long-term sinks, with microplastics accumulating in sediment due to their high density or other biological, chemical, and physical factors. The atmosphere is also an important pathway by which airborne microplastics may enter aquatic habitats. Here, we compare for first time microplastics type and concentration in these key environmental mediums (air, water and sediment) along a major river (Ganges), from sea to source to understand 1) the abundance, 2) the spatial distribution, and 3) characteristics. Mean microplastic abundance settling from the atmosphere was 41.12 MP m\\(^2\\) day\\(^{−1}\\); while concentrations in sediment were 57.00 MP kg\\(^{−1}\\) and in water were 0.05 MP L\\(^{−1}\\). Across all sites and environmental mediums, rayon (synthetically altered cellulose) was the dominant polymer (54–82 %), followed by acrylic (6–23 %) and polyester (9–17 %). Fibres were the dominant shape (95–99 %) and blue was the most common colour (48–79 %). Across water and sediment environmental mediums, the number of microplastics per sample increased from the source of the Ganges to the sea. Additionally, higher population densities correlated with increased microplastic abundance for air and water samples. We suggest that clothing is likely to be the prominent source of microplastics to the river system, influenced by atmospheric deposition, wastewater and direct input (e.g. handwashing of clothes in the Ganges), especially in high density population areas. However, we suggest that subsequent microplastic release to the marine environment is strongly influenced by polymer type and shape, with a large proportion of denser microplastics settling in sediment prior to the river discharging to the ocean."
  },
  {
    "objectID": "posts2/Mussels.html",
    "href": "posts2/Mussels.html",
    "title": "The restoration potential of offshore mussel farming on degraded seabed habitat",
    "section": "",
    "text": "Bridger et al., 2022\nThe United Kingdom’s first large-scale, offshore, long-line mussel farm deployed its first ropes in 2013 in Lyme Bay, southwest United Kingdom, located in an area of seabed that was heavily degraded due to historic bottom-towed fishing. It was hypothesised that due to the artificial structures that accumulate mussels and exclude destructive fishing practices, the seabed could be restored. To assess the restoration potential of the farm and its ecosystem interactions over time, a multi-method, annual monitoring approach was undertaken. Here, we tested the effects of the farm trial stations on the seabed habitat, epifauna and demersal species over 5 years. Responses of % mussel cover, sessile and sedentary, and mobile taxa were measured using three video methods. Within 2 years of infrastructure deployment, mussel clumps and shells were detected below the headlines, increasing the structural complexity of the seabed. After 4 years, there was a significantly greater abundance of mobile taxa compared to the Controls that remained open to trawling. Commercial European lobster and edible crab were almost exclusively recorded within the farm. We discuss whether these findings can be considered a restoration of the seabed and how these data can be used to inform the future management of offshore mariculture globally."
  },
  {
    "objectID": "blogs/GreenlandTale.html",
    "href": "blogs/GreenlandTale.html",
    "title": "A Greenlandic Tale",
    "section": "",
    "text": "During the summer of 2023 I was invited to be a benthic taxonomist on board the research vessel, Tarajoq, while it undertook fishery surveys of Atlantic Cod and Shrimp for the Greenland Institute of Natural Resources. The opportunity arose through a colleague from the University of Plymouth and was perfectly timed alongside my duties as a post-doctoral researcher at the University of Nantes. In this post I would have many learning experiences, increase my knowledge of fisheries, provide assistance to the annual monitor efforts of Greenland and develop international collaborative connections.",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july",
    "href": "blogs/GreenlandTale.html#th-july",
    "title": "A Greenlandic Tale",
    "section": "10th July",
    "text": "10th July\nToday marks the first day of what I expect will be a spectacular trip. My friend Simon dropped me at the airport with good time. Yet, all the normal anxieties and doubts of going through airport security gripped me, but, as always, there were no issues. We took off from Nantes in glorious sunshine with a gleam of perspiration coating my forehead. This being partly due to my anxiety at flying and partly due to dressing for the Copenhagen climes. It was an easy short flight with quite special views across the Wadden Sea dotted with islets, banks and channels. The clear views and smooth flight quite rapidly turned into a juddering uneasy descent into the city. The approach into Copenhagen, similar to that of Dublin, Hong Kong or Vancouver, seems to be straight into the sea with the runway appearing at the very last second. Once through the airport at the other end the walk to the hotel, which I have been booked into, could not have been easier. Having freshened up and deposited my luggage I quickly turned around and was on the metro into town. I have little to no knowledge of the city but had been given one recommendation. Therefore, I headed straight for “The Little Mermaid” known locally as “Den Lille Havfrue”.\n\n\n\nDen Lille Havfrue - The Little Mermaid\n\n\nIt is a bronze sculpture of a young lady sat in a side-saddle style on a rock on the coast. The ripples of the boats’ wakes brush her feet while the tourists pose, and the pigeons pray on the treats of the unsuspecting crowd. Being a working port, Copenhagen has the industrial concrete factories overlooking the water, while also containing a plethora of quaint small wood or red brick façade houses. This quick sojourn in the Danish capital was an added bonus. The real journey starts tomorrow.",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july-1",
    "href": "blogs/GreenlandTale.html#th-july-1",
    "title": "A Greenlandic Tale",
    "section": "12th July",
    "text": "12th July\nThe approach to Narsarsuaq was the most breathtaking view I have ever seen. Flying in we descended through the low clouds that stretched over the vast north of Greenland. As far as the eye could see were pure white glaciers that gave sharp contrast to the dark crags of rock, towering above even us on our descent. The landing approach brought us gliding in over the fjord where my new home, Tarajoq, sat waiting.\n\n\nDescent into Narsarsuaq\nDescent into Narsarsuaq\n\n\nI had never seen glaciers or icebergs up close before but already within less than 24 hours I had become desensitised. Being formed by glacial movement, I see strong similarities between the Greenland landscape and the glens of the Scottish Highlands. We were alongside (in the harbour) at Narsaq waiting for the last members of the crew to join us. During this time I had the not-so-minor task of familiarising myself with Greenland Fauna.\n\n\nApproaching Tarajoq through icebergs\nApproaching Tarajoq through icebergs\n\n\n\n\n\n\n\n\n\n\n\nRV Tarajoq\n\n\n\n\n\n\n\nIcebergs in the Harbour at Narsaq\n\n\n\n\n\n\n\nIceberg in the Greenlandic Fjords",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july-2",
    "href": "blogs/GreenlandTale.html#th-july-2",
    "title": "A Greenlandic Tale",
    "section": "16th July",
    "text": "16th July\nLife on board Tarajoq was now starting to fall into a routine, the most breath-taking, stunning and surreal routine but not without numbing fatigue at the end of each shift. The cruise personnel were split into cooks (most important), ‘fish people’ (fisheries scientists), benthos people (us), deck crew, mechanics, helmsmen, skipper and cruise leader. We sailed up the east coast of Greenland, going from predetermined station to predetermined station. At each, we carried out a fishery trawl. All personnel were split onto shifts, so, as a whole, we fished 24 hours a day. My shift was midday to midnight and alongside the fish people we sorted through each catch.\n\n\n\n\n\n\n\n\n\nCute Little (Cottunculus microps)\n\n\n\n\n\n\n\nSorting Different Species of Fish\n\n\n\n\n\n\n\n\n\nA Scary Deep Sea Fish (Caulophryne jordani)\n\n\n\n\n\n\n\nDifferent sizes of Deep Sea Fish (Careproctus reinhardti)\n\n\n\n\n\n\n\nA Rather Large and Bitey Wolf Fish\n\n\n\n\n\nThe fish people took the fish and we took everything else. After a few days the wee beasties we caught were mostly becoming known to me, meaning our job of sorting, identifying and counting was becoming easier. While a lot of what we caught was similar between trawls we were starting to get mind-blowingly cool species and specimens too.\n\n\n\n\n\n\n\n\n\nIntricate Basket Star (Gorgonocephalus sp.)\n\n\n\n\n\n\n\nSelection of Many Armed Starfish (Crossaster sp.)\n\n\n\n\n\n\n\n\n\nA Phakellia Sponge plus Me\n\n\n\n\n\n\n\nExample Tray of Benthic Specimens\n\n\n\n\n\n\n\nSea Spider (Pycnogonida sp.)\n\n\n\n\n\nSo far, we had caught and released with a tag, 3 Greenland sharks. At over 4 metres long they will all have been very old. When I say old, I cannot be sure, no one is, but it is likely they are older than the USA, maybe even alive during the Tudor times in the UK. Other amazing specimens of fish had also turned up, including multiple 30 + kg Cod, Halibut, Pollack and Rays.\n\n\n\n\n\n\n\n\n\nA Massive Greenlandic Shark (Somniosus microcephalus)\n\n\n\n\n\n\n\n\n\nA 32 kg Cod (Gadus morhua)\n\n\n\n\n\n\n\nA 28 kg Halibut (Hippoglossus hippoglossus)\n\n\n\n\n\n\n\nA 20 kg Pollack (Polliachus virens)\n\n\n\n\n\n\n\n\n\nA 24 kg Flapper Skate (Dipturus batis)\n\n\n\n\n\nOnce my shift was done, especially after processing 700kg of sponge by hand, I was in need of a cold dip. While swimming in the sea was out of the question, we did have a huge dunk tank and a tap that drew sea water at around 4°C from the water around us. This allowed cold dips before turning in for the night.\n\n\n\n\n\n\n\n\n\nA Trawl full of Sponges (Geodia sp.)\n\n\n\n\n\n\n\nMy “Swimming Pool”\n\n\n\n\n\nWe were skirting around but also through vast ice flows that were both spectacular to see in the day and terrifying to hear in the night, as we smashed our path through them.\n\n\nThe Bow of Tarajoq Gliding through the Water.\nThe Bow of Tarajoq Gliding through the Water.",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html#nd-july",
    "href": "blogs/GreenlandTale.html#nd-july",
    "title": "A Greenlandic Tale",
    "section": "22nd July",
    "text": "22nd July\nThe routine continued: wake up; take a multivitamin; grab a coffee and head to the bridge; admire either the beautiful vistas or rolling waves, whale blows, icebergs and Greenlandic mountains or feel the chill of fear looking out into impenetrable fog with icebergs looming out of the nothingness; then it would be time to eat before descending to the factory to sort, identify and count all species we caught within the trawl.\n\n\n\n\n\n\n\n\n\nBoat Lights Searching Through the Fog.\n\n\n\n\n\n\n\nImpenetrable Fog in the Middle of the Day\n\n\n\n\n\nAs we headed north along the eastern shelf of Greenland we could feel the cold intensifying. This was most apparent during my evening “swims”. The “swimming pool” was a reconstituted fishing box, previously used to store fish on ice to keep fresh; now it was filled daily with sea water of around 1-6 °C and sat in by myself and others of the crew. While the shock to the system seems extreme, the serenity I felt being sat in near-freezing water after a 12 hour shift spent lifting and moving (often entirely by hand) up to 4,500 kg of sponge was so relaxing, revitalising and reinvigorating. This cold dip then helped, while Tarajoq rocked with the swell, to send me off to the most tranquil, unbroken sleep I have ever experienced. The trawls, apart from literal tonnes of sponges, have yielded an amazing range of species from ancient Greenland sharks, monster cod and halibut to intricate deep sea Paragorgia corals. So far, the highlights had been the cute and sassy bobtail squids (Rossia sp.), grumpy octopuses (Bathypolypus sp.) and a small star covered smoothhound (Squalus acanthius). The trawls had even yielded long dead specimens: the vertebrae and a rib from some sort of whale.\n\n\n\n\n\n\n\n\n\nA sample of Deep Sea Coral (Paragorgia)\n\n\n\n\n\n\n\nA Small Squid (Rossia sp.)\n\n\n\n\n\n\n\nA Grumpy Octopus (Bathypolypus sp.)\n\n\n\n\n\n\n\n\n\nA Starry Smoothound (Squalus acanthius) about to be tagged.\n\n\n\n\n\n\n\nA Single Whale Vertebrae\n\n\n\n\n\nIn between trawls we got up to an hour of break, if the previous trawl didn’t catch too much. In that time, I took to sitting on top of the spare trawl nets, which were stored on the upper decks, looking out to sea, just waiting for the blow of a whale or the passing flight of the many sea birds that share these fishing grounds. We were gliding across almost gloss smooth waters but this was just “the calm before the storm”.\n\n\n\nMy View from the Spare Trawl Nets.\n\n\nThe cook told me we were about to meet a summer storm. If this happened the skipper’s job would be to decide where to weather the storm. As we wouldn’t be able to fish during the storm we would need to hide either inshore in one of the many fjords along Greenland’s east coast or head further offshore in deeper, calmer waters. Both scenarios would have had their advantages and disadvantages but only once the storm arrived could the decision be made.\n\n\nThe beginning of Stormy Seas.\nThe beginning of Stormy Seas.",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july-3",
    "href": "blogs/GreenlandTale.html#th-july-3",
    "title": "A Greenlandic Tale",
    "section": "27th July",
    "text": "27th July\nThankfully the weather forecasts were correct, and a quick steam up north meant we skirted the storm, avoiding the worst of the weather. That being said, we still were exposed to the odd 5-10 metre wave with consistent 4 metre rolling swells for 3 days. The rocking motion made my sleep even better, although I know many who barely slept a wink. All crew and researchers got into the habit of continuously swaying, pre-emptively moving with the ocean rather than against it. As the storm abated and the weather cleared it allowed for one of the most vividly coloured sunsets I have ever seen. The whole world turned to sepia in the orange-red glow, giving the appearance of a dramatic fire raging just out of view. With it the sunset brought thin streaks of purple cloud, forming wisps of colour across the expanse of clear blue sky. After days that felt like weeks of close fog and clouds, the clear skies all the way to the horizon gave a feeling of relief. The sea was beginning to calm, although it still held some of the energy of the storm.\n\n\n\n\n\n\n\n\n\nBeginning of Sunset after Stormy Seas.\n\n\n\n\n\n\n\nSunset Streaked sky.\n\n\n\n\n\nThis calming gave way to the now regular spouts of air and water signifying the continual migration, movement and searching for food by the biggest animals to ever live. While a professional or even an avid amateur can tell the whale from the shape, size and frequency of its breaths, I could not The only species easy to identify, which whalers took great advantage of, is the sperm whale. The elongated head and offset nostril means a sperm whales spout shoots out at 45 degrees forward and off to one side. I had now seen many spouts of air, many off to one side, yet apart from the momentary glimpse of a tail or back, I hadn’t seen any of these whales up close.\n\n\nA Whales Spout alongside Sunset.\nA Whales Spout alongside Sunset.\n\n\nMy day-to-day shift continued as before with the odd species alluding me, but most were becoming well known to me. Alongside our fishing trawls we were also deploying benthic grabs, which take a grab of all species in the sediment, and video camera sleds, which in the same way as the trawl, is dragged along the seabed and captures all in its path. However, it captures organisms in their natural habitat and more importantly captures them only on film. Comparing the video, grab and trawl results side-by-side allowed us to see both the differences and similarities between the methods. As we came to the end of the survey each trawl became more special as I savoured each new identification. Once the survey was finished our next adventure would be the crossing to Iceland. The fine and clear forecast boded well for an easy crossing and while I was keen to see Iceland it would mean an end to this trip, which I was far less keen on.\n\n\n\n\n\n\n\n\n\nA Benthic Grab.\n\n\n\n\n\n\n\nThe Video Trawl.",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july-4",
    "href": "blogs/GreenlandTale.html#th-july-4",
    "title": "A Greenlandic Tale",
    "section": "29th July",
    "text": "29th July\nToday marked the day when all sampling sites had been sampled. As with most fieldwork this trip included a few days of contingency for bad weather or equipment failure. However, through luck, efficient planning and hard work we had finished early. We still had other duties, such as cleaning, equipment maintenance, as well as trying to tag as many Cod (Gadus morhua) as possible. During this increased downtime we were sailing along the Greenlandic coast, over calm seas, under clear skies and alongside a vast array of glacial mountainsides. This route brought us closer to more icebergs, which drifted along beside us, often following our wake, driven by the currents on the surface and at other times moving at odds to local wind, waves and currents.\n\n\n\nThe Flat Waters with a Lone Gigantic Iceberg\n\n\nBeing closer in to the shore brought us nearer to the playgrounds of a range of species. Unlike when we were offshore, surrounded by Fullmars and the occasional spout of migrating whales, inshore we saw a variety of seabirds and larger pods of whales. Gannets, which I had not spotted so far, now circled us, occasionally pausing and then plummeting into the cold waves, and Petrels flitted mere centimetres above the lightly moving water, giving the illusion of dancing across the peaks and troughs of the barely shifting swell. With the days of contingency not wanting to be wasted, the ships primary goal was to find cod. Up until that point we had been trawling specific stations to assess, in a standardised way, the distribution of this highly prized fish. Now we were on the hunt. If we could catch a large quantity we could insert tags into the fish that means when that individual is caught we can see where it ends up. Each tag was a small plastic streamer with a unique ID number. This means any fisher who catches a tagged fish can tell the Greenlandic fisheries institute where and when it was caught. These types of passive tracking can allow researchers to find out where fish go and when they go there, to better understand and therefore manage these fish.\n\n\n\nThe Cod Tagger with Tags",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html#st-july",
    "href": "blogs/GreenlandTale.html#st-july",
    "title": "A Greenlandic Tale",
    "section": "31st July",
    "text": "31st July\nAfter two and a half days, still working on 12 hour shifts, we had managed to tag over 2,000 cod. The work was short and intense; when a trawl came up, if it has cod we tagged them. If no cod came up then the trawl was emptied, reset and redeployed. When the trawl was full of cod we jumped to action stations. Firstly, all cod in the trawl were separated from the rest of the catch and placed in a holding tank. These were then left and monitored for health and survival for half an hour. Alive individuals were selected from the tank, laid out on a measuring tape to be recorded, a small plastic tag inserted into the cod alongside the dorsal fin and straight away released back into the sea.\n\n\nThe Process of Tagging a Cod\nThe Process of Tagging a Cod\n\n\nThis method was the same for all species we tagged throughout the trip, with the exception of the Greenland sharks, which were measured, tagged and released on deck as quickly as possible, to minimise time out of water and therefore stress to the animal.\n\n\n\n\n\n\n\n\n\nTagging the Starry Smoothound (Squalus acanthius)\n\n\n\n\n\n\n\n\n\nA Greenlandic Shark being Tagged and Measured on Deck\n\n\n\n\n\nHowever, the last trawl for this trip had come up; the last cod tagged; the last organism identified.\n\n\n\nThe Last Trawl Coming onto Deck.\n\n\nSometime that day we would set sail for Iceland, crossing the Denmark strait and bound for Reykjavik. The crossing would take around 24 hours and we cleaned and tidied to keep busy. As the shift structure ended for all scientists, we played a game of ‘who can stay up latest’ for the night shifters and ‘who can wake up earliest’ for the day shifters. Over the last few days, we had slowly been transitioning to Icelandic time from Greenlandic time. With the last trawl came two huge and beautiful Greenland sharks.\n\n\n\nThe Two Greenlandic Sharks from the Last Trawl.\n\n\nOver the last 21 days at sea we had caught 7 of these slow, hardy giants, yet, I was still in awe of them every time. Their size and age just made me wonder what had happened over their lifetimes: what ships had sailed above them? Unbeknownst to them, did they cross the path of the Titanic or witness the U-boats of World War 2? With a potential age of more than 500 years, did they even see the Niña, Pinta and Santa Maria as they sailed to discover the people already living in North America? Very unlikely given the route Columbus took but the incredible age of these creatures and their relatively unknown behaviour highlights how little we still know about our oceans.\n\n\n\nA Greenlandic Shark.",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html#rd-august",
    "href": "blogs/GreenlandTale.html#rd-august",
    "title": "A Greenlandic Tale",
    "section": "3rd August",
    "text": "3rd August\nIt was my last night in the North. We landed in Iceland in Hafnarfjörður the day before and, after being cleared by customs, were allowed to get our feet on dry land. After almost 3 weeks on board, being ashore was a big shock. It was interesting to realise how continuously noisy the boat had been: the constant thrum of the engines, the crashing of waves, the booming sound of our hull hitting icebergs. All these noises just faded into the background. This made the still, calm, quiet of land feel almost eerie, perhaps too quiet! Hafnarfjörður is only 6 km south of Reykjavik, so we quickly made the short trip to the capital. After visiting the magnificent cathedral, which my sister tells me was informed by the ‘post-industrial rationalist movement in Holland and Southern Germany’, we decided to wander around Reykjavik but somehow got continually waylaid by liquid refreshment. The dry ship rules of the three weeks had produced a healthy thirst. From some extensive tasting I could conclude that the brewing culture in Reykjavik is amazing, the only drawback being the eye-watering price tag for a pint: £8-£10!!\n\n\n\nA Pint Outside in the Reykjavik Sun.\n\n\nThis was my last night as a scientist on board the research vessel Tarajoq. My travel home would see me fly from Reykjavik to Paris then catch a train from Paris back home to Nantes. I would be lying if I said I wasn’t keen to be heading home but it would also be a huge lie to say I hadn’t loved this trip. It kept me on my toes with hundreds of new experiences. I had been so active carrying baskets of sponges to identify; washing down the grab we had used to sample mud from over five hundred metres depth; helping tag cod or counting and recording hundreds of jellyfish at a time. When not physically active the intellectual tasks had been just as rewarding, working out the taxonomic identities of different deep-sea spiders, squid, starfish or any number of other groups of species. The work itself had been extremely fun, with brand new species almost every trawl.\n\n\n\nMy Last View of Tarajoq.\n\n\nHowever, as fun and interesting as all these elements had been, the overwhelming comfort I felt at being on a boat, at sea, travelling across the great expanse of the ocean outstrips them all. It makes me realise that wherever I am, whatever I do, it must include being on, in or under the sea.\n\n\n\nA Moment of Contemplation? Or Exhaustion?\n\n\n\n\n\nDen Lille Havfrue - The Little Mermaid\nDescent into Narsarsuaq\nApproaching Tarajoq through icebergs\nRV Tarajoq\nIcebergs in the Harbour at Narsaq\nIceberg in the Greenlandic Fjords\nCute Little (Cottunculus microps)\nSorting Different Species of Fish\nA Scary Deep Sea Fish (Caulophryne jordani)\nDifferent sizes of Deep Sea Fish (Careproctus reinhardti)\nA Rather Large and Bitey Wolf Fish\nIntricate Basket Star (Gorgonocephalus sp.)\nSelection of Many Armed Starfish (Crossaster sp.)\nA Phakellia Sponge plus Me\nExample Tray of Benthic Specimens\nSea Spider (Pycnogonida sp.)\nA Massive Greenlandic Shark (Somniosus microcephalus)\nA 32 kg Cod (Gadus morhua)\nA 28 kg Halibut (Hippoglossus hippoglossus)\nA 20 kg Pollack (Polliachus virens)\nA 24 kg Flapper Skate (Dipturus batis)\nA Trawl full of Sponges (Geodia sp.)\nMy “Swimming Pool”\nThe Bow of Tarajoq Gliding through the Water.\nBoat Lights Searching Through the Fog.\nImpenetrable Fog in the Middle of the Day\nA sample of Deep Sea Coral (Paragorgia)\nA Small Squid (Rossia sp.)\nA Grumpy Octopus (Bathypolypus sp.)\nA Starry Smoothound (Squalus acanthius) about to be tagged.\nA Single Whale Vertebrae\nMy View from the Spare Trawl Nets.\nThe beginning of Stormy Seas.\nBeginning of Sunset after Stormy Seas.\nSunset Streaked sky.\nA Whales Spout alongside Sunset.\nA Benthic Grab.\nThe Video Trawl.\nThe Flat Waters with a Lone Gigantic Iceberg\nThe Cod Tagger with Tags\nThe Process of Tagging a Cod\nTagging the Starry Smoothound (Squalus acanthius)\nA Greenlandic Shark being Tagged and Measured on Deck\nThe Last Trawl Coming onto Deck.\nThe Two Greenlandic Sharks from the Last Trawl.\nA Greenlandic Shark.\nA Pint Outside in the Reykjavik Sun.\nMy Last View of Tarajoq.\nA Moment of Contemplation? Or Exhaustion?",
    "crumbs": [
      "Home",
      "Travel Blogs",
      "A Greenlandic Tale"
    ]
  },
  {
    "objectID": "consultancy.html",
    "href": "consultancy.html",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\n\n\n\n\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects.\n\n\n\n\n\n\n\n\n\n\n\n\nI have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "consultancy.html#statistical-experience",
    "href": "consultancy.html#statistical-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\n\n\n\n\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "consultancy.html#consultancy-fieldwork-experience",
    "href": "consultancy.html#consultancy-fieldwork-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "I have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "consultancy.html#statistical-experience-1",
    "href": "consultancy.html#statistical-experience-1",
    "title": "Ecological and Statistical Consultancy",
    "section": "Statistical Experience",
    "text": "Statistical Experience\nMy statistical experiences have ranged from:\n\n\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "consultancy.html#consultancy-fieldwork-experience-1",
    "href": "consultancy.html#consultancy-fieldwork-experience-1",
    "title": "Ecological and Statistical Consultancy",
    "section": "Consultancy Fieldwork Experience",
    "text": "Consultancy Fieldwork Experience\n\n\nI have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "posts2/OysterTables.html",
    "href": "posts2/OysterTables.html",
    "title": "Mapping intertidal oyster farms using unmanned aerial vehicles (UAV) high-resolution multispectral data",
    "section": "",
    "text": "Román et al., 2021\nIn France, oyster aquaculture has been historically developed in intertidal zones, with shellfish farming areas covering much of the Atlantic coast. Monitoring these off-bottom cultures where oysters are grown in plastic mesh-bags set on trestle tables is mandatory for the maritime administration to check compliance with a Structural Plan Document (SPD), while also being important for stock assessment in relation to carrying capacity issues. However, traditional monitoring methods are time-consuming, labor-intensive, and inefficient in covering large intertidal areas. In this study, we used a new GIS-based analytical method to assess the potential of high-resolution Unmanned Aerial Vehicle (UAV) multispectral data to retrieve spatial information on oyster-farming structures using Bourgneuf Bay (France) as a case-study. A non-parametric machine learning algorithm was applied to four UAV flight orthomosaics collected at different altitudes (12, 30, 50, and 120 m) to identify oyster mesh-bags. These supervised classifications achieved overall accuracies above 95% for all tested altitudes. In addition, an accurate distinction of oyster-bag mesh sizes (4, 9 and 14 mm) was obtained for 12–50 m flights, but there was a lower accuracy at 120 m. Across all flights, the 4 mm mesh size was the least well detected (72.14% Producer Accuracy). This information can be used to identify bags with specific mesh-sizes used for spat or adult grow out. Finally, we accurately measured oyster table heights using a high-resolution Digital Surface Model (DSM) derived from Structure from Motion (SfM) photogrammetry. The 50 m flight was suggested as the best compromise to obtain precise measurements of the oyster table heights while covering larger areas than lower altitude flights. This demonstrates that UAV technology can provide a set of spatial variables relevant for shellfish farmers and coastal managers in an efficient, rapid, and non-destructive way to monitor the extent and characteristics of oyster-farming areas regularly."
  },
  {
    "objectID": "posts2/Ganges.html",
    "href": "posts2/Ganges.html",
    "title": "The Abundance and Characteristics of Microplastics in Surface Water in the Transboundary Ganges River",
    "section": "",
    "text": "Napper et al., 2021\nMicroplastics (plastic &lt; 5 mm in size) are now known to contaminate riverine systems but understanding about how their concentrations vary spatially and temporally is limited. This information is critical to help identify key sources and pathways of microplastic and develop management interventions. This study provides the first investigation of microplastic abundance, characteristics and temporal variation along the Ganges river; one of the most important catchments of South Asia. From 10 sites along a 2575 km stretch of the river, 20 water samples (3600 L in total) were filtered (60 samples each from pre- and post-monsoon season). Overall, 140 microplastic particles were identified, with higher concentrations found in the pre-monsoon (71.6%) than in post-monsoon (61.6%) samples. The majority of microplastics were fibres (91%) and the remaining were fragments (9%). We estimate that the Ganges, with the combined flows of the Brahmaputra and Meghna rivers (GBM), could release up to 1–3 billion (10\\(^9\\)) microplastics into the Bay of Bengal (north-eastern portion of the Indian Ocean) every day. This research provides the first step in understanding microplastic contamination in the Ganges and its contribution to the oceanic microplastic load."
  },
  {
    "objectID": "posts2/LymeSummary.html",
    "href": "posts2/LymeSummary.html",
    "title": "Lessons from Lyme Bay (UK) to inform policy, management, and monitoring of Marine Protected Areas",
    "section": "",
    "text": "Renn et al., 2021\nThis decade represents a critical period to profoundly rethink human–nature interactions in order to address the interwoven climate and biodiversity crises. Marine Protected Areas (MPAs) demonstrate promise for increasing ecosystem resilience and reversing habitat and population declines, but outcomes vary considerably from context to context. Partially protected areas offer a compromise between ecological recovery and the social needs of local communities, but their success is contingent on an array of factors. This in-depth review summarizes 15 years of marine conservation research and impact in Lyme Bay (southwest UK), to serve as a model for the future adoption of partially protected MPAs. The findings from the UK’s longest integrated socioecological monitoring MPA study are presented and supplemented by an evaluation of the whole-site management approach as a core element of Lyme Bay’s achievements. The journey from research to improved monitoring and ambitious policy is illustrated within and interspersed with stories of novel discoveries, ongoing challenges, and method developments. What started as a dedicated group of community members has grown into an immense collaboration between fishers, scientists, NGOs, and regulators, and their combined efforts have sent ripple effects of positive change across the globe."
  },
  {
    "objectID": "posts2/Storms.html",
    "href": "posts2/Storms.html",
    "title": "Rewilding of Protected Areas Enhances Resilience of Marine Ecosystems to Extreme Climatic Events",
    "section": "",
    "text": "Sheehan et al., 2021\nMarine protected areas (MPAs) are employed as tools to manage human impacts, especially fishing pressure. By excluding the most destructive activities MPAs can rewild degraded areas of seabed habitat. The potential for MPAs to increase ecosystem resilience from storms is, however, not understood, nor how such events impact seabed habitats. Extreme storm disturbance impact was studied in Lyme Bay MPA, Southwest United Kingdom, where the 2008 exclusion of bottom-towed fishing from the whole site allowed recovery of degraded temperate reef assemblages to a more complex community. Severe storm impacts in 2013–2014 resulted in major damage to the seabed so that assemblages in the MPA were more similar to sites where fishing continued than at any point since the designation of the MPA; the communities were not dominated by species resistant to physical disturbance. Nevertheless, annual surveys since 2014 have demonstrated that the initial recovery of MPA assemblages was much quicker than that seen following the cessation of chronic towed fishing impact in 2008. Likewise, General Additive Mixed Effect Models (GAMMs) showed that inside the MPA increases in diversity metrics post-Storm were greater and more consistent over time than post-Bottom-Towed Fishing. As extreme events are likely to become more common with climate change, wave exposure observations indicated that 29% of coastal reef MPAs around the United Kingdom may be exposed to comparable wave climate extremes, and may be similarly impacted. This paper therefore provides an insight into the likely extent and magnitude of ecological responses of seabed ecosystems to future extreme disturbance events"
  },
  {
    "objectID": "BasicRTutorials/DataManipulation.html",
    "href": "BasicRTutorials/DataManipulation.html",
    "title": "Data Manipulation in R",
    "section": "",
    "text": "Packages often have their own example datasets within them, or sometimes a package can be used to store just data without functions etc.\nWe will look at the palmer penguins dataset\n\n#install.packages(\"palmerpenguins\")\n\nlibrary(palmerpenguins)\ndata(penguins)\n\n\n\n\nPenguins Image by Allison Horst\n\n\nThis becomes a ‘promise’ of a data set, we have to do something with it to get it properly, lets take a look inside\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nThis gives us two datasets in our global environment\nUsing summary() we can see which columns have NAs and which don’t.\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nThe penguins data set is fairly well organised but we can still do a bit more with it if we want\n\nsummary(penguins_raw)\n\n  studyName         Sample Number      Species             Region         \n Length:344         Min.   :  1.00   Length:344         Length:344        \n Class :character   1st Qu.: 29.00   Class :character   Class :character  \n Mode  :character   Median : 58.00   Mode  :character   Mode  :character  \n                    Mean   : 63.15                                        \n                    3rd Qu.: 95.25                                        \n                    Max.   :152.00                                        \n                                                                          \n    Island             Stage           Individual ID      Clutch Completion \n Length:344         Length:344         Length:344         Length:344        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    Date Egg          Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm)\n Min.   :2007-11-09   Min.   :32.10      Min.   :13.10     Min.   :172.0      \n 1st Qu.:2007-11-28   1st Qu.:39.23      1st Qu.:15.60     1st Qu.:190.0      \n Median :2008-11-09   Median :44.45      Median :17.30     Median :197.0      \n Mean   :2008-11-27   Mean   :43.92      Mean   :17.15     Mean   :200.9      \n 3rd Qu.:2009-11-16   3rd Qu.:48.50      3rd Qu.:18.70     3rd Qu.:213.0      \n Max.   :2009-12-01   Max.   :59.60      Max.   :21.50     Max.   :231.0      \n                      NA's   :2          NA's   :2         NA's   :2          \n Body Mass (g)      Sex            Delta 15 N (o/oo) Delta 13 C (o/oo)\n Min.   :2700   Length:344         Min.   : 7.632    Min.   :-27.02   \n 1st Qu.:3550   Class :character   1st Qu.: 8.300    1st Qu.:-26.32   \n Median :4050   Mode  :character   Median : 8.652    Median :-25.83   \n Mean   :4202                      Mean   : 8.733    Mean   :-25.69   \n 3rd Qu.:4750                      3rd Qu.: 9.172    3rd Qu.:-25.06   \n Max.   :6300                      Max.   :10.025    Max.   :-23.79   \n NA's   :2                         NA's   :14        NA's   :13       \n   Comments        \n Length:344        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nThe raw data has a lot of extra information that may or may not be important for us. The raw data has lots of difficult to deal with column names.",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Data Manipulation in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataManipulation.html#what-is-tidy-data",
    "href": "BasicRTutorials/DataManipulation.html#what-is-tidy-data",
    "title": "Data Manipulation in R",
    "section": "What is ‘Tidy’ data?",
    "text": "What is ‘Tidy’ data?\nFrom the original paper discussing this: “Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).”\nThis means that Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\nIn tidy data:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThis may not seem that important or even intelligible now but when we start plotting data it becomes very important.",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Data Manipulation in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataManipulation.html#enough-theory---dplyr-and-tidyr",
    "href": "BasicRTutorials/DataManipulation.html#enough-theory---dplyr-and-tidyr",
    "title": "Data Manipulation in R",
    "section": "Enough Theory - dplyr and tidyr",
    "text": "Enough Theory - dplyr and tidyr\n \nFor data manipulation and organisation we will rely heavily on the dplyr and tidyr packages, which have a suite of functions that can be used in isolation or combined to perform complex data manipulation and organisation.\n\nEasy to Read Code\nFor writing easy to follow and understand code/scripts with complex sequences of functions, putting our code across multiple lines is a technique we can use.\nThis can be done by doing an enter/carriage return after a comma inside of a function between arguments.\nThis technique changes nothing of how the function works (we can check if the two outputs are equal with all.equal())\n\ndf_without_enters&lt;-data.frame(Column1=c(1.3,5.8,5.122,3.00,7.12),Column2=c(1,5,5,3,7))\n\ndf_with_enters&lt;-data.frame(\n  Column1=c(1.3,5.8,5.122,3.00,7.12),\n  Column2=c(1,5,5,3,7)\n  )\n\nall.equal(df_without_enters,df_with_enters)\n\n[1] TRUE\n\n\n\n\nPiping (Native and maggittr)\n\nWhat do we do when we want to apply multiple functions in a sequence but don’t want to create loads of objects in our global environment?\nOne option is putting one function inside of another etc (Sometimes fine). This is called nesting.\n\nNestingFunctions&lt;-summary(subset(df_with_enters, Column1==1.3))\n\nThis can be okay but generally is hard to follow, as the last function that is applied is the first one you read from left to right.\nIn R there is an operator that allows you to pass the result from one function into the next function this is called the Native Pipe |&gt;\nAgain we can check this creates the same thing with all.equal()\n\nNativePipingingFunctions&lt;-df_with_enters|&gt;\n  subset(Column1==1.3)|&gt;\n  summary()\n\nall.equal(NestingFunctions,NativePipingingFunctions)\n\n[1] TRUE\n\n\nThis operator is actually quite new and was based on another commonly used pipe (and more superior in my mind).\nThe magittr pipe %&gt;% (shift+cmd+m or shift+ctrl+m) was from a package called magittr that is automatically loaded by any tidyverse package.\nIt works very similarly to the native pipe with some subtle changes.\nAgain I feel the magittr pipe makes code that is easier to read.\n\nMaggittrPipingingFunctions&lt;-df_with_enters %&gt;% \n  subset(Column1==1.3) %&gt;% \n  summary()\n\nall.equal(MaggittrPipingingFunctions,NativePipingingFunctions)\n\n[1] TRUE\n\n\nThe pipe can be thought of as “and then”\nSo above, the df_with_enters is subset where Column1 is equal to 1.3 and then the summary function is used.\nWhen using one function it is not needed, but when using multiple functions in a row piping makes code a lot easier to read and understand what order functions have been carried out in.\nAgain this readability has drawbacks in being slower (for small data of 100s of rows this may be 0.0001 of a second but big data 10000000000s of rows it might be a few seconds)\nLater on we will use pipes in long sequences of functions and it will become clearer how useful they are.\n\n\nFiltering\nThe dplyr function filter() is a row wise subsetter, based on a statement from the dataframe.\nWhen we looked at the summary() of penguins we saw some NAs in the biometric columns and also in the sex column.\nIf we want to remove NA’s there are many ways, to be selective we can filter our dataset.\nTo subset data, we create a logic clause that then filters the dataset by that clause/statement,\nFor example if we want to select all rows of the data set where the data is from a female penguin we can do this by:\n\nfemale_penguins&lt;- penguins %&gt;% \n  filter(sex==\"female\")\n\nfemale_penguins\n\n# A tibble: 165 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.5          17.4               186        3800\n 2 Adelie  Torgersen           40.3          18                 195        3250\n 3 Adelie  Torgersen           36.7          19.3               193        3450\n 4 Adelie  Torgersen           38.9          17.8               181        3625\n 5 Adelie  Torgersen           41.1          17.6               182        3200\n 6 Adelie  Torgersen           36.6          17.8               185        3700\n 7 Adelie  Torgersen           38.7          19                 195        3450\n 8 Adelie  Torgersen           34.4          18.4               184        3325\n 9 Adelie  Biscoe              37.8          18.3               174        3400\n10 Adelie  Biscoe              35.9          19.2               189        3800\n# ℹ 155 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNotice there are two =’s!!\nThis is used to create our clause/statement, we filter (keep) the rows of the pengiuns dataset if the sex column contains “females”, if just one equals (=) is used it won’t work.\nOr we might want all the penguins above 5 kg.\n\nheavier_penguins&lt;- penguins %&gt;% \n  filter(body_mass_g&gt;= 5000)\n\nheavier_penguins\n\n# A tibble: 67 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           50            16.3               230        5700\n 2 Gentoo  Biscoe           50            15.2               218        5700\n 3 Gentoo  Biscoe           47.6          14.5               215        5400\n 4 Gentoo  Biscoe           46.7          15.3               219        5200\n 5 Gentoo  Biscoe           46.8          15.4               215        5150\n 6 Gentoo  Biscoe           49            16.1               216        5550\n 7 Gentoo  Biscoe           48.4          14.6               213        5850\n 8 Gentoo  Biscoe           49.3          15.7               217        5850\n 9 Gentoo  Biscoe           49.2          15.2               221        6300\n10 Gentoo  Biscoe           48.7          15.1               222        5350\n# ℹ 57 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThere are a range of symbols we can use such as more than (&gt;), less than (&lt;), more than or equal to (&gt;=), less than or equal to (&lt;=), is equal to (==), and (&), or (|).\nWe can even use multiple clauses or statements in one call to filter,\nSo if we want all the heavier female penguins\n\nheavier_female_penguins&lt;- penguins %&gt;% \n  filter(body_mass_g&gt;= 5000 & sex==\"female\")\n\n\nheavier_female_penguins\n\n# A tibble: 8 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo  Biscoe           45.1          14.5               215        5000\n2 Gentoo  Biscoe           42.9          13.1               215        5000\n3 Gentoo  Biscoe           45.1          14.5               207        5050\n4 Gentoo  Biscoe           49.1          14.8               220        5150\n5 Gentoo  Biscoe           44.9          13.3               213        5100\n6 Gentoo  Biscoe           46.5          14.8               217        5200\n7 Gentoo  Biscoe           50.5          15.2               216        5000\n8 Gentoo  Biscoe           45.2          14.8               212        5200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSometimes we might want to filter with multiple answers of a categorical variable,\nFor example if we wanted all penguins from Biscoe and Torgersen island\nTo do this we can make a vector of the names we want, then filter by that vector (%in%).\n\nIslands_we_Want&lt;-c(\"Biscoe\",\"Torgersen\")\n\nBiscoe_Torgersen_penguins&lt;- penguins %&gt;% \n  filter(island%in%Islands_we_Want)\n\nBiscoe_Torgersen_penguins\n\n# A tibble: 220 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 210 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHere we will make use of !, this means the opposite of the clause (not this), a subtraction sign can also be used (-).\nWe also use %in% which is used to tell filter there are more than one elements or we can use it for NAs that we want to get rid of as NA is not classed like normal data (It is a lack of data not a character or number).\n\npenguins_someNAs&lt;-penguins %&gt;% \n  filter(!body_mass_g%in%NA)\n\nIf we now look at the number of rows of the datasets we can see only two rows were removed. (not all the NAs)\nWe just removed the rows with NA in the body_mass_g column.\n\nnrow(penguins)\n\n[1] 344\n\nnrow(penguins_someNAs)\n\n[1] 342\n\nsummary(penguins_someNAs)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :151   Biscoe   :167   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :123   Torgersen: 51   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  :  9   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n\n\nStill 9 NAs in sex\n\npenguins_noNAs&lt;-penguins_someNAs %&gt;% \n  filter(!sex%in%NA)\n\nsummary(penguins_noNAs)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :146   Biscoe   :163   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :123   1st Qu.:39.50   1st Qu.:15.60  \n Gentoo   :119   Torgersen: 47   Median :44.50   Median :17.30  \n                                 Mean   :43.99   Mean   :17.16  \n                                 3rd Qu.:48.60   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172       Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190       1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197       Median :4050                Median :2008  \n Mean   :201       Mean   :4207                Mean   :2008  \n 3rd Qu.:213       3rd Qu.:4775                3rd Qu.:2009  \n Max.   :231       Max.   :6300                Max.   :2009  \n\n\nWe might want to remove all rows where an NA is in any of the columns (not always advisable)\n\npenguins_noNAs_quickly&lt;-penguins %&gt;% \n  drop_na()\n\nall.equal(penguins_noNAs,penguins_noNAs_quickly)\n\n[1] TRUE\n\n\n\n\nSelecting\nThe dplyr function select() is a column wise subsetter based on a statement of column names.\nSo we can select or deselect a few named columns using select.\nAnd as with filter we can use - or ! to say not this column/statement.\n\nThree_Columns&lt;-penguins_raw %&gt;% \n  select(studyName,Species,Island)\n\nAll_But_Three_Columns&lt;-penguins_raw %&gt;% \n  select(-studyName,-Species,-Island)\n\nnames(Three_Columns)\n\n[1] \"studyName\" \"Species\"   \"Island\"   \n\nnames(All_But_Three_Columns)\n\n [1] \"Sample Number\"       \"Region\"              \"Stage\"              \n [4] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n [7] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[10] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[13] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\n\nWe can also use a statement for consistencies across columns (contains() or even starts_with() or ends_with())\nFor example all columns that contains() an “s” or even combining a statement with other specific selections\n\nS_Columns&lt;-penguins_raw %&gt;% \n  select(contains(\"s\"))\n\nS_Columns_No_Sex&lt;-penguins_raw %&gt;% \n  select(contains(\"s\"),-Sex)\n\nS_Columns_Plus_Region&lt;-penguins_raw %&gt;% \n  select(contains(\"s\"),Region)\n\nnames(S_Columns)\n\n[1] \"studyName\"     \"Sample Number\" \"Species\"       \"Island\"       \n[5] \"Stage\"         \"Body Mass (g)\" \"Sex\"           \"Comments\"     \n\nnames(S_Columns_No_Sex)\n\n[1] \"studyName\"     \"Sample Number\" \"Species\"       \"Island\"       \n[5] \"Stage\"         \"Body Mass (g)\" \"Comments\"     \n\nnames(S_Columns_Plus_Region)\n\n[1] \"studyName\"     \"Sample Number\" \"Species\"       \"Island\"       \n[5] \"Stage\"         \"Body Mass (g)\" \"Sex\"           \"Comments\"     \n[9] \"Region\"       \n\n\nWe can even make a vector of column names and then pass that vector to select() using the all_of() or any_of() functions.\n\nColumns_We_Want&lt;-c(\"Region\",\"Island\",\"studyName\",\"Stage\")\n\nColumns_From_Vector&lt;-penguins_raw %&gt;% \n  select(all_of(Columns_We_Want))\n\nColumns_Not_From_Vector&lt;-penguins_raw %&gt;% \n  select(-all_of(Columns_We_Want))\n\nnames(Columns_From_Vector)\n\n[1] \"Region\"    \"Island\"    \"studyName\" \"Stage\"    \n\nnames(Columns_Not_From_Vector)\n\n [1] \"Sample Number\"       \"Species\"             \"Individual ID\"      \n [4] \"Clutch Completion\"   \"Date Egg\"            \"Culmen Length (mm)\" \n [7] \"Culmen Depth (mm)\"   \"Flipper Length (mm)\" \"Body Mass (g)\"      \n[10] \"Sex\"                 \"Delta 15 N (o/oo)\"   \"Delta 13 C (o/oo)\"  \n[13] \"Comments\"           \n\n\nAnother nice feature of select is that the order is maintained, so the order of things we select is used to order the columns,\nSo if we want to move a certain column towards the beginning of the df we can do this using select() and put everything() to say everything else after the columns we put first\n\nRegion_First&lt;-penguins_raw %&gt;% \n  select(Region,everything())\n\nRegion_Then_Island_First&lt;-penguins_raw %&gt;% \n  select(Region,Island,everything())\n\nnames(penguins_raw)\n\n [1] \"studyName\"           \"Sample Number\"       \"Species\"            \n [4] \"Region\"              \"Island\"              \"Stage\"              \n [7] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n[10] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[13] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[16] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\nnames(Region_First)\n\n [1] \"Region\"              \"studyName\"           \"Sample Number\"      \n [4] \"Species\"             \"Island\"              \"Stage\"              \n [7] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n[10] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[13] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[16] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\nnames(Region_Then_Island_First)\n\n [1] \"Region\"              \"Island\"              \"studyName\"          \n [4] \"Sample Number\"       \"Species\"             \"Stage\"              \n [7] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n[10] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[13] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[16] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\n\n\n\nMutating\nWe have data but maybe we want to transform that data and either replace the original column or create a new column.\nWe can use dplyr’s mutate() to do this.\nLets convert the body mass column into a new column that is in kg.\n\npenguins_kgbodymass&lt;-penguins %&gt;% \n  mutate(body_mass_kg=body_mass_g/1000)\n\nsummary(penguins_kgbodymass)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year       body_mass_kg  \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007   Min.   :2.700  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007   1st Qu.:3.550  \n Median :197.0     Median :4050   NA's  : 11   Median :2008   Median :4.050  \n Mean   :200.9     Mean   :4202                Mean   :2008   Mean   :4.202  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009   3rd Qu.:4.750  \n Max.   :231.0     Max.   :6300                Max.   :2009   Max.   :6.300  \n NA's   :2         NA's   :2                                  NA's   :2      \n\n\nWe can also paste information from other columns together into another new column.\nWe shall use the paste() function then we will put what character we want to separate each element by using the sep argument.\n\npenguins_ExtraInfo&lt;-penguins_noNAs %&gt;% \n  mutate(Info=paste(species,island,sex,sep=\"_\"))\n\nunique(penguins_ExtraInfo$Info)\n\n [1] \"Adelie_Torgersen_male\"   \"Adelie_Torgersen_female\"\n [3] \"Adelie_Biscoe_female\"    \"Adelie_Biscoe_male\"     \n [5] \"Adelie_Dream_female\"     \"Adelie_Dream_male\"      \n [7] \"Gentoo_Biscoe_female\"    \"Gentoo_Biscoe_male\"     \n [9] \"Chinstrap_Dream_female\"  \"Chinstrap_Dream_male\"   \n\n\nWe can even do calculations that are based on and element in another column at the same row.\nThere are a few ways to do this, the simplest is an if_else() statement.\nWith if_else() there are three arguments, the first argument is the statement (is it female), the second argument is what to do if the statement is true and the third argument is what to do if the statement is false.\nLets pretend that when an Adelie penguin was studied they were incorrectly weighed by 200 g.\nWe shall replace the old body_mass_g with new corrected weight, but only for Adelie penguins.\nTo look at the change we will plot a histogram of body weights for both weights.\n\npenguins_if_else&lt;-penguins_noNAs %&gt;% \n  mutate(body_mass_g=if_else(species==\"Adelie\",\n                             body_mass_g+200,\n                             as.numeric(body_mass_g)))\n\nhist(penguins_noNAs$body_mass_g)\n\n\n\n\n\n\n\nhist(penguins_if_else$body_mass_g)\n\n\n\n\n\n\n\n\nWhile fine for one single statement, multiple if_else() statements can create horrible code.\nFor this we can use case_when(),\nWhere we take a statement, then use ~ to say the new column value, then a comma before the next statement\nMaybe Gentoos were also miss-measured but the other way round (too big).\n\npenguins_case_when&lt;-penguins_noNAs %&gt;% \n  mutate(body_mass_g=case_when(species==\"Adelie\"~body_mass_g+200,\n                               species==\"Gentoo\"~body_mass_g-200,\n                               species==\"Chinstrap\"~body_mass_g))\n\nhist(penguins_noNAs$body_mass_g)\n\n\n\n\n\n\n\nhist(penguins_case_when$body_mass_g)\n\n\n\n\n\n\n\n\nWith case_when we have to be careful if all of our statements don’t cover all the data.\nIf there is a condition not covered it will return NA values.\nTo avoid this we can do a final statement with TRUE~Our_Default\n\npenguins_case_when_Missing&lt;-penguins_noNAs %&gt;% \n  mutate(body_mass_g=case_when(species==\"Adelie\"~body_mass_g+200,\n                               species==\"Gentoo\"~body_mass_g-200))\n\npenguins_case_when_TRUE&lt;-penguins_noNAs %&gt;% \n  mutate(body_mass_g=case_when(species==\"Adelie\"~body_mass_g+200,\n                               species==\"Gentoo\"~body_mass_g-200,\n                               TRUE~body_mass_g))\n\nall.equal(penguins_case_when_TRUE,penguins_case_when_Missing)\n\n[1] \"Component \\\"body_mass_g\\\": 'is.NA' value mismatch: 68 in current 0 in target\"\n\n\nThis creates 68 NA values in our data, so need to be aware of this.\nBut we could use it for bug checking.\n\n\nSummarise by Groups\nOften we will want to see summaries of data across groups, using a combination of group_by() and summarise() can give use these summary stats.\nWe can group by one, two or many columns, the more groups the less data will summarised in each group.\nIf we don’t group by a column it will not be in the final dataset.\n\npenguins_noNAs %&gt;% \n  group_by(year) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g))\n\n# A tibble: 3 × 2\n   year Mean_body_mass\n  &lt;int&gt;          &lt;dbl&gt;\n1  2007          4153.\n2  2008          4263.\n3  2009          4200.\n\npenguins_noNAs %&gt;% \n  group_by(year,species) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 9 × 3\n# Groups:   year [3]\n   year species   Mean_body_mass\n  &lt;int&gt; &lt;fct&gt;              &lt;dbl&gt;\n1  2007 Adelie             3714.\n2  2007 Chinstrap          3694.\n3  2007 Gentoo             5100 \n4  2008 Adelie             3742 \n5  2008 Chinstrap          3800 \n6  2008 Gentoo             5028.\n7  2009 Adelie             3665.\n8  2009 Chinstrap          3725 \n9  2009 Gentoo             5157.\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g))\n\n`summarise()` has grouped output by 'year', 'species'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 15 × 4\n# Groups:   year, species [9]\n    year species   island    Mean_body_mass\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;\n 1  2007 Adelie    Biscoe             3620 \n 2  2007 Adelie    Dream              3708.\n 3  2007 Adelie    Torgersen          3785 \n 4  2007 Chinstrap Dream              3694.\n 5  2007 Gentoo    Biscoe             5100 \n 6  2008 Adelie    Biscoe             3628.\n 7  2008 Adelie    Dream              3756.\n 8  2008 Adelie    Torgersen          3856.\n 9  2008 Chinstrap Dream              3800 \n10  2008 Gentoo    Biscoe             5028.\n11  2009 Adelie    Biscoe             3858.\n12  2009 Adelie    Dream              3651.\n13  2009 Adelie    Torgersen          3489.\n14  2009 Chinstrap Dream              3725 \n15  2009 Gentoo    Biscoe             5157.\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island,sex) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g))\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 30 × 5\n# Groups:   year, species, island [15]\n    year species   island    sex    Mean_body_mass\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;\n 1  2007 Adelie    Biscoe    female          3470 \n 2  2007 Adelie    Biscoe    male            3770 \n 3  2007 Adelie    Dream     female          3269.\n 4  2007 Adelie    Dream     male            4102.\n 5  2007 Adelie    Torgersen female          3475 \n 6  2007 Adelie    Torgersen male            4139.\n 7  2007 Chinstrap Dream     female          3569.\n 8  2007 Chinstrap Dream     male            3819.\n 9  2007 Gentoo    Biscoe    female          4619.\n10  2007 Gentoo    Biscoe    male            5553.\n# ℹ 20 more rows\n\n\nWe can also use groups to count numbers of rows within each group.\nAlthough the table() function in base r does the same but it is harder to read and use when lots of columns selected.\n\npenguins_noNAs %&gt;% \n  group_by(year,species,island,sex) %&gt;% \n  summarise(Number=n())\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 30 × 5\n# Groups:   year, species, island [15]\n    year species   island    sex    Number\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt;\n 1  2007 Adelie    Biscoe    female      5\n 2  2007 Adelie    Biscoe    male        5\n 3  2007 Adelie    Dream     female      9\n 4  2007 Adelie    Dream     male       10\n 5  2007 Adelie    Torgersen female      8\n 6  2007 Adelie    Torgersen male        7\n 7  2007 Chinstrap Dream     female     13\n 8  2007 Chinstrap Dream     male       13\n 9  2007 Gentoo    Biscoe    female     16\n10  2007 Gentoo    Biscoe    male       17\n# ℹ 20 more rows\n\npenguins_noNAs %&gt;% \n  select(year,species,island,sex) %&gt;% \n  table()\n\n, , island = Biscoe, sex = female\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      5         0     16\n  2008      9         0     22\n  2009      8         0     20\n\n, , island = Dream, sex = female\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      9        13      0\n  2008      8         9      0\n  2009     10        12      0\n\n, , island = Torgersen, sex = female\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      8         0      0\n  2008      8         0      0\n  2009      8         0      0\n\n, , island = Biscoe, sex = male\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      5         0     17\n  2008      9         0     23\n  2009      8         0     21\n\n, , island = Dream, sex = male\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007     10        13      0\n  2008      8         9      0\n  2009     10        12      0\n\n, , island = Torgersen, sex = male\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      7         0      0\n  2008      8         0      0\n  2009      8         0      0\n\n\n\n\nWide and Long Data\nTidy data is generally in what could be considered a long format, where each row is an individual observations often having a column that repeats itself.\nBut for some visualisation tools or for making nice looking tables it might be better to be in wide format.\nLets take some of the summaries from above to create a wide database from our last summary which was hard to read because of its length.\nTo go between wide and long data we will use pivot functions from tidyr, namely pivot_wider() and pivot_longer().\n\n#install.packages(\"tidyr\")\n\nlibrary(tidyr)\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island,sex) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g)) %&gt;% \n  pivot_wider(names_from = species, values_from = Mean_body_mass)\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 18 × 6\n# Groups:   year, island [9]\n    year island    sex    Adelie Chinstrap Gentoo\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1  2007 Biscoe    female  3470        NA   4619.\n 2  2007 Biscoe    male    3770        NA   5553.\n 3  2007 Dream     female  3269.     3569.    NA \n 4  2007 Dream     male    4102.     3819.    NA \n 5  2007 Torgersen female  3475        NA     NA \n 6  2007 Torgersen male    4139.       NA     NA \n 7  2008 Biscoe    female  3244.       NA   4627.\n 8  2008 Biscoe    male    4011.       NA   5411.\n 9  2008 Dream     female  3412.     3472.    NA \n10  2008 Dream     male    4100      4128.    NA \n11  2008 Torgersen female  3519.       NA     NA \n12  2008 Torgersen male    4194.       NA     NA \n13  2009 Biscoe    female  3447.       NA   4786.\n14  2009 Biscoe    male    4269.       NA   5511.\n15  2009 Dream     female  3358.     3523.    NA \n16  2009 Dream     male    3945      3927.    NA \n17  2009 Torgersen female  3194.       NA     NA \n18  2009 Torgersen male    3784.       NA     NA \n\n\nIt is still quite long but we could also add more info into the wider columns (e.g. year or sex)\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island,sex) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g)) %&gt;% \n  pivot_wider(names_from = c(species,year), values_from = Mean_body_mass)\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 6 × 11\n# Groups:   island [3]\n  island sex   Adelie_2007 Chinstrap_2007 Gentoo_2007 Adelie_2008 Chinstrap_2008\n  &lt;fct&gt;  &lt;fct&gt;       &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1 Biscoe fema…       3470             NA        4619.       3244.            NA \n2 Biscoe male        3770             NA        5553.       4011.            NA \n3 Dream  fema…       3269.          3569.         NA        3412.          3472.\n4 Dream  male        4102.          3819.         NA        4100           4128.\n5 Torge… fema…       3475             NA          NA        3519.            NA \n6 Torge… male        4139.            NA          NA        4194.            NA \n# ℹ 4 more variables: Gentoo_2008 &lt;dbl&gt;, Adelie_2009 &lt;dbl&gt;,\n#   Chinstrap_2009 &lt;dbl&gt;, Gentoo_2009 &lt;dbl&gt;\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island,sex) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g)) %&gt;% \n  pivot_wider(names_from = c(species,sex), values_from = Mean_body_mass)\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 9 × 8\n# Groups:   year, island [9]\n   year island    Adelie_female Adelie_male Chinstrap_female Chinstrap_male\n  &lt;int&gt; &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1  2007 Biscoe            3470        3770               NA             NA \n2  2007 Dream             3269.       4102.            3569.          3819.\n3  2007 Torgersen         3475        4139.              NA             NA \n4  2008 Biscoe            3244.       4011.              NA             NA \n5  2008 Dream             3412.       4100             3472.          4128.\n6  2008 Torgersen         3519.       4194.              NA             NA \n7  2009 Biscoe            3447.       4269.              NA             NA \n8  2009 Dream             3358.       3945             3523.          3927.\n9  2009 Torgersen         3194.       3784.              NA             NA \n# ℹ 2 more variables: Gentoo_female &lt;dbl&gt;, Gentoo_male &lt;dbl&gt;\n\n\nOften as ecologists we will be surveying a whole community and counting numbers of each different species at each site.\nThis data often comes to us as wide data, our summary of counts could be turned into a wide df (we will fill NAs as 0s) and we can the convert it back to a long dataframe.\n\nWideCounts&lt;-penguins_noNAs %&gt;% \n  group_by(year,species,island,sex) %&gt;% \n  summarise(Number=n()) %&gt;% \n  pivot_wider(names_from = species,values_from = Number, values_fill = 0)\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\nWideCounts\n\n# A tibble: 18 × 6\n# Groups:   year, island [9]\n    year island    sex    Adelie Chinstrap Gentoo\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt;     &lt;int&gt;  &lt;int&gt;\n 1  2007 Biscoe    female      5         0     16\n 2  2007 Biscoe    male        5         0     17\n 3  2007 Dream     female      9        13      0\n 4  2007 Dream     male       10        13      0\n 5  2007 Torgersen female      8         0      0\n 6  2007 Torgersen male        7         0      0\n 7  2008 Biscoe    female      9         0     22\n 8  2008 Biscoe    male        9         0     23\n 9  2008 Dream     female      8         9      0\n10  2008 Dream     male        8         9      0\n11  2008 Torgersen female      8         0      0\n12  2008 Torgersen male        8         0      0\n13  2009 Biscoe    female      8         0     20\n14  2009 Biscoe    male        8         0     21\n15  2009 Dream     female     10        12      0\n16  2009 Dream     male       10        12      0\n17  2009 Torgersen female      8         0      0\n18  2009 Torgersen male        8         0      0\n\n\nOkay so we now have a count of different sexes of species of penguins measured in different years and islands\nLets make this data long, to do this we have to tell the function which columns are to be pivoted, and what we want to call the new columns.\nWe can either tell it which columns should or should not be pivoted, or we can even say which position columns to use with numbers. (using the colon means from one thing to the other thing)\n\nLongCounts_1&lt;-WideCounts %&gt;% \n  pivot_longer(c(Adelie,Chinstrap,Gentoo),names_to = \"species\",values_to = \"Number\")\n\nLongCounts_2&lt;-WideCounts %&gt;% \n  pivot_longer(-c(year,island,sex),names_to = \"species\",values_to = \"Number\")\n\nLongCounts_3&lt;-WideCounts %&gt;% \n  pivot_longer(4:6,names_to = \"species\",values_to = \"Number\")\n\nLongCounts_4&lt;-WideCounts %&gt;% \n  pivot_longer(-c(1:3),names_to = \"species\",values_to = \"Number\")\n\nLets check they are all the same to finish off.\n\nall.equal(\n  LongCounts_1,\n  LongCounts_2\n)\n\n[1] TRUE\n\nall.equal(\n  LongCounts_3,\n  LongCounts_4\n)\n\n[1] TRUE\n\nall.equal(\n  LongCounts_1,\n  LongCounts_4\n)\n\n[1] TRUE",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Data Manipulation in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html",
    "href": "BasicRTutorials/IntroductionR.html",
    "title": "Introduction to R",
    "section": "",
    "text": "To install R we can install it from the internet by either googling or\nFor Windows: https://cran.r-project.org/bin/windows/base/\nFor Mac: https://cran.r-project.org/bin/macosx/\nFor Linux: https://cran.r-project.org/bin/linux/\n\n\n\n\nR is an open source statistical programming language. It can be used through many Graphic User Interfaces (GUI) my preference is to use RStudio but VSCode is good and you can also code in base R (as well as many others).\nWe can install RStudio from https://posit.co/download/rstudio-desktop/",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#simple-mathematics",
    "href": "BasicRTutorials/IntroductionR.html#simple-mathematics",
    "title": "Introduction to R",
    "section": "Simple Mathematics",
    "text": "Simple Mathematics\n\n6*6\n\n[1] 36\n\n4*3/4\n\n[1] 3\n\n4+3/5\n\n[1] 4.6\n\n(4+3)/5 # mathematical ordering matters!\n\n[1] 1.4\n\nsqrt(144)\n\n[1] 12\n\npi\n\n[1] 3.141593\n\n\nWe can use either &lt;- or = to assign a value, list or dataframe into an object, thus saving it to R’s global environment for use later\nAn object is something (usually some sort of data) that is saved in temporary memory (global environment)\n\na&lt;- 17",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#functions",
    "href": "BasicRTutorials/IntroductionR.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\nIn R we can use functions to do tasks for us, they normally precede a parenthesis (), some are named after what they do and some are less well named,\nWithin functions there are ‘arguments’ (like options), what you put into these arguments will define how they perform.\nOne function used very often is c(),\nWe use c() to concatenate elements together, which means combine them into a vector, which is a series of values\n\nb&lt;- c(1,5,5,3,7)\n\nWe can apply functions to an object\n\nmean(b)\n\n[1] 4.2\n\n\nIf we want to check the documentation for a package we can go to the Help window, or type ? before the function name.\n\n?mean()\n\nWe can then perform different functions between objects\n\na*b\n\n[1]  17  85  85  51 119\n\nmean(a*b)\n\n[1] 71.4\n\n\nWe can even save the results to a new object\n\np&lt;-a*b\n\nThen we can look at what is in the new object by running the object or printing it (print())\n\np\n\n[1]  17  85  85  51 119\n\nprint(p)\n\n[1]  17  85  85  51 119\n\n\nWe can also create data systematically with R\nFor example a sequence of 10 values going up by 1\n\nSequence&lt;-seq(from=1,to=10,by=0.5)\n\nSequence\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\nAnotherSequence&lt;-c(1:200)\n\nAnotherSequence\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200\n\n\nWe will come back to generating data systematically later.",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#data-types",
    "href": "BasicRTutorials/IntroductionR.html#data-types",
    "title": "Introduction to R",
    "section": "Data Types",
    "text": "Data Types\nIn R there are many different types of data, the most common four are Numeric, Integer, Character and Factor.\nLogical and Complex are also data types but very rarely used explicitly. (logical is used a lot by functions but we rarely use it ourselves)\nNumeric data is any real numbers so 8 or 12.3 or 1.00000002 etc, while Integer data is just whole numbers 3, 4, 111 etc\nCharacter data are words or letters surrounded by quotations (either ” or ‘) such as “A”, “Red”, ’Treated’, Character data has no order to it in Rs ‘mind’\nFactor data is like character data but r (or you) have assigned an order to it e.g. “A”, “B”, “C”",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#objects",
    "href": "BasicRTutorials/IntroductionR.html#objects",
    "title": "Introduction to R",
    "section": "Objects",
    "text": "Objects\nAs we saw above we can store data in R as an Object, these can be many different types and combinations,\nThe most common Object types are Vectors, Lists, Matrices, DataFrames and Arrays,\nThe main differences of these Object types are what types and combinations of data can be stored in them and how many dimensions they have,\n\nVector\nA single group of one data type (it could be Numeric, Character, Integer, Factor), with one dimension is called a Vector.\n\nVector_Numeric&lt;- c(1.3,5.8,5.122,3.00,7.12)\n\nVector_Integer&lt;- as.integer(c(1,5,5,3,7)) \n# we change between data types with these functions \n\nVector_Character&lt;- c(\"This\",\"is\",\"A\",\"Character Vector\")\n\nVector_Factor&lt;-as.factor(c(\"This\",\"is\",\"A\",\"Character\", \"Vector\")) \n# Notice how r automatically orders alphabetically if we don't tell it the order\n\nWe can also change between types (even if they don’t fit that description)\n\nConvert_Numbers_To_Characters&lt;-as.character(Vector_Numeric)\n\nConvert_Numbers_To_Characters\n\n[1] \"1.3\"   \"5.8\"   \"5.122\" \"3\"     \"7.12\" \n\n\nNow our numbers are thought of as characters, so we can’t apply numeric operations to them!\n\n\nMatrix\nMultiple groups of one data type (it could be Numeric, Character, Integer, Factor), with two dimensions is called a Matrix.\n\nMatrix_Numeric&lt;- as.matrix(c(1.3,5.8,5.122,3.00,7.12))\n\nMatrix_Character&lt;- as.matrix(c(\"This\",\"is\",\"A\",\"Character Matrix\"))\n\n\n\nDataFrame\nMultiple groups of a combination of data types (it could be Numeric, Character, Integer, Factor), with two dimensions is called a Dataframe. Each element of a dataframe must be the last length as the other elements.\n\ndf&lt;-data.frame(Column1=c(1.3,5.8,5.122,3.00,7.12),Column2=c(1,5,5,3,7),Column3=Vector_Factor)\n\n\n\nList\nMultiple groups of a combination of data types or object types (it could be Numeric, Character, Integer, Factor or vectors, dataframes or matrices of these), with two dimensions is called a List. Each element in a list can be a different length to the other elements.\n\nList_Numeric&lt;-list(c(1.3,5.8,5.122,3.00,7.12),\n                   c(1,5,5,3,7))\n\nList_From_Vectors&lt;-list(Vector_Character,Matrix_Numeric,Matrix_Numeric)\n\nList_Different_Lengths&lt;-list(Item1=c(1,2,3,4,5,6),Item2=c(\"a\",\"B\",\"C\",\"D\"), Item3=seq(from=1,to=100,by=1))\n\nList_Different_Lengths\n\n$Item1\n[1] 1 2 3 4 5 6\n\n$Item2\n[1] \"a\" \"B\" \"C\" \"D\"\n\n$Item3\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\n\n\nArray\nMultiple groups of one data type (it could be Numeric, Character, Integer, Factor or vectors or matrices), with more than two dimensions is called an Array.\n\nArray_1d&lt;-array(c(Matrix_Numeric,c(1.3,5.8,5.122,3.00,7.12)),dim=c(5))\nArray_2d&lt;-array(c(Matrix_Numeric,c(1.3,5.8,5.122,3.00,7.12)),dim=c(5,2))\nArray_3d&lt;-array(c(Matrix_Numeric,c(1.3,5.8,5.122,3.00,7.12)),dim=c(5,2,2))\n\nArrays are rarely used so probably won’t discuss much further.\n\n\n\nTable of Object Types and Their Dimensions",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#indexing",
    "href": "BasicRTutorials/IntroductionR.html#indexing",
    "title": "Introduction to R",
    "section": "Indexing",
    "text": "Indexing\nObjects have dimensions and we can use a technique called indexing to select specific elements of an object\nWe use square brackets to do this,\nIf the object is 1 dimensional, one number will return one value\n\nVector_Numeric[4]\n\n[1] 3\n\n\nIf the object is 2 dimensional, one number will return that column\n\ndf[2]\n\n  Column2\n1       1\n2       5\n3       5\n4       3\n5       7\n\n\nby adding a comma we can select the from both dimensions (rows first, then columns)\n\ndf[4,2]\n\n[1] 3\n\n\nIf we want all rows but only a specific column we add a comma without a number\n\ndf[,2]\n\n[1] 1 5 5 3 7\n\n\nAnd vice verse\n\ndf[2,]\n\n  Column1 Column2 Column3\n2     5.8       5      is\n\n\nWe can also use multiple numbers inside c() to select multiple elements\nFor example, row 4 and 2 of all columns\n\ndf[c(2,4),]\n\n  Column1 Column2   Column3\n2     5.8       5        is\n4     3.0       3 Character\n\n\nOr we can use -c() to select all but the mentioned elements\nFor example, all rows but not columns 2 and 4\n\ndf[,-c(2,4)]\n\n  Column1   Column3\n1   1.300      This\n2   5.800        is\n3   5.122         A\n4   3.000 Character\n5   7.120    Vector",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#packages",
    "href": "BasicRTutorials/IntroductionR.html#packages",
    "title": "Introduction to R",
    "section": "Packages",
    "text": "Packages\nR relies upon packages, groups of specific functions, which can be installed from the internet and then loaded into a script.\nBase R, a package already installed and loaded within R, is very powerful and useful but less user friendly for some tasks.\nFrom Base R we can use the install.packages() function to install a package from online repositories.\nR assumes you want to download packages from CRAN (the official online repository but sometimes you might want to download from other repositories)\n\n#install.packages(\"dplyr\") \n\nYou only have to do this when you first want the package or want to update it or when you have updated r.\nOnce a package is installed we have to tell R that we want to use functions from this package so we load it\n\nlibrary(dplyr) \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nThis needs to be run every new R session when this package is used.\nWe can now run functions from the dplyr library, specifically dplyr is a package, which is part of a group or ‘ecosystem’ of packages called the tidyverse\nWe will use this group of packages for reading and writing data into and out of R (readr), manipulating and organising data (dplyr and tidyr) and visualisng data (ggplot2)",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#data-inspection",
    "href": "BasicRTutorials/IntroductionR.html#data-inspection",
    "title": "Introduction to R",
    "section": "Data Inspection",
    "text": "Data Inspection\nFirst we can make some data into a dataframe, explore this data, then save it as a file and then read the file back into r.\nR has some very useful random and non-random data generation functions\n\nYear &lt;- seq(from=1950,to=2023,by=1)\nTreatment &lt;- c(\"Control\",\"Treatment 1\",\"Treatment 2\")\nRep&lt;- seq(from=1,to=10,by=1)\n\nThese are three vectors, which we can check information about them with a few simple functions\n\nlength(Year)\n\n[1] 74\n\nsummary(Year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1950    1968    1986    1986    2005    2023 \n\nlength(Treatment)\n\n[1] 3\n\nsummary(Treatment)\n\n   Length     Class      Mode \n        3 character character \n\nlength(Rep)\n\n[1] 10\n\nsummary(Rep)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00 \n\n\nWe want to combine these vectors so we have a row for each rep, year and treatment, we can do this by expanding the grid and create a new dataframe called df.\nWe can inspect specific elements of a dataframe too\n\ndf&lt;-expand.grid(Year=Year,Treatment=Treatment,Rep=Rep)\n\nclass(df) # type of object\n\n[1] \"data.frame\"\n\nnrow(df) # number of rows\n\n[1] 2220\n\nncol(df) # number of columns\n\n[1] 3\n\ndim(df) # dimensions of object\n\n[1] 2220    3\n\nhead(df) # the first 6 rows of the df\n\n  Year Treatment Rep\n1 1950   Control   1\n2 1951   Control   1\n3 1952   Control   1\n4 1953   Control   1\n5 1954   Control   1\n6 1955   Control   1\n\ntail(df) # the last 6 rows of the df\n\n     Year   Treatment Rep\n2215 2018 Treatment 2  10\n2216 2019 Treatment 2  10\n2217 2020 Treatment 2  10\n2218 2021 Treatment 2  10\n2219 2022 Treatment 2  10\n2220 2023 Treatment 2  10\n\n\nThis df is all the meta data we want for our dataframe that we want to now make up some response data\n\nResponse&lt;-rnorm(n=nrow(df),mean = 15,sd=8) # we need the response to be same length as the df so we use nrow() for the number of values we want.\n\nWe can then combine this to our df, the dollar sign is used to select one dimension (column) from within an object (here a dataframe)\nthe column Response isn’t present in the data but by assigning our Response vector to it with df$Response &lt;- Response it adds a new column called Response to the dataframe df.\n\ndf$Response&lt;-Response",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#saving-and-loading-data",
    "href": "BasicRTutorials/IntroductionR.html#saving-and-loading-data",
    "title": "Introduction to R",
    "section": "Saving and Loading Data",
    "text": "Saving and Loading Data\n\nData Writing\nOnce we have our data set we can save it to our computer, but where that is on our computer is important.\nTo do this we need to know where R is looking for files on your computer. This is called your current working directory.\nThis information is displayed at the top of the console in Rstudio or you can use the base R function getwd().\nWe can set our working directory to change where this is in r (not recommended normally), or we can use our saving/loading functions to look in the correct folders (recommended).\nSide Note: there is a method for not really needing either of these called using Projects (highly recommended) but that is a bit more advanced so lets leave that for now.\nLets find out where our current working directory is, we can then create a new folder in that location, then save our df to that location.\n\ngetwd()\n\ndir.create(\"NewFolderName/\")\n\nWe now can save the df we created to this new folder using the write.csv() function from base r, or even better the write_csv() function from the readr package\nTo save to our folder we only need to say the directory we want the file saved to, followed by a /, then the name of the file with file extension.\nInside reading and writing functions such as write_csv() the main argument will be where is the file to be save to or taken from and we write this out as a character string inside quotations.\n\n# install.packages(\"readr\")\n\nlibrary(readr)\n\nwrite_csv(df,\"NewFolderName/OurNewFile.csv\")\n\n\n\nData Reading\nOften we don’t want to make fake data as done here, but we will have our own data set that we want to read in from our computer to then clean, organise, manipulate, visualise, analyse and report on.\nThese data are normally saved as excel spreadsheets. However, Excel is awful and should never be used for reproducible science! That being said it is often where a data spreadsheet starts before we bring it into R.\nExcel spreadsheets (.xsl) have lots of added information that actually is not needed and becomes complicated to work with so the easiest file format to read into R is a Comma Separated Values sheet (.csv)\nWe can convert our spreadsheet in Excel to a .csv file, then we read in the .csv file with the base function read.csv() or even better the readr function read_csv().\nAgain, inside reading and writing functions such as read_csv() the main argument will be where the file is. we write this out with a character string inside quotations.\nTo navigate up or down inside folders on your computer you use / to signify a folder, with the highest level folder on the far left\nFor example:\n\nMy_DF&lt;-read_csv(\"NewFolderName/OurNewFile.csv\")",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataVisualisation.html",
    "href": "BasicRTutorials/DataVisualisation.html",
    "title": "Data Visualisation in R",
    "section": "",
    "text": "Lets make some data as we did in the introduction\n\nYear &lt;- seq(from=1950,to=2023,by=1)\nTreatment &lt;- c(\"Control\",\"Treatment 1\",\"Treatment 2\")\nRep&lt;- seq(from=1,to=10,by=1)\n\ndf&lt;-expand.grid(Year=Year,Treatment=Treatment,Rep=Rep)\n\nResponse&lt;-rnorm(n=nrow(df),mean = 15,sd=8) \n\ndf$Response&lt;-Response\n\n\n\nWe can check the data we have using a base plot function\nThe base plot() function will create a grid that is each column of the data set plotted against all the others, this is fine for continuous data (Such as bill_depth_mm)\nbut it is hard to understand or read fully.\n\nplot(df)\n\n\n\n\n\n\n\n\nWe can define x and y axes from columns of the data set\n\nplot(x=df$Year,y=df$Response)\n\n\n\n\n\n\n\n\nIt automatically does a scatter plot, maybe we want to colour different treatments differently? and perhaps some axis labels that are more clear?\n\nplot(x=df$Year,\n     y=df$Response,\n     col=df$Treatment, \n     xlab=\"Year\",\n     ylab=\"Response\")\n\n\n\n\n\n\n\n\nSome of the arguments in this function are well named but you need to know the names of the arguments to properly use a function best,\nyou can easily find documentation of what arguments are within a function by typing two ? before the function in the console like so:\n\n?plot()\n\n# plot is a common function name \n# so we have to go to the base::plot() section of the help\n\nWith the points coloured we should add a legend to the plot, this is accomplished with another function run after the plot function.\n\nplot(x=df$Year,\n     y=df$Response,\n     col=unique(df$Treatment), \n     xlab=\"Year\",\n     ylab=\"Response\",\n     pch=19)\nlegend(x = \"topright\",          # Position\n       legend = unique(df$Treatment),  # Legend texts\n       col = c(1,2, 3),           # point colors\n       pch=19)                    # point type\n\n\n\n\n\n\n\n\nHere we can see that the response is totally random, regardless of treatment and year.\nFor interesting plotting we can change this, we can add patterns and jitter (random noise) to our data based on its treatment.\nWe will use some of our data manipulation code to do this.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf_1&lt;-df %&gt;% \n  mutate(Response_updated=case_when(\n    Treatment==\"Control\"~jitter(Response)*((Year-1930)/5)-50,\n    Treatment==\"Treatment 1\"~jitter(Response)*((Year-1930)*-2)+1000,\n    Treatment==\"Treatment 2\"~jitter(Response)*((Year-1930)*1.5)-4))\n\nWe can now calculate some summary statistics such as mean or standard deviation using group_by and summarise\n\ndf_1_summary&lt;-df_1 %&gt;% \n  group_by(Treatment) %&gt;% \n  summarise(MeanResponse=mean(Response_updated),\n            SDResponse=sd(Response_updated))\n\ndf_1_summary\n\n# A tibble: 3 × 3\n  Treatment   MeanResponse SDResponse\n  &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n1 Control             124.       114.\n2 Treatment 1        -687.      1146.\n3 Treatment 2        1221.       852.\n\ndf_1_summary_year&lt;-df_1 %&gt;% \n  group_by(Treatment,Year) %&gt;% \n  summarise(MeanResponse=mean(Response_updated),\n            SDResponse=sd(Response_updated))\n\n`summarise()` has grouped output by 'Treatment'. You can override using the\n`.groups` argument.\n\ndf_1_summary_year\n\n# A tibble: 222 × 4\n# Groups:   Treatment [3]\n   Treatment  Year MeanResponse SDResponse\n   &lt;fct&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 Control    1950         3.30       27.5\n 2 Control    1951         7.57       28.4\n 3 Control    1952        28.2        34.2\n 4 Control    1953        21.5        35.9\n 5 Control    1954        11.9        25.2\n 6 Control    1955        20.7        42.7\n 7 Control    1956        48.1        33.2\n 8 Control    1957        25.5        34.0\n 9 Control    1958        60.3        49.0\n10 Control    1959        30.7        24.4\n# ℹ 212 more rows\n\n\n\n\n\n\n\n\nFor more complex (and very simple) plots ggplot2 is by far the best package!\nThe gg in ggplot2 and a lot of the associated packages stands for grammar of graphics, which is a very old framework for good visualisation.\nWe don’t need to go far into it, but it effectively relies on three elements:\n\nData\nMapping\nGeometry\n\nIn every plot we will want to plot Data, it may be 1 value, 1 dimensional, 2 dimensional…. all the way up to multi facetted and dimensional.\nWe then want to assign some element of the data to elements of the plot, this is called mapping.\nThis mapping will change based on the geometry (shape/style).\nFor example earlier with base plot we Mapped a column (Data) to be the x axis and another to the y. (base plot assumed our geometry was a scatter plot)\n\n\n\nUsing these principles we can build some very impressive and complicated plots.\nWe first make a blank ggplot saying what data we will include using ggplot().\nFor elements of the plot (aesthetics) that change with data from the df we use the aes() function to show what column we want to map to what element (mapping=aes()).\nThen we want to add layers (geometries) to the plot for what we want to plot and how we want it to look.\n\nggplot(data=df_1,\n       mapping=aes(x=Year,\n                   y=Response_updated,\n                   colour=Treatment))+\n  geom_point()\n\n\n\n\n\n\n\n\nThis looks good but could be nicer! There are many geom types to dictate the type of plot.\nDifferent geom types have different required aesthetic elements, we could check by using ??geom_point for example\nGenerally we use x, y, colour, fill and alpha (transparency). (Sometimes z or label or many others)\nUnlike base plotting ggplot creates a legend automatically, this becomes more complex as the plot becomes more complex but is generally a nice default.\n\nggplot(df_1,aes(x=Response_updated,colour=Treatment))+\n  geom_density()\n\n\n\n\n\n\n\nggplot(df_1,aes(x=Year,y=Response_updated,fill=Treatment))+\n  geom_col(alpha=0.5)\n\n\n\n\n\n\n\nggplot(df_1,aes(x=Treatment,y=Response_updated,fill=Treatment))+\n  geom_boxplot(alpha=0.5)\n\n\n\n\n\n\n\n\nWe can also add new data into the same plot, so we could use the summary data too, for this we add the data to the geom (geometry) we want it used for.\nHere we have both the fill and the colour with the same levels (Treatment), ggplot will automatically combine these into one legend.\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))\n\n\n\n\n\n\n\n\n\n\n\nWe can now change the style and appearance of the whole plot with the function theme()\nThere are also some preset theme functions like theme_classic() or theme_bw()\nWe can also change the appearance with our mapping elements to define the colours or fill we want using the scale_colour_manual() and scale_fill_manual() functions. This is also true for all mapping, the x or y axis can be edited with scale_x/y_*() functions).\nThere are many different scale functions within ggplot so we some times use a * to say that it could be any that we use.\nMaybe we want to set odd x breaks or flip the y axis upside down?\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_x_continuous(breaks = c(1955,1982,2010,2023))+ \n  scale_y_reverse()+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe could even separate plots by another column (facetting)\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  facet_wrap(~Rep, nrow=2)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can add up different layers but also edit all elements of the plot using either the theme() or scale_*_() functions\nThese are all the elements you can change within theme:\ntheme( line, rect, text, title, aspect.ratio, axis.title, axis.title.x, axis.title.x.top, axis.title.x.bottom, axis.title.y, axis.title.y.left, axis.title.y.right, axis.text, axis.text.x, axis.text.x.top, axis.text.x.bottom, axis.text.y, axis.text.y.left, axis.text.y.right, axis.ticks, axis.ticks.x, axis.ticks.x.top, axis.ticks.x.bottom, axis.ticks.y, axis.ticks.y.left, axis.ticks.y.right, axis.ticks.length, axis.ticks.length.x, axis.ticks.length.x.top, axis.ticks.length.x.bottom, axis.ticks.length.y, axis.ticks.length.y.left, axis.ticks.length.y.right, axis.line, axis.line.x, axis.line.x.top, axis.line.x.bottom, axis.line.y, axis.line.y.left, axis.line.y.right, legend.background, legend.margin, legend.spacing, legend.spacing.x, legend.spacing.y, legend.key, legend.key.size, legend.key.height, legend.key.width, legend.text, legend.text.align, legend.title, legend.title.align, legend.position, legend.direction, legend.justification, legend.box, legend.box.just, legend.box.margin, legend.box.background, legend.box.spacing, panel.background, panel.border, panel.spacing, panel.spacing.x, panel.spacing.y, panel.grid, panel.grid.major, panel.grid.minor, panel.grid.major.x, panel.grid.major.y, panel.grid.minor.x, panel.grid.minor.y, panel.ontop, plot.background, plot.title, plot.title.position, plot.subtitle, plot.caption, plot.caption.position, plot.tag, plot.tag.position, plot.margin, strip.background, strip.background.x, strip.background.y, strip.clip, strip.placement, strip.text, strip.text.x, strip.text.x.bottom, strip.text.x.top, strip.text.y, strip.text.y.left, strip.text.y.right, strip.switch.pad.grid, strip.switch.pad.wrap )\nEach one can be edited by its own element, so for a background it is a rectangle so we might say (inside theme()) but for a grid line it is a line.\nplot.background=element_rect(fill=“grey80”) this will mean the rectangular element of the plot background will be filled with the colour grey80\npanel.grid.major.y= element_line(colour = “green”,linetype = “dotdash”,linewidth =2) this will mean the axis lines element of the panel background will be a green dotdashed line of linewidth 2 with the colour grey80\nWe can add titles and subtitles or change the legend title inside the labs function (if we only change the fill title it will split the colour and fill legends),\nWe can even add geoms that are labels or texts or other shapes/lines not base on our data\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\")\n        \n        )\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\")\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1),\n        panel.grid.major.y= element_line(colour = \"green\",linetype = \"dotdash\",linewidth =2)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")+\n  geom_segment(aes(x=1960,xend=2020,y=2500,yend=-2500))\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1),\n        panel.grid.major.y= element_line(colour = \"green\",linetype = \"dotdash\",linewidth =2),\n        panel.grid.minor.x= element_line(colour = \"blue\",linetype = \"solid\",linewidth =3),\n        panel.grid.minor.y= element_line(colour = \"pink\",linetype = \"solid\",linewidth =2.5)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")+\n  geom_segment(aes(x=1960,xend=2020,y=2500,yend=-2500))\n\n\n\n\n\n\n\n\nThe above plot is horrendous but shows different important elements we might want to add or take away from plots.\nOnce we have got a plot to how we want it we can then save it as a file on our computer using the ggsave() function\nto do this we can save our plot as an object, then provide the location we want to save the file and its name and extension in quotations, and its size,\nwe will save to the current directory.\n\nPlot_To_Save&lt;-ggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\nggsave(\"This_Is_Our_First_Saved_Plot.png\", Plot_To_Save, width=10, height=10) # dont forget to put the file type at the end!!! we will use .png",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Data Visualisation in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataVisualisation.html#ggplot2",
    "href": "BasicRTutorials/DataVisualisation.html#ggplot2",
    "title": "Data Visualisation in R",
    "section": "",
    "text": "For more complex (and very simple) plots ggplot2 is by far the best package!\nThe gg in ggplot2 and a lot of the associated packages stands for grammar of graphics, which is a very old framework for good visualisation.\nWe don’t need to go far into it, but it effectively relies on three elements:\n\nData\nMapping\nGeometry\n\nIn every plot we will want to plot Data, it may be 1 value, 1 dimensional, 2 dimensional…. all the way up to multi facetted and dimensional.\nWe then want to assign some element of the data to elements of the plot, this is called mapping.\nThis mapping will change based on the geometry (shape/style).\nFor example earlier with base plot we Mapped a column (Data) to be the x axis and another to the y. (base plot assumed our geometry was a scatter plot)\n\n\n\nUsing these principles we can build some very impressive and complicated plots.\nWe first make a blank ggplot saying what data we will include using ggplot().\nFor elements of the plot (aesthetics) that change with data from the df we use the aes() function to show what column we want to map to what element (mapping=aes()).\nThen we want to add layers (geometries) to the plot for what we want to plot and how we want it to look.\n\nggplot(data=df_1,\n       mapping=aes(x=Year,\n                   y=Response_updated,\n                   colour=Treatment))+\n  geom_point()\n\n\n\n\n\n\n\n\nThis looks good but could be nicer! There are many geom types to dictate the type of plot.\nDifferent geom types have different required aesthetic elements, we could check by using ??geom_point for example\nGenerally we use x, y, colour, fill and alpha (transparency). (Sometimes z or label or many others)\nUnlike base plotting ggplot creates a legend automatically, this becomes more complex as the plot becomes more complex but is generally a nice default.\n\nggplot(df_1,aes(x=Response_updated,colour=Treatment))+\n  geom_density()\n\n\n\n\n\n\n\nggplot(df_1,aes(x=Year,y=Response_updated,fill=Treatment))+\n  geom_col(alpha=0.5)\n\n\n\n\n\n\n\nggplot(df_1,aes(x=Treatment,y=Response_updated,fill=Treatment))+\n  geom_boxplot(alpha=0.5)\n\n\n\n\n\n\n\n\nWe can also add new data into the same plot, so we could use the summary data too, for this we add the data to the geom (geometry) we want it used for.\nHere we have both the fill and the colour with the same levels (Treatment), ggplot will automatically combine these into one legend.\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))\n\n\n\n\n\n\n\n\n\n\n\nWe can now change the style and appearance of the whole plot with the function theme()\nThere are also some preset theme functions like theme_classic() or theme_bw()\nWe can also change the appearance with our mapping elements to define the colours or fill we want using the scale_colour_manual() and scale_fill_manual() functions. This is also true for all mapping, the x or y axis can be edited with scale_x/y_*() functions).\nThere are many different scale functions within ggplot so we some times use a * to say that it could be any that we use.\nMaybe we want to set odd x breaks or flip the y axis upside down?\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_x_continuous(breaks = c(1955,1982,2010,2023))+ \n  scale_y_reverse()+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe could even separate plots by another column (facetting)\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  facet_wrap(~Rep, nrow=2)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can add up different layers but also edit all elements of the plot using either the theme() or scale_*_() functions\nThese are all the elements you can change within theme:\ntheme( line, rect, text, title, aspect.ratio, axis.title, axis.title.x, axis.title.x.top, axis.title.x.bottom, axis.title.y, axis.title.y.left, axis.title.y.right, axis.text, axis.text.x, axis.text.x.top, axis.text.x.bottom, axis.text.y, axis.text.y.left, axis.text.y.right, axis.ticks, axis.ticks.x, axis.ticks.x.top, axis.ticks.x.bottom, axis.ticks.y, axis.ticks.y.left, axis.ticks.y.right, axis.ticks.length, axis.ticks.length.x, axis.ticks.length.x.top, axis.ticks.length.x.bottom, axis.ticks.length.y, axis.ticks.length.y.left, axis.ticks.length.y.right, axis.line, axis.line.x, axis.line.x.top, axis.line.x.bottom, axis.line.y, axis.line.y.left, axis.line.y.right, legend.background, legend.margin, legend.spacing, legend.spacing.x, legend.spacing.y, legend.key, legend.key.size, legend.key.height, legend.key.width, legend.text, legend.text.align, legend.title, legend.title.align, legend.position, legend.direction, legend.justification, legend.box, legend.box.just, legend.box.margin, legend.box.background, legend.box.spacing, panel.background, panel.border, panel.spacing, panel.spacing.x, panel.spacing.y, panel.grid, panel.grid.major, panel.grid.minor, panel.grid.major.x, panel.grid.major.y, panel.grid.minor.x, panel.grid.minor.y, panel.ontop, plot.background, plot.title, plot.title.position, plot.subtitle, plot.caption, plot.caption.position, plot.tag, plot.tag.position, plot.margin, strip.background, strip.background.x, strip.background.y, strip.clip, strip.placement, strip.text, strip.text.x, strip.text.x.bottom, strip.text.x.top, strip.text.y, strip.text.y.left, strip.text.y.right, strip.switch.pad.grid, strip.switch.pad.wrap )\nEach one can be edited by its own element, so for a background it is a rectangle so we might say (inside theme()) but for a grid line it is a line.\nplot.background=element_rect(fill=“grey80”) this will mean the rectangular element of the plot background will be filled with the colour grey80\npanel.grid.major.y= element_line(colour = “green”,linetype = “dotdash”,linewidth =2) this will mean the axis lines element of the panel background will be a green dotdashed line of linewidth 2 with the colour grey80\nWe can add titles and subtitles or change the legend title inside the labs function (if we only change the fill title it will split the colour and fill legends),\nWe can even add geoms that are labels or texts or other shapes/lines not base on our data\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\")\n        \n        )\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\")\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1),\n        panel.grid.major.y= element_line(colour = \"green\",linetype = \"dotdash\",linewidth =2)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")+\n  geom_segment(aes(x=1960,xend=2020,y=2500,yend=-2500))\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1),\n        panel.grid.major.y= element_line(colour = \"green\",linetype = \"dotdash\",linewidth =2),\n        panel.grid.minor.x= element_line(colour = \"blue\",linetype = \"solid\",linewidth =3),\n        panel.grid.minor.y= element_line(colour = \"pink\",linetype = \"solid\",linewidth =2.5)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")+\n  geom_segment(aes(x=1960,xend=2020,y=2500,yend=-2500))\n\n\n\n\n\n\n\n\nThe above plot is horrendous but shows different important elements we might want to add or take away from plots.\nOnce we have got a plot to how we want it we can then save it as a file on our computer using the ggsave() function\nto do this we can save our plot as an object, then provide the location we want to save the file and its name and extension in quotations, and its size,\nwe will save to the current directory.\n\nPlot_To_Save&lt;-ggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\nggsave(\"This_Is_Our_First_Saved_Plot.png\", Plot_To_Save, width=10, height=10) # dont forget to put the file type at the end!!! we will use .png",
    "crumbs": [
      "Home",
      "Basic R Tutorials",
      "Data Visualisation in R"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "",
    "text": "Bede Ffinian Rowe Davies Post-Doctoral Researcher bedeffinian@gmail.com Laboratoire ISOMer, UFR Sciences et Techniques Nantes University, France.\n\n\n\n\nMy name is Bede Ffinian Rowe Davies a Post-Doctoral Researcher in Marine Ecology and Remote Sensing at the University of Nantes. I am a researcher interested in the large scale changes in marine ecosystems in relation to anthropogenic activities and drivers, using novel techniques alongside traditional methods.\n\n\nThe current project I am taking part in aims to create scaleable and generaliseable classification models to use Spectral Reflectance measured from Satellite imagery to predict spatial extent of intertidal habitats across the whole of Northern Europe. This can assist biodiversity management over great spatial scales while also providing real-time data (assuming regular available satellite imagery).\n\n\n\n\n\n\n\n\nAfter my PhD I was a Post-Doctoral researcher at the University of Plymouth assessing changes in Blue Carbon stores around Jersey in the UK Channel Islands. The project aimed to assess the stored carbon within sedimentary habitats in Jersey. This information was then being used alongside spatial habitat data to create a preliminary estimate of total carbon within sedimentary habitats across Jersey’s territorial waters. Cores and Grabs were taken across Jersey’s waters, then prepared and analysed for their total and organic carbon content with CHN analysis.\n\n\n\nI completed my PhD based at the University of Plymouth. The title of my PhD was: The Effectiveness of Partially Protected Marine Areas for Ecosystem Based Fisheries Management. The work centred around the Marine Protected Area (MPA) in Lyme Bay (UK), which excluded all demersal towed fishing activity across 206km\\(^2\\) of seabed in 2008. University of Plymouth staff and students have monitored the benthic environment across the bay (inside and outside the MPA) annually, using a range of underwater sampling equipment.\nThe methods include:\n\nPassive Acoustic Monitoring (PAM)\nTowed Underwater Video Systems (TUVs)\nBaited Remote Underwater Video Systems (BRUVs)\n\n\nThe objective of my PhD was to assess how the protection affected the benthic ecosystems inside vs outside the MPA.\n\nMy duties included:\n\nthe continued application of annual surveys alongside other members of the team,\norganisation of the 12 year data sets for BRUVs and TUVs, and 5 years of PAM recordings,\nanalysis of BRUVs and PAM data sets,\nstatistical analysis of the time series data,\nassessment of the applicability of the methods,\nthe creation of journal style articles for publication.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\n\n\n\n\n\n\n\n\n\n\n\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation.",
    "crumbs": [
      "Home",
      "Who am I and What have I done?",
      "Dr. Bede Ffinian Rowe Davies"
    ]
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "",
    "text": "I have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\n\n\n\n\n\n\n\n\n\n\n\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation.",
    "crumbs": [
      "Home",
      "Who am I and What have I done?",
      "Dr. Bede Ffinian Rowe Davies"
    ]
  },
  {
    "objectID": "index.html#projects-1",
    "href": "index.html#projects-1",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "Projects",
    "text": "Projects\nI have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation.",
    "crumbs": [
      "Home",
      "Who am I and What have I done?",
      "Dr. Bede Ffinian Rowe Davies"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BetaGLMs.html",
    "href": "StatisticsTutorials/BetaGLMs.html",
    "title": "Beta GLMs",
    "section": "",
    "text": "Here we will use the Proportion of Gasoline yielded from Crude Oil after distillation and fractionation give the gravity of the crude oil, pressure of the crude oil, the temperature (in F) at which 10 percent of the crude oil had vaporised and temperature (in F) when all crude oil had vaporised. First of all, because it makes me uncomfortable, we will convert the temperatures to \\(\\circ\\)C.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(performance)\nlibrary(patchwork)\n\n# install.packages(\"betareg\")\n\nlibrary(betareg)\n\ndata(\"GasolineYield\", package = \"betareg\")\n \nglimpse(GasolineYield)\n\nRows: 32\nColumns: 6\n$ yield    &lt;dbl&gt; 0.122, 0.223, 0.347, 0.457, 0.080, 0.131, 0.266, 0.074, 0.182…\n$ gravity  &lt;dbl&gt; 50.8, 50.8, 50.8, 50.8, 40.8, 40.8, 40.8, 40.0, 40.0, 40.0, 3…\n$ pressure &lt;dbl&gt; 8.6, 8.6, 8.6, 8.6, 3.5, 3.5, 3.5, 6.1, 6.1, 6.1, 6.1, 6.1, 6…\n$ temp10   &lt;dbl&gt; 190, 190, 190, 190, 210, 210, 210, 217, 217, 217, 220, 220, 2…\n$ temp     &lt;dbl&gt; 205, 275, 345, 407, 218, 273, 347, 212, 272, 340, 235, 300, 3…\n$ batch    &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7…\n\ndf&lt;-GasolineYield %&gt;% \n  mutate(temp=(temp-32)*5/9,\n         temp10=(temp10-32)*5/9)\n\n\n\n\nHere we will see if the the temperature that the crude oil totally evaporates and the pressure of the crude oil effects the proportional yield of gasoline from that crude oil.\nThis is a fairly simple model with two fixed effect and can be written as:\nYield of Gasoline ~ Pressure + Temperature\n\n\n\nAs a proportion, Yield of Gasoline can be between 0 and 1, but all real numbers between these upper and lower limits. Therefore it will be a Beta distribution.\n\n\n\nOur data are fairly well distributed across the values\n\np1&lt;-ggplot(df,aes(x=temp))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Temperature (°C)\",y=\"Density\")\n\np2&lt;-ggplot(df,aes(x=pressure))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Pressure\",y=\"Density\")\n\np1+p2\n\n\n\n\n\n\n\n\nAs these are looking fine, we shall fit out model with a Beta distribution. The Beta family is not initiated in GLM so we could use the betareg function from the betareg package. However, for predicting standard error and syntax reasons this is very different from all the GLMs we have already carried out. So we will use the gam() function. This is from the mgcv package and used to model non linear General Additive Models using something called splines. We can use it without using splines and it will behave identically to the glm() function. Infact, for all the GLM tutorials we could have swapped the glm() function for gam(). We will cover GAMs later on but GAMs are GLMs that have smooth terms attached to fixed effects.\n\n# install.packages(\"mgcv\")\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\nglm1 &lt;- gam(yield ~ pressure+temp, data = df, family = betar(link=\"logit\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nThese plots look fairly good, very little patterns in the Fitted vs Residuals and the points generally follow the 1:1 qqnorm plot, with a few points and low and high values not following the line.\n\nsummary(glm1)\n\n\nFamily: Beta regression(119.257) \nLink function: logit \n\nFormula:\nyield ~ pressure + temp\n\nParametric coefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.180079   0.261752  -19.79   &lt;2e-16 ***\npressure     0.174289   0.016830   10.36   &lt;2e-16 ***\ntemp         0.017455   0.001261   13.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.894   Deviance explained = 91.1%\n-REML = -54.103  Scale est. = 1         n = 32\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Yield\\; of\\;  Gasoline = Beta(y',\\phi)\\]\n\\[y'=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Pressure \\\\\n\\beta_{2} Temperature \\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Beta distribution requires two shape parameters (\\(y'\\) and \\(\\phi\\)), where \\(y'\\) must be above zero and below 1, we must convert out linear equation results (\\(y\\)) so that it is bound between 0 and 1. This means we use the link function, which for Beta models is by default logit. We can use a different link function if we want such as probit or clogit. But generally the default is good.\n\nglm1$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nAs always we then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with pressure and temperature the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average yield of gasoline based on those pressures and temperatures.\nWe can tell the predict function to get standard errors. Currently, there are no simple methods for estimating the standard error for a betareg object. This is why we used the gam() function. We will only predict three temperature values to make things easier to plot and we can colour by those temperatures.\n\nNewData_1&lt;-expand_grid(pressure=seq(min(df$pressure),max(df$pressure),length.out=50),\n                      temp=c(100,150,200)\n                      )\n\nPred&lt;-predict(glm1,NewData_1,se.fit=T,type=\"response\")\n\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewData)+\n  geom_ribbon(aes(x=pressure,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    fill=as.factor(temp)),\n              alpha=0.7)+\n  geom_line(aes(x=pressure,\n                 y=response,\n                  colour=as.factor(temp)))+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  labs(x=\"Pressure\",y=\"Response Variable (Yeild of Gasoline)\",\n       fill=\"Temperature (°C)\",colour=\"Temperature (°C)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked. To colour by temperature in the original data we will set groups\n\ndf_1&lt;-df %&gt;% \n  mutate(temp=case_when(temp&lt;=125~100,\n                              temp&gt;125 & temp&lt;=175~150,\n                              temp&gt;175~200))\n\nggplot(NewData)+\n  geom_point(data=df_1,aes(x=pressure,y=yield,colour=as.factor(temp)))+\n  geom_ribbon(aes(x=pressure,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    fill=as.factor(temp)),\n              alpha=0.7)+\n  geom_line(aes(x=pressure,\n                 y=response,\n                  colour=as.factor(temp)))+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  labs(x=\"Pressure\",y=\"Response Variable (Yeild of Gasoline)\",\n       fill=\"Temperature (°C)\",colour=\"Temperature (°C)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nThis looks quite good. However, again it is based off of very little data, only 32 data points, which could be misleading. However it illustrates our point well and shows that from this data, given our assumptions and understanding of the effects, Yield increases with increasing pressure and also increases with increasing temperature.\n\n\n\n\nThis dataset is a number of reading accuracy scores from children with and without dyslexia as well as their non-verbal IQ scores.\n\n\ndata(\"ReadingSkills\", package = \"betareg\")\n\n\n\nWe want to assess, from this data, if reading accuracy is increase by an individuals non-verbal IQ and if this effect is influenced by them having been diagnosed as dyslexic or not.\nWe can write the stats model as:\nAccuracy ~ IQ*Dyslexia\n\n\n\nHere the accuracy score is a value from 0 to 1, as all accuracy scores have to logically be. Therefore, we will use the Beta distribution.\n\n\n\nLets check all our fixed effects.\n\np1&lt;-ggplot(ReadingSkills,aes(x=dyslexia))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Dyslexia Diagnosis\",y=\"Count\")\n\n\np2&lt;-ggplot(ReadingSkills,aes(x=iq))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Non-Verbal IQ\",y=\"Density\")\n\n\n(p1+p2)\n\n\n\n\n\n\n\n\nThese both look well spread or evenly spread generally. Although less Dyslexic diagnoses.\n\nglm2&lt;-gam(accuracy ~ iq*dyslexia,data=ReadingSkills, family = betar(link=\"logit\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nHere the qqnorm plot is okay, with a bit of under prediction but nothing too wrong. Now these residuals vs fitted values look horrible. However, for homoskedastity in residuals we want a mirror image above and below the 0 line. We are actually getting a pretty good mirror image, our data is just split between two clear groups. This may be an issue that we haven’t come across or we may chose to ignore it. Here we will proceed with caution.\n\nsummary(glm2)\n\n\nFamily: Beta regression(10.455) \nLink function: logit \n\nFormula:\naccuracy ~ iq * dyslexia\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.3205     0.1308  10.098  &lt; 2e-16 ***\niq            0.1582     0.1363   1.160    0.246    \ndyslexia     -0.9626     0.1308  -7.361 1.83e-13 ***\niq:dyslexia  -0.2156     0.1363  -1.582    0.114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.673   Deviance explained =   76%\n-REML = -46.461  Scale est. = 1         n = 44\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Reading \\;Accuracy = Beta(y',\\phi)\\]\n\\[y'=y^{-1}\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} IQ:Dyslexia\\\\\n+ \\beta_{2} Dyslexia\\\\\n+ \\beta_{3} IQ\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Beta distribution requires two shape parameters (\\(y'\\) and \\(\\phi\\)), where \\(y'\\) must be above zero and below 1, we must convert out linear equation results (\\(y\\)) so that it is bound between 0 and 1. This means we use the link function, which for Beta models is by default logit. We can use a different link function if we want such as probit or clogit. But generally the default is good.\n\nglm2$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age and diet the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average weight based on those ages and diets.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(iq=seq(min(ReadingSkills$iq),max(ReadingSkills$iq),length.out=100),\n                       dyslexia=as.factor(c(\"no\",\"yes\")))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=iq,ymax=Upr,ymin=Lwr,fill=dyslexia),\n              alpha=0.6)+\n  geom_line(aes(x=iq,y=response,colour=dyslexia))+\n  labs(x=\"Non-Verbal IQ\",y=\"Predicted Reading Accuracy\",\n       fill=\"Dyslexia\\nDiagnosis\",colour=\"Dyslexia\\nDiagnosis\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=ReadingSkills,aes(x=iq,y=accuracy,colour=dyslexia))+\n  geom_ribbon(aes(x=iq,ymax=Upr,ymin=Lwr,fill=dyslexia),\n              alpha=0.6)+\n  geom_line(aes(x=iq,y=response,colour=dyslexia))+\n  labs(x=\"Non-Verbal IQ\",y=\"Predicted Reading Accuracy\",\n       fill=\"Dyslexia\\nDiagnosis\",colour=\"Dyslexia\\nDiagnosis\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nHere we have some clear differences in reading accuracy seen between dyslexic diagnosis no and yes, but from this model we wouldn’t say this effect changes with non-verbal iq. However, as a topic to analyse Dyslexia is a far more complex subject than these 44 observations.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Beta GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BetaGLMs.html#data-loading---gasoline",
    "href": "StatisticsTutorials/BetaGLMs.html#data-loading---gasoline",
    "title": "Beta GLMs",
    "section": "",
    "text": "Here we will use the Proportion of Gasoline yielded from Crude Oil after distillation and fractionation give the gravity of the crude oil, pressure of the crude oil, the temperature (in F) at which 10 percent of the crude oil had vaporised and temperature (in F) when all crude oil had vaporised. First of all, because it makes me uncomfortable, we will convert the temperatures to \\(\\circ\\)C.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(performance)\nlibrary(patchwork)\n\n# install.packages(\"betareg\")\n\nlibrary(betareg)\n\ndata(\"GasolineYield\", package = \"betareg\")\n \nglimpse(GasolineYield)\n\nRows: 32\nColumns: 6\n$ yield    &lt;dbl&gt; 0.122, 0.223, 0.347, 0.457, 0.080, 0.131, 0.266, 0.074, 0.182…\n$ gravity  &lt;dbl&gt; 50.8, 50.8, 50.8, 50.8, 40.8, 40.8, 40.8, 40.0, 40.0, 40.0, 3…\n$ pressure &lt;dbl&gt; 8.6, 8.6, 8.6, 8.6, 3.5, 3.5, 3.5, 6.1, 6.1, 6.1, 6.1, 6.1, 6…\n$ temp10   &lt;dbl&gt; 190, 190, 190, 190, 210, 210, 210, 217, 217, 217, 220, 220, 2…\n$ temp     &lt;dbl&gt; 205, 275, 345, 407, 218, 273, 347, 212, 272, 340, 235, 300, 3…\n$ batch    &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7…\n\ndf&lt;-GasolineYield %&gt;% \n  mutate(temp=(temp-32)*5/9,\n         temp10=(temp10-32)*5/9)\n\n\n\n\nHere we will see if the the temperature that the crude oil totally evaporates and the pressure of the crude oil effects the proportional yield of gasoline from that crude oil.\nThis is a fairly simple model with two fixed effect and can be written as:\nYield of Gasoline ~ Pressure + Temperature\n\n\n\nAs a proportion, Yield of Gasoline can be between 0 and 1, but all real numbers between these upper and lower limits. Therefore it will be a Beta distribution.\n\n\n\nOur data are fairly well distributed across the values\n\np1&lt;-ggplot(df,aes(x=temp))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Temperature (°C)\",y=\"Density\")\n\np2&lt;-ggplot(df,aes(x=pressure))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Pressure\",y=\"Density\")\n\np1+p2\n\n\n\n\n\n\n\n\nAs these are looking fine, we shall fit out model with a Beta distribution. The Beta family is not initiated in GLM so we could use the betareg function from the betareg package. However, for predicting standard error and syntax reasons this is very different from all the GLMs we have already carried out. So we will use the gam() function. This is from the mgcv package and used to model non linear General Additive Models using something called splines. We can use it without using splines and it will behave identically to the glm() function. Infact, for all the GLM tutorials we could have swapped the glm() function for gam(). We will cover GAMs later on but GAMs are GLMs that have smooth terms attached to fixed effects.\n\n# install.packages(\"mgcv\")\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\nglm1 &lt;- gam(yield ~ pressure+temp, data = df, family = betar(link=\"logit\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nThese plots look fairly good, very little patterns in the Fitted vs Residuals and the points generally follow the 1:1 qqnorm plot, with a few points and low and high values not following the line.\n\nsummary(glm1)\n\n\nFamily: Beta regression(119.257) \nLink function: logit \n\nFormula:\nyield ~ pressure + temp\n\nParametric coefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.180079   0.261752  -19.79   &lt;2e-16 ***\npressure     0.174289   0.016830   10.36   &lt;2e-16 ***\ntemp         0.017455   0.001261   13.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.894   Deviance explained = 91.1%\n-REML = -54.103  Scale est. = 1         n = 32\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Yield\\; of\\;  Gasoline = Beta(y',\\phi)\\]\n\\[y'=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Pressure \\\\\n\\beta_{2} Temperature \\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Beta distribution requires two shape parameters (\\(y'\\) and \\(\\phi\\)), where \\(y'\\) must be above zero and below 1, we must convert out linear equation results (\\(y\\)) so that it is bound between 0 and 1. This means we use the link function, which for Beta models is by default logit. We can use a different link function if we want such as probit or clogit. But generally the default is good.\n\nglm1$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nAs always we then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with pressure and temperature the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average yield of gasoline based on those pressures and temperatures.\nWe can tell the predict function to get standard errors. Currently, there are no simple methods for estimating the standard error for a betareg object. This is why we used the gam() function. We will only predict three temperature values to make things easier to plot and we can colour by those temperatures.\n\nNewData_1&lt;-expand_grid(pressure=seq(min(df$pressure),max(df$pressure),length.out=50),\n                      temp=c(100,150,200)\n                      )\n\nPred&lt;-predict(glm1,NewData_1,se.fit=T,type=\"response\")\n\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewData)+\n  geom_ribbon(aes(x=pressure,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    fill=as.factor(temp)),\n              alpha=0.7)+\n  geom_line(aes(x=pressure,\n                 y=response,\n                  colour=as.factor(temp)))+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  labs(x=\"Pressure\",y=\"Response Variable (Yeild of Gasoline)\",\n       fill=\"Temperature (°C)\",colour=\"Temperature (°C)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked. To colour by temperature in the original data we will set groups\n\ndf_1&lt;-df %&gt;% \n  mutate(temp=case_when(temp&lt;=125~100,\n                              temp&gt;125 & temp&lt;=175~150,\n                              temp&gt;175~200))\n\nggplot(NewData)+\n  geom_point(data=df_1,aes(x=pressure,y=yield,colour=as.factor(temp)))+\n  geom_ribbon(aes(x=pressure,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    fill=as.factor(temp)),\n              alpha=0.7)+\n  geom_line(aes(x=pressure,\n                 y=response,\n                  colour=as.factor(temp)))+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  labs(x=\"Pressure\",y=\"Response Variable (Yeild of Gasoline)\",\n       fill=\"Temperature (°C)\",colour=\"Temperature (°C)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nThis looks quite good. However, again it is based off of very little data, only 32 data points, which could be misleading. However it illustrates our point well and shows that from this data, given our assumptions and understanding of the effects, Yield increases with increasing pressure and also increases with increasing temperature.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Beta GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BetaGLMs.html#data-loading---dyslexic-reading",
    "href": "StatisticsTutorials/BetaGLMs.html#data-loading---dyslexic-reading",
    "title": "Beta GLMs",
    "section": "",
    "text": "This dataset is a number of reading accuracy scores from children with and without dyslexia as well as their non-verbal IQ scores.\n\n\ndata(\"ReadingSkills\", package = \"betareg\")\n\n\n\nWe want to assess, from this data, if reading accuracy is increase by an individuals non-verbal IQ and if this effect is influenced by them having been diagnosed as dyslexic or not.\nWe can write the stats model as:\nAccuracy ~ IQ*Dyslexia\n\n\n\nHere the accuracy score is a value from 0 to 1, as all accuracy scores have to logically be. Therefore, we will use the Beta distribution.\n\n\n\nLets check all our fixed effects.\n\np1&lt;-ggplot(ReadingSkills,aes(x=dyslexia))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Dyslexia Diagnosis\",y=\"Count\")\n\n\np2&lt;-ggplot(ReadingSkills,aes(x=iq))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Non-Verbal IQ\",y=\"Density\")\n\n\n(p1+p2)\n\n\n\n\n\n\n\n\nThese both look well spread or evenly spread generally. Although less Dyslexic diagnoses.\n\nglm2&lt;-gam(accuracy ~ iq*dyslexia,data=ReadingSkills, family = betar(link=\"logit\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nHere the qqnorm plot is okay, with a bit of under prediction but nothing too wrong. Now these residuals vs fitted values look horrible. However, for homoskedastity in residuals we want a mirror image above and below the 0 line. We are actually getting a pretty good mirror image, our data is just split between two clear groups. This may be an issue that we haven’t come across or we may chose to ignore it. Here we will proceed with caution.\n\nsummary(glm2)\n\n\nFamily: Beta regression(10.455) \nLink function: logit \n\nFormula:\naccuracy ~ iq * dyslexia\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.3205     0.1308  10.098  &lt; 2e-16 ***\niq            0.1582     0.1363   1.160    0.246    \ndyslexia     -0.9626     0.1308  -7.361 1.83e-13 ***\niq:dyslexia  -0.2156     0.1363  -1.582    0.114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.673   Deviance explained =   76%\n-REML = -46.461  Scale est. = 1         n = 44\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Reading \\;Accuracy = Beta(y',\\phi)\\]\n\\[y'=y^{-1}\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} IQ:Dyslexia\\\\\n+ \\beta_{2} Dyslexia\\\\\n+ \\beta_{3} IQ\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Beta distribution requires two shape parameters (\\(y'\\) and \\(\\phi\\)), where \\(y'\\) must be above zero and below 1, we must convert out linear equation results (\\(y\\)) so that it is bound between 0 and 1. This means we use the link function, which for Beta models is by default logit. We can use a different link function if we want such as probit or clogit. But generally the default is good.\n\nglm2$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age and diet the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average weight based on those ages and diets.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(iq=seq(min(ReadingSkills$iq),max(ReadingSkills$iq),length.out=100),\n                       dyslexia=as.factor(c(\"no\",\"yes\")))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=iq,ymax=Upr,ymin=Lwr,fill=dyslexia),\n              alpha=0.6)+\n  geom_line(aes(x=iq,y=response,colour=dyslexia))+\n  labs(x=\"Non-Verbal IQ\",y=\"Predicted Reading Accuracy\",\n       fill=\"Dyslexia\\nDiagnosis\",colour=\"Dyslexia\\nDiagnosis\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=ReadingSkills,aes(x=iq,y=accuracy,colour=dyslexia))+\n  geom_ribbon(aes(x=iq,ymax=Upr,ymin=Lwr,fill=dyslexia),\n              alpha=0.6)+\n  geom_line(aes(x=iq,y=response,colour=dyslexia))+\n  labs(x=\"Non-Verbal IQ\",y=\"Predicted Reading Accuracy\",\n       fill=\"Dyslexia\\nDiagnosis\",colour=\"Dyslexia\\nDiagnosis\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Beta GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BetaGLMs.html#some-caveats-1",
    "href": "StatisticsTutorials/BetaGLMs.html#some-caveats-1",
    "title": "Beta GLMs",
    "section": "",
    "text": "Here we have some clear differences in reading accuracy seen between dyslexic diagnosis no and yes, but from this model we wouldn’t say this effect changes with non-verbal iq. However, as a topic to analyse Dyslexia is a far more complex subject than these 44 observations.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Beta GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html",
    "href": "StatisticsTutorials/IntroductionGLMs.html",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Statistical analysis in r is very easy (on the whole) but needs a bit more background to fully understand. But for most of our situations we will want to use a group of models called General Linear Models (GLMs) or if more complex General Linear Mixed Effects Models (GLMMs). These modelling frameworks plus the additions/alterations from their defaults have lots of different names but in ecology and specifically r the names here are the most commonly used.\n\n\nThese modelling frameworks can be used to model almost all situations that will arise in our work. We are almost always using models to describe relationships. We can then use those models to predict future events but this is less common and more nuanced so we might use other tools for that.\nIn more traditional “Fischer” style statistics there are a wealth of different tests for each different scenario such as t-test, ANOVA, Kruskal-Wallis, Friedman, Mann-Whitney-U etc. These tests were created, designed and developed from work by a researcher called Fischer, with many ecologists/researchers generally holding onto this style as it has been used and taught for decades. However, they were designed and created for very specific cases. This means that they are extremely powerful and useful in those situations but are almost always used in slightly different situations or scenarios and so lose their power.\n\n\n\nOld Stats Test - Image from https://onishlab.colostate.edu/\n\n\nGLMs and GLMMs can be used in all the same cases as these older style tests as well as many many more. GLMs and by extension GLMMs have the advantage of being more consistency with terminology, have higher flexibility and you do not need to know 1000 different tests to use them. There are important options to chose within GLMs and GLMMs but as I mention they are more consistent and easier to check or adhere to their assumptions.\n\n\n\nDiscussion, examples and code here about GLMs and GLMMs will be carried out and more aimed at Frequentist methods of analysis. However, as Richard McElreath points out amazingly in his Statistical Rethinking Course (GO CHECK IT OUT IT IS AMAZING AND FREE!! https://www.youtube.com/@rmcelreath or https://github.com/rmcelreath), the general principles are the same between Frequentist and Bayesian methods, the important part is interpretation and building scientific and statistical models to allow Causal Inference! (i.e. does the model actually provide us with evidence of a cause and if so what is that cause.)\n\n\n\nBayes vs Frequentists Killed by Doge - Flagrantly stolen from https://github.com/rmcelreath\n\n\n\n\n\nEffectively General Linear Models are ways of creating generalising lines or linear equations that best describe the data we put into them given some equation of interest.\nWe can summarise most with three equations. We have the linear equation \\(y = \\beta x + c\\) and how that relates to the data we sample (\\(s\\)) \\(s = Distribution(y,\\sigma)\\) .\nThis will often be written generally as:\n\\[s = Distribution(y',\\sigma)\\]\n\\[y'=f(y)\\]\n\\[y = \\beta x + c\\]\nThe first of these equations is saying that the data we recorded (\\(s\\)) comes from a some distribution (We will define this later) with a mean of \\(y'\\) and a standard deviation of \\(\\sigma\\). \\(y\\) is the result of an effect (\\(\\beta\\)) multiplied by some fixed effect (\\(x\\)) and an intercept value (\\(c\\)). Between these two equations we have a link function (\\(f()\\)). This converts \\(y\\) into a value that the distribution function will accept \\(y'\\), again we will define what these elements are later. The elements that we want our GLM to tell us is the effect of x (\\(\\beta\\)) and the intercept (\\(c\\)). We don’t have to do maths at all for GLMs but it can help some people understand what the model is telling us. (If you don’t like equations/maths ignore this bit - But we will discuss link functions later on).\n\n\n\nGenerally we will have a response variable (Dependent Variable) with one or more fixed effects (independent variables).\nHere I will discuss exclusively univariate modelling (where only one variable is the response variable) but multivariate analysis could follow the same framework, it is just more complicated to implement.\nOur research question will dictate how we create our scientific model, which dictates what statistical model to use and then how we interpret that statistical model.\nMany researchers like to draw out what is called a DAG, which diagrammatically shows assumed influences of factors, and how the directionallity of that influence. At first these are quite difficult to formulate until you have created a few. Therefore, the easiest way I can think to do this is saying some sort of hypothesis or statement in English then we can turn that statement into the Statistical model.\nFor example: I hypothesise that the flipper length of a penguin is described primarily by the species of that penguin but also influenced by its sex. Effectively Flipper length changes with Species and Sex.\n\n\n\nPenguins Image by Allison Horst\n\n\nHere our response variable is Flipper length (Dependant variable) and our fixed effects (Independent variables) are Species and Sex.\nIn statistical notation we could write this in a couple of ways depending on our assumptions:\n1). flipper_length_mm ~ sex * species\n2). flipper_length_mm ~ sex + species\nFirstly, we see on the left hand side of the ~ (tilde) we have our response variable, with our fixed effects on the other side.\nSecondly, we see that we either multiply or add these fixed effects. The difference between the two is subtle but important.\nEquation 1 is saying that flipper length is effected by species and by sexes but that the effect of sex is different depending on the species. (females always bigger in one species but males always bigger in another but one species is always much larger than the other).\nEquation 2 is saying that flipper length is effected by species and by sexes but that the effect of sex is the same or similar regardless of the species. (males always bigger across all species, but sizes different across species).\nWe may think either equation is correct given our knowledge of penguins.\n\n\n\nThe next most important element is what type of data our response variable is. This is the step that is often carried out wrong and leads to many inaccuracies of analysis.\nWhen modelling we are using our sample to make inference about a population. Therefore, our model should be based on our assumptions of the population that we took the sample from.\nThe type of data your response variable comes from may not be exactly what your response variable data looks like! However, if we have sufficiently sampled the population and not introduced any selection or sampling bias we should be confident in saying our sample (response variable) comes from the distribution of the population. We should be aware of this when analysing data from specific geographies or demographics etc!!!\nSo we should be able to tell the type of response variable data before we even collect the samples themselves! WE SHOULD NOT LOOK AT A GRAPH OF OUR DATA AND CHOOSE THE DISTRIBUTION IT COMES FROM JUST BY THE SHAPE!!!! (Speaking from experience and errors made!)\n\n\nThere are a group of widely used distributions that will cover almost all situations.\nThey are:\n\nGaussian\nBeta\nGamma\nPoisson\nBinomial/Bernoulli\n\nThere are many, many, many others such as Student-t or Direchlet or Tweedie. All of which are very useful and utilised throughout science but we won’t go through them here.\n\n\n\nDistributions - https://learnbyinsight.com/2020/09/27/probability-distribution-an-aid-to-know-the-data/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo when deciding what type of model to run we can first think whether our data are numerica or categoric. We will only discuss numerical data (temperature or weight or numbers of fish) but sometimes we might have a categorical response variable, such as questionnaire answers. Within these types of model there might be order to the categories (Not likely, Somewhat likely, Very likely) or there is no logical order (green, blue, purple). There are mixed effects models for these that are widely used, such as Multinomial Logisitic Regression for non-ordered and Ordinal Logistic Regression for ordered categorical response variables, but again we won’t discuss them here.\n\n\n\nOnce we know our data are numeric, we need to choose what type: continuous or integers. This is quite an easy one, can you measure it in decimals or is it complete units. When studying nesting birds we don’t count half eggs; a nest will have 1, 2, 3 etc. eggs. But if we are measuring the width of a tree it might be 4 cm or 2.365441 m.\n\n\n\n\n\n\n\n\n\nNest - https://www.thespruce.com\n\n\n\n\n\n\n\nTree - https://fennerschool-associated.anu.edu.au\n\n\n\n\n\n\n\n\nOnce we know our data are continuous numeric, we can think can the values be negative? A response variable that is continuous and can be negative is mostly modelled under the most common distribution: Gaussian. Examples of this could be temperature measured in \\(^\\circ\\)C or acceleration. A response variable that cannot be negative or even 0, would most correctly be: Gamma. However, Gaussian models are very often used for these models as a Guassian distribution is simpler computationally and mathematically, and only really causes issues when the values approach 0. This is actually most continuous values we measure, such as thickness of a stem, fish length, body mass or area of habitat.\n\n\n\n\n\n\n\n\n\n\n\n\nIf your response variable is continuous, and it may be negative or not, but is bound between an upper and lower known bound then it would most correctly be: Beta. For most practical terms Beta distribution is between 0 and 1. However, most data between bounds could be scaled to 0 and 1 and keep all of their characteristics. For example, percentage or proportional data are effectively between 0 and 1 and can be modelling using a Beta distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nWe know our data are integers but do we know whether there is an upper limit? If our data could potentially be very big as counts then we will use the Poisson distribution. This is for response variables like number of eggs in a nest or abundance of fish or the number of different species found along a transect. If we know there is an upper limit to the integers we could get then we will use a Binomial model. This could be for the number of eggs inside a six egg carton. The most common version of this is when we have a binary response variable (either 1 or 0). Presence or absence of a rare species for example. This is binary Binomial but is sometimes referred to specifically as Bernoulli.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now know what distribution we will most likely use to analyse our response data, so the next step is organising our fixed effects so the model treats them how we would like them to be treated. Effectively fixed effects are either numeric or categorical variables. With GLMs there is no different name (nor changes in code) generally used for what combination of numeric or categorical variables are used. So we could have multiple numeric effects, multiple categorical factors or a mixture of both.\n\n\nIn r numeric effects are quite straight forward, “what is the effect on y with a one unit change of value of x?”. However, if our numeric column has really big values, big range and a skewed distribution we may have issues with our model not working or running as we expect. For fixed effects this is easy to overcome, generally we would use some sort of transformation that maintains the structure of the data but minimises the issues the model runs into.\n\nSkewed Data\n\nFor example for skewed data, if our fixed effect was based on population size with the nearest 2 km, we may have some seriously big values in cities and seriously small values in the countryside. For these data we may use a log transformation, or a square root.\n\n\n\n\n\n\n\n\n\n\nOverly Large Data\n\nIf our fixed effect was a really big value but actually we are interested in small changes we might consider it differently. For example we might look at Year, in our data that would be above 2000 generally but actually if we have 5 years of data we are looking at it would be easier for the model to convert our data to year 1, year 2, year 2 rather than 2020, 2021, 2022. Here we maintain the difference of 1 year between each value so change the scale.\n\n\n\n\n\n\n\n\n\n\nAwkwardly Placed but ‘Nicely’ Distributed Data\n\nSometimes we may have a distribution of values that is ‘nicely’ distributed, with a clear mean, most values centered on that mean with less values further away from that mean. But if that mean is quite large, or really really small, or really really negative we may have issues with models not working correctly. This can also occur when we have more than one fixed effect and they are on very different scales. For example, body measurements as fixed effects where one body measurement is on the scale of metres and another is millimetres. For these we can centered and scale, sometimes call normalise, our effect by subtracting the mean from every value and dividing by the standard deviation.\n\n\n\nCategorical factors can be less simple. R takes the first level of the categorical effect as the baseline or reference state, then compares all others to that one reference level. If you only have two levels then that is fine, the model will tell you the effect of changing from state 1 to state 2. This is directional. However, with more than two levels the results in r will only tell you the pairwise comparisons from state 1 to state 2, state 1 to state 3, state 1 to state 4 …. state 1 to state n. This is harder to interpret in the results table but if you plot the model it becomes easy. It does mean that we can make sure if there is a control or base level in our factor we can use that as our reference level. This is done by ordering our column so that our desired base level is first in the column. If the column is a character style data r will use the first level alphabetically as our reference data. Sometimes we won’t care though, we just have to be aware that it is happening.\n\n\n\n\n\n\nGLMs and GLMMs have some broad assumptions for them to be appropriately used. Regardless of the distribution type an Independence of sample is assumed as well as homoscedasticity and normality of residuals is assumed.\nIf we have created an appropriate statistical model that takes into consideration the heirarchy/structure of the data our samples should be independent.\nSo what are residuals and what is homoscedasticity?\n\n\n\nTo assess the assumptions of our models we can look at the residual distance between the model line and the points, these are called the residuals of the model.\nIn its most basic version the residuals are the distance between the model and the raw data, then the plot of those residuals against the fitted values (or our response variable) can show how the sizes of residuals change across the fitted values. Homoscedasticity means homogeneity of variance or that the variance is consistent across the whole range of data. Therefore, ideally we want the residuals to represent a cloud of points with no clear patterns across the range of the response variable (fitted values), this is very subjective but a useful qualitative assessment of how good our model is. If our model was missing a clear pattern in the raw data then we would see a clear pattern in the residuals.\nIf we see patterns in our Response Variable vs Residuals plots (not just a random cloud of points) this may mean we missed an important fixed effect in our model (Underlying structure of the system has been ignored), the data do not come from the assigned distribution, there are a high number of zeros in our data or the data do come from the correct distribution but we have been biased in our sampling method (There could be many other reasons but these are the main ones).\n\n\n\n\n\n\n\n\n\n\n\n\nFor the vast majority of GLM and GLMMs, there is an assumption that our residuals are normally distributed. Basically this means that our residuals have a mean distance from the model and most of the values are around this mean. Then the likelihood or probability of having much larger or much smaller values than the mean becomes smaller and smaller. We can therefore plot our residuals as a histogram to see if it follows a generally normal or bell shaped distribution. We can also do what is called a Quantile-Quantile plot, which creates a theoretical distribution of values given the data range, then plots it against the empirical or true distribution. If the residual distribution are perfectly the same as the theoretical distribution it follows a 1 to 1 line (y=x). Again these are more qualitative checks where some variation and divergence from normality is fine.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce we are happy with our model, we can assess what the model tells us about our data. Our interpretation of the model will depend on the statistical model we used and the assumptions we made about our raw data. The best way to interpret a model, once we are happy with it, is to plot the model. To do this we can simulate data that covers the range of our fixed effects, then use the model to predict our response variable for all the values of these fixed effects we simulated. As a final check of how well our model has Generalised the patterns of the raw data we can also plot the raw data alongside our modelled data.\n\n\n\nExample of Model Prediction from Davies et al., 2022. Here points and error bars show raw data and lines with shading show model estimates and 95% confidence intervals.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#short-and-probably-biased-history",
    "href": "StatisticsTutorials/IntroductionGLMs.html#short-and-probably-biased-history",
    "title": "Introduction GLMs",
    "section": "",
    "text": "These modelling frameworks can be used to model almost all situations that will arise in our work. We are almost always using models to describe relationships. We can then use those models to predict future events but this is less common and more nuanced so we might use other tools for that.\nIn more traditional “Fischer” style statistics there are a wealth of different tests for each different scenario such as t-test, ANOVA, Kruskal-Wallis, Friedman, Mann-Whitney-U etc. These tests were created, designed and developed from work by a researcher called Fischer, with many ecologists/researchers generally holding onto this style as it has been used and taught for decades. However, they were designed and created for very specific cases. This means that they are extremely powerful and useful in those situations but are almost always used in slightly different situations or scenarios and so lose their power.\n\n\n\nOld Stats Test - Image from https://onishlab.colostate.edu/\n\n\nGLMs and GLMMs can be used in all the same cases as these older style tests as well as many many more. GLMs and by extension GLMMs have the advantage of being more consistency with terminology, have higher flexibility and you do not need to know 1000 different tests to use them. There are important options to chose within GLMs and GLMMs but as I mention they are more consistent and easier to check or adhere to their assumptions.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#side-note---bayesfrequentistwho-cares",
    "href": "StatisticsTutorials/IntroductionGLMs.html#side-note---bayesfrequentistwho-cares",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Discussion, examples and code here about GLMs and GLMMs will be carried out and more aimed at Frequentist methods of analysis. However, as Richard McElreath points out amazingly in his Statistical Rethinking Course (GO CHECK IT OUT IT IS AMAZING AND FREE!! https://www.youtube.com/@rmcelreath or https://github.com/rmcelreath), the general principles are the same between Frequentist and Bayesian methods, the important part is interpretation and building scientific and statistical models to allow Causal Inference! (i.e. does the model actually provide us with evidence of a cause and if so what is that cause.)\n\n\n\nBayes vs Frequentists Killed by Doge - Flagrantly stolen from https://github.com/rmcelreath",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#what-are-glms",
    "href": "StatisticsTutorials/IntroductionGLMs.html#what-are-glms",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Effectively General Linear Models are ways of creating generalising lines or linear equations that best describe the data we put into them given some equation of interest.\nWe can summarise most with three equations. We have the linear equation \\(y = \\beta x + c\\) and how that relates to the data we sample (\\(s\\)) \\(s = Distribution(y,\\sigma)\\) .\nThis will often be written generally as:\n\\[s = Distribution(y',\\sigma)\\]\n\\[y'=f(y)\\]\n\\[y = \\beta x + c\\]\nThe first of these equations is saying that the data we recorded (\\(s\\)) comes from a some distribution (We will define this later) with a mean of \\(y'\\) and a standard deviation of \\(\\sigma\\). \\(y\\) is the result of an effect (\\(\\beta\\)) multiplied by some fixed effect (\\(x\\)) and an intercept value (\\(c\\)). Between these two equations we have a link function (\\(f()\\)). This converts \\(y\\) into a value that the distribution function will accept \\(y'\\), again we will define what these elements are later. The elements that we want our GLM to tell us is the effect of x (\\(\\beta\\)) and the intercept (\\(c\\)). We don’t have to do maths at all for GLMs but it can help some people understand what the model is telling us. (If you don’t like equations/maths ignore this bit - But we will discuss link functions later on).",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#first-step-of-analysis---statistical-model-formulation",
    "href": "StatisticsTutorials/IntroductionGLMs.html#first-step-of-analysis---statistical-model-formulation",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Generally we will have a response variable (Dependent Variable) with one or more fixed effects (independent variables).\nHere I will discuss exclusively univariate modelling (where only one variable is the response variable) but multivariate analysis could follow the same framework, it is just more complicated to implement.\nOur research question will dictate how we create our scientific model, which dictates what statistical model to use and then how we interpret that statistical model.\nMany researchers like to draw out what is called a DAG, which diagrammatically shows assumed influences of factors, and how the directionallity of that influence. At first these are quite difficult to formulate until you have created a few. Therefore, the easiest way I can think to do this is saying some sort of hypothesis or statement in English then we can turn that statement into the Statistical model.\nFor example: I hypothesise that the flipper length of a penguin is described primarily by the species of that penguin but also influenced by its sex. Effectively Flipper length changes with Species and Sex.\n\n\n\nPenguins Image by Allison Horst\n\n\nHere our response variable is Flipper length (Dependant variable) and our fixed effects (Independent variables) are Species and Sex.\nIn statistical notation we could write this in a couple of ways depending on our assumptions:\n1). flipper_length_mm ~ sex * species\n2). flipper_length_mm ~ sex + species\nFirstly, we see on the left hand side of the ~ (tilde) we have our response variable, with our fixed effects on the other side.\nSecondly, we see that we either multiply or add these fixed effects. The difference between the two is subtle but important.\nEquation 1 is saying that flipper length is effected by species and by sexes but that the effect of sex is different depending on the species. (females always bigger in one species but males always bigger in another but one species is always much larger than the other).\nEquation 2 is saying that flipper length is effected by species and by sexes but that the effect of sex is the same or similar regardless of the species. (males always bigger across all species, but sizes different across species).\nWe may think either equation is correct given our knowledge of penguins.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#second-step-of-analysis---response-variable-distribution",
    "href": "StatisticsTutorials/IntroductionGLMs.html#second-step-of-analysis---response-variable-distribution",
    "title": "Introduction GLMs",
    "section": "",
    "text": "The next most important element is what type of data our response variable is. This is the step that is often carried out wrong and leads to many inaccuracies of analysis.\nWhen modelling we are using our sample to make inference about a population. Therefore, our model should be based on our assumptions of the population that we took the sample from.\nThe type of data your response variable comes from may not be exactly what your response variable data looks like! However, if we have sufficiently sampled the population and not introduced any selection or sampling bias we should be confident in saying our sample (response variable) comes from the distribution of the population. We should be aware of this when analysing data from specific geographies or demographics etc!!!\nSo we should be able to tell the type of response variable data before we even collect the samples themselves! WE SHOULD NOT LOOK AT A GRAPH OF OUR DATA AND CHOOSE THE DISTRIBUTION IT COMES FROM JUST BY THE SHAPE!!!! (Speaking from experience and errors made!)\n\n\nThere are a group of widely used distributions that will cover almost all situations.\nThey are:\n\nGaussian\nBeta\nGamma\nPoisson\nBinomial/Bernoulli\n\nThere are many, many, many others such as Student-t or Direchlet or Tweedie. All of which are very useful and utilised throughout science but we won’t go through them here.\n\n\n\nDistributions - https://learnbyinsight.com/2020/09/27/probability-distribution-an-aid-to-know-the-data/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo when deciding what type of model to run we can first think whether our data are numerica or categoric. We will only discuss numerical data (temperature or weight or numbers of fish) but sometimes we might have a categorical response variable, such as questionnaire answers. Within these types of model there might be order to the categories (Not likely, Somewhat likely, Very likely) or there is no logical order (green, blue, purple). There are mixed effects models for these that are widely used, such as Multinomial Logisitic Regression for non-ordered and Ordinal Logistic Regression for ordered categorical response variables, but again we won’t discuss them here.\n\n\n\nOnce we know our data are numeric, we need to choose what type: continuous or integers. This is quite an easy one, can you measure it in decimals or is it complete units. When studying nesting birds we don’t count half eggs; a nest will have 1, 2, 3 etc. eggs. But if we are measuring the width of a tree it might be 4 cm or 2.365441 m.\n\n\n\n\n\n\n\n\n\nNest - https://www.thespruce.com\n\n\n\n\n\n\n\nTree - https://fennerschool-associated.anu.edu.au\n\n\n\n\n\n\n\n\nOnce we know our data are continuous numeric, we can think can the values be negative? A response variable that is continuous and can be negative is mostly modelled under the most common distribution: Gaussian. Examples of this could be temperature measured in \\(^\\circ\\)C or acceleration. A response variable that cannot be negative or even 0, would most correctly be: Gamma. However, Gaussian models are very often used for these models as a Guassian distribution is simpler computationally and mathematically, and only really causes issues when the values approach 0. This is actually most continuous values we measure, such as thickness of a stem, fish length, body mass or area of habitat.\n\n\n\n\n\n\n\n\n\n\n\n\nIf your response variable is continuous, and it may be negative or not, but is bound between an upper and lower known bound then it would most correctly be: Beta. For most practical terms Beta distribution is between 0 and 1. However, most data between bounds could be scaled to 0 and 1 and keep all of their characteristics. For example, percentage or proportional data are effectively between 0 and 1 and can be modelling using a Beta distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nWe know our data are integers but do we know whether there is an upper limit? If our data could potentially be very big as counts then we will use the Poisson distribution. This is for response variables like number of eggs in a nest or abundance of fish or the number of different species found along a transect. If we know there is an upper limit to the integers we could get then we will use a Binomial model. This could be for the number of eggs inside a six egg carton. The most common version of this is when we have a binary response variable (either 1 or 0). Presence or absence of a rare species for example. This is binary Binomial but is sometimes referred to specifically as Bernoulli.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#third-step-of-analysis---organising-fixed-effects",
    "href": "StatisticsTutorials/IntroductionGLMs.html#third-step-of-analysis---organising-fixed-effects",
    "title": "Introduction GLMs",
    "section": "",
    "text": "We now know what distribution we will most likely use to analyse our response data, so the next step is organising our fixed effects so the model treats them how we would like them to be treated. Effectively fixed effects are either numeric or categorical variables. With GLMs there is no different name (nor changes in code) generally used for what combination of numeric or categorical variables are used. So we could have multiple numeric effects, multiple categorical factors or a mixture of both.\n\n\nIn r numeric effects are quite straight forward, “what is the effect on y with a one unit change of value of x?”. However, if our numeric column has really big values, big range and a skewed distribution we may have issues with our model not working or running as we expect. For fixed effects this is easy to overcome, generally we would use some sort of transformation that maintains the structure of the data but minimises the issues the model runs into.\n\nSkewed Data\n\nFor example for skewed data, if our fixed effect was based on population size with the nearest 2 km, we may have some seriously big values in cities and seriously small values in the countryside. For these data we may use a log transformation, or a square root.\n\n\n\n\n\n\n\n\n\n\nOverly Large Data\n\nIf our fixed effect was a really big value but actually we are interested in small changes we might consider it differently. For example we might look at Year, in our data that would be above 2000 generally but actually if we have 5 years of data we are looking at it would be easier for the model to convert our data to year 1, year 2, year 2 rather than 2020, 2021, 2022. Here we maintain the difference of 1 year between each value so change the scale.\n\n\n\n\n\n\n\n\n\n\nAwkwardly Placed but ‘Nicely’ Distributed Data\n\nSometimes we may have a distribution of values that is ‘nicely’ distributed, with a clear mean, most values centered on that mean with less values further away from that mean. But if that mean is quite large, or really really small, or really really negative we may have issues with models not working correctly. This can also occur when we have more than one fixed effect and they are on very different scales. For example, body measurements as fixed effects where one body measurement is on the scale of metres and another is millimetres. For these we can centered and scale, sometimes call normalise, our effect by subtracting the mean from every value and dividing by the standard deviation.\n\n\n\nCategorical factors can be less simple. R takes the first level of the categorical effect as the baseline or reference state, then compares all others to that one reference level. If you only have two levels then that is fine, the model will tell you the effect of changing from state 1 to state 2. This is directional. However, with more than two levels the results in r will only tell you the pairwise comparisons from state 1 to state 2, state 1 to state 3, state 1 to state 4 …. state 1 to state n. This is harder to interpret in the results table but if you plot the model it becomes easy. It does mean that we can make sure if there is a control or base level in our factor we can use that as our reference level. This is done by ordering our column so that our desired base level is first in the column. If the column is a character style data r will use the first level alphabetically as our reference data. Sometimes we won’t care though, we just have to be aware that it is happening.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#fourth-step-of-analysis---assessing-model-functioning",
    "href": "StatisticsTutorials/IntroductionGLMs.html#fourth-step-of-analysis---assessing-model-functioning",
    "title": "Introduction GLMs",
    "section": "",
    "text": "GLMs and GLMMs have some broad assumptions for them to be appropriately used. Regardless of the distribution type an Independence of sample is assumed as well as homoscedasticity and normality of residuals is assumed.\nIf we have created an appropriate statistical model that takes into consideration the heirarchy/structure of the data our samples should be independent.\nSo what are residuals and what is homoscedasticity?\n\n\n\nTo assess the assumptions of our models we can look at the residual distance between the model line and the points, these are called the residuals of the model.\nIn its most basic version the residuals are the distance between the model and the raw data, then the plot of those residuals against the fitted values (or our response variable) can show how the sizes of residuals change across the fitted values. Homoscedasticity means homogeneity of variance or that the variance is consistent across the whole range of data. Therefore, ideally we want the residuals to represent a cloud of points with no clear patterns across the range of the response variable (fitted values), this is very subjective but a useful qualitative assessment of how good our model is. If our model was missing a clear pattern in the raw data then we would see a clear pattern in the residuals.\nIf we see patterns in our Response Variable vs Residuals plots (not just a random cloud of points) this may mean we missed an important fixed effect in our model (Underlying structure of the system has been ignored), the data do not come from the assigned distribution, there are a high number of zeros in our data or the data do come from the correct distribution but we have been biased in our sampling method (There could be many other reasons but these are the main ones).\n\n\n\n\n\n\n\n\n\n\n\n\nFor the vast majority of GLM and GLMMs, there is an assumption that our residuals are normally distributed. Basically this means that our residuals have a mean distance from the model and most of the values are around this mean. Then the likelihood or probability of having much larger or much smaller values than the mean becomes smaller and smaller. We can therefore plot our residuals as a histogram to see if it follows a generally normal or bell shaped distribution. We can also do what is called a Quantile-Quantile plot, which creates a theoretical distribution of values given the data range, then plots it against the empirical or true distribution. If the residual distribution are perfectly the same as the theoretical distribution it follows a 1 to 1 line (y=x). Again these are more qualitative checks where some variation and divergence from normality is fine.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#fifth-step-of-analysis---model-interpretation",
    "href": "StatisticsTutorials/IntroductionGLMs.html#fifth-step-of-analysis---model-interpretation",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Once we are happy with our model, we can assess what the model tells us about our data. Our interpretation of the model will depend on the statistical model we used and the assumptions we made about our raw data. The best way to interpret a model, once we are happy with it, is to plot the model. To do this we can simulate data that covers the range of our fixed effects, then use the model to predict our response variable for all the values of these fixed effects we simulated. As a final check of how well our model has Generalised the patterns of the raw data we can also plot the raw data alongside our modelled data.\n\n\n\nExample of Model Prediction from Davies et al., 2022. Here points and error bars show raw data and lines with shading show model estimates and 95% confidence intervals.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BinomialGLMs.html",
    "href": "StatisticsTutorials/BinomialGLMs.html",
    "title": "Binomial GLMs",
    "section": "",
    "text": "Okay let’s grab the data from the yarrr dataset for diamonds to assess what drives the value of diamonds based on their weight, clarity and colour.\n\nlibrary(performance)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n#install.packages(\"yarrr\")\nlibrary(yarrr)\n\nLoading required package: jpeg\nLoading required package: BayesFactor\nLoading required package: coda\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\n************\nWelcome to BayesFactor 0.9.12-4.5. If you have questions, please contact Richard Morey (richarddmorey@gmail.com).\n\nType BFManual() to open the manual.\n************\nLoading required package: circlize\n========================================\ncirclize version 0.4.15\nCRAN page: https://cran.r-project.org/package=circlize\nGithub page: https://github.com/jokergoo/circlize\nDocumentation: https://jokergoo.github.io/circlize_book/book/\n\nIf you use it in published research, please cite:\nGu, Z. circlize implements and enhances circular visualization\n  in R. Bioinformatics 2014.\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(circlize))\n========================================\n\nyarrr v0.1.5. Citation info at citation('yarrr'). Package guide at yarrr.guide()\nEmail me at Nathaniel.D.Phillips.is@gmail.com\n\nAttaching package: 'yarrr'\n\nThe following object is masked from 'package:ggplot2':\n\n    diamonds\n\ndata(diamonds)\nsummary(diamonds)\n\n     weight          clarity           color          value      \n Min.   : 5.360   Min.   :0.4400   Min.   :2.00   Min.   :174.0  \n 1st Qu.: 8.598   1st Qu.:0.8900   1st Qu.:4.00   1st Qu.:184.0  \n Median : 9.805   Median :1.0000   Median :5.00   Median :189.6  \n Mean   : 9.901   Mean   :0.9996   Mean   :4.96   Mean   :189.4  \n 3rd Qu.:11.155   3rd Qu.:1.1200   3rd Qu.:6.00   3rd Qu.:194.9  \n Max.   :14.270   Max.   :1.4400   Max.   :8.00   Max.   :206.4  \n\n\n\n\n\nSo we could model the actual diamond value but for this example we will split the value into high and low values (higher than 190 or lower than 190).\n\ndf_bin&lt;-diamonds %&gt;% \n  mutate(binary=if_else(value&gt;190,1,0))\n\nggplot(df_bin,aes(x=as.factor(binary)))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Binary Value Above 190 or not\",y=\"Count\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe imagine that the colour, weight and clarity will all influence the value of a diamond, thus we shall additively include each of these variables in the model.\nBinary Value (above 190) ~ Clarity + Weight + Colour\n\n\n\nWe created the response variable to be either 1 or 0. We therefore know that it is a binomial distribution but more specifically it is a Bernoulli distribution. For modelling in r the binomial distribution family is what is used.\n\n\n\nOkay so lets look at our fixed effects, we will use density for continuous values (clarity and weight), while bars can show the counts for the integer Colour. Here colour is an ordinal integer, therefore we will model it as such. If we had Colour as a category (such as red, green, blue etc) we would want to convert it to be a factor for modelling as it would be more nominal.\n\np1&lt;-ggplot(df_bin,aes(x=clarity))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Clarity\",y=\"Density\")+\n  theme_classic()\n\np2&lt;-ggplot(df_bin,aes(x=color))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Colour\",y=\"Count\")+\n  theme_classic()\n\np3&lt;-ggplot(df_bin,aes(x=weight))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Weight\",y=\"Denisty\")+\n  theme_classic()\n\n\np1+p2+p3\n\n\n\n\n\n\n\n\nLet’s fit the glm using these our statistical model from above.\n\nglm1&lt;-glm(binary~clarity+color+weight,data=df_bin, family= \"binomial\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_histogram(aes(x=Residuals))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Count\")\n\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nOkay, here we have a different looking plot than before as the response variable is not continuous or integers. Therefore, homogeneity of residuals is less important, but normality of residuals is. Here we see that the qqnorm plot looks fairly good but high values are moving away from normality. Generally the model seems to fit well so we will interpret it.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = binary ~ clarity + color + weight, family = \"binomial\", \n    data = df_bin)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -18.8009     3.4634  -5.428 5.69e-08 ***\nclarity       9.2910     1.9629   4.733 2.21e-06 ***\ncolor        -0.3836     0.2481  -1.547    0.122    \nweight        1.1251     0.1968   5.716 1.09e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 207.52  on 149  degrees of freedom\nResidual deviance: 106.67  on 146  degrees of freedom\nAIC: 114.67\n\nNumber of Fisher Scoring iterations: 6\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[ Value \\:Above \\:190 \\:(or \\:Not) = Binomial(N, Pr)\\]\n\\[Pr=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Weight + \\beta_{2} Colour + \\beta_{3} Clarity+ Intercept\n\\end{aligned}\n\\]\nHere our link function is slightly different again. This time it is a logit. This is effectively a way of converting a value to be a probability.\nWe can check this is the link function r is using here.\n\nglm1$family$link\n\n[1] \"logit\"\n\n\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just Clarity, Color and Weight the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Probability that a Diamond has a value over 190 based on its weight, color and clarity.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval, we make sure these Upper and Lower confidence intervals don’t stray outside of 0 and 1).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\nTo make plotting easier I will set a Low, middle and high value for weight and color as clarity has the highest influence on diamond value.\n\nNewData_1&lt;-expand.grid(weight=c(min(df_bin$weight),mean(df_bin$weight),max(df_bin$weight)),\n                       color=c(2,5,8),\n                       clarity=seq(min(df_bin$clarity),max(df_bin$clarity),length.out=50)\n                       ) %&gt;% \n  mutate(Weight_Group=factor(case_when(weight==min(df_bin$weight)~\"Low\",\n                                weight==mean(df_bin$weight)~\"Mid\",\n                                weight==max(df_bin$weight)~\"High\"\n                                ),levels=c(\"Low\",\"Mid\",\"High\")),\n         Color_Group=factor(case_when(color==2~\"Low Colour\",\n                                color==5~\"Mid Colour\",\n                                color==8~\"High Colour\"\n                                ),levels=c(\"Low Colour\",\"Mid Colour\",\"High Colour\")))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Upr=case_when(Upr&gt;1~1,\n                       TRUE~Upr),\n         Lwr=case_when(Lwr&lt;0~0,\n                       TRUE~Lwr))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=clarity,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Weight_Group),\n              alpha=0.6)+\n  geom_line(aes(x=clarity,\n                 y=response,\n                  colour=Weight_Group))+\n  facet_wrap(~Color_Group,ncol = 1)+\n   scale_colour_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"))+\n  labs(x=\"Clarity\",y=\"Response Variable (Probability of Diamond value over 190)\",\n       fill=\"Weight Group\",colour=\"Weight Group\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nFrom this graph we can see that Higher Weight class leads to a higher probability of a diamond being over 190 in value, likewise higher clarity leads to higher probability of high value. Colour seems to have less effect, although this is hard to see from this graph.\nNow lets plot this model output over the raw values to see how well the model has worked. We will create new columns to show the Colour and Weight Group from the raw data. I will set arbitary cut offs, which could be interquartiles or thirds of the data. Or if there were specific values of interest these could be plotted on their own.\n\nRaw_df_bin&lt;-df_bin %&gt;% \n  mutate(Weight_Group=factor(case_when(weight&lt;=7.5~\"Low\",\n                                weight&gt;7.5 & weight&lt;12.5~\"Mid\",\n                                weight&gt;=12.5~\"High\"\n                                ),levels=c(\"Low\",\"Mid\",\"High\")),\n         Color_Group=factor(case_when(color&lt;=3~\"Low Colour\",\n                                color&gt;3 & color&lt;7~\"Mid Colour\",\n                                color&gt;=7~\"High Colour\"\n                                ),levels=c(\"Low Colour\",\"Mid Colour\",\"High Colour\")))\n\n\nggplot(NewData)+\n  geom_ribbon(aes(x=clarity,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Weight_Group),\n              alpha=0.6)+\n  geom_line(aes(x=clarity,\n                 y=response,\n                  colour=Weight_Group))+\n  geom_point(data=Raw_df_bin,aes(x=clarity,y=binary,colour=Weight_Group))+\n  facet_grid(Weight_Group~Color_Group)+\n   scale_colour_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"))+\n  labs(x=\"Clarity\",y=\"Response Variable (Probability of Diamond value over 190)\",\n       fill=\"Weight Group\",colour=\"Weight Group\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good, with more uncertainty where there are less values to influence the prediction. This is a very simplified model that is not taking into consideration many different factors, such as origin of diamond, the current diamond market etc. It is also a relatively small data set.\n\n\n\n\nLets create a more complex Binomial model. This data set is the frequency of affairs within the last year with their gender, age, yearsmarried, children, religiousness, education, occupation and self rating of marriage.\n\n#install.packages(\"AER\")\nlibrary(\"AER\")\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\ndata(Affairs, package=\"AER\")\n\nsummary(Affairs)\n\n    affairs          gender         age         yearsmarried    children \n Min.   : 0.000   female:315   Min.   :17.50   Min.   : 0.125   no :171  \n 1st Qu.: 0.000   male  :286   1st Qu.:27.00   1st Qu.: 4.000   yes:430  \n Median : 0.000                Median :32.00   Median : 7.000            \n Mean   : 1.456                Mean   :32.49   Mean   : 8.178            \n 3rd Qu.: 0.000                3rd Qu.:37.00   3rd Qu.:15.000            \n Max.   :12.000                Max.   :57.00   Max.   :15.000            \n religiousness     education       occupation        rating     \n Min.   :1.000   Min.   : 9.00   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:14.00   1st Qu.:3.000   1st Qu.:3.000  \n Median :3.000   Median :16.00   Median :5.000   Median :4.000  \n Mean   :3.116   Mean   :16.17   Mean   :4.195   Mean   :3.932  \n 3rd Qu.:4.000   3rd Qu.:18.00   3rd Qu.:6.000   3rd Qu.:5.000  \n Max.   :5.000   Max.   :20.00   Max.   :7.000   Max.   :5.000  \n\nAffairs_binom&lt;-Affairs %&gt;% \n  mutate(affairs_bin=if_else(affairs&gt;0,1,0))\n\nOur response variable will be a summary of the affairs column for occurrence or not of an affair in the last year where affairs was how often someone engaged in extramarital sexual intercourse during the past year? 0 = none, 1 = once, 2 = twice, 3 = 3 times, 7 = 4–10 times, 12 = monthly, 12 = weekly, 12 = daily.\n\n\nWe will assess the frequency of affairs and whether someones gender, religiousness affects this value, we will also include yearsmarried and whether the pattern of yearsmarried changes depending having children or not.\nThis is a bit more complex model with some interacting fixed effects and additional additive fixed effects and can be written as:\nFrequency of Affairs ~ Children*Yearsmarried + Gender + Religiousness\n\n\n\nAs before there are only integer values for affairs or none in the last year. This type of data could be presented in a different way if we wanted but we will use this scale for now.\n\n\n\nLets check all our fixed effects. For numeric values we can assess their distribution, categorical we can see the number of samples and if it is relatively even. From the way our data has been stored, they are all ordered categories but r has read some as numeric values. This is why we code them as factors first. Gender and children are already factors. We could use the yearsmarried as a categry but we will use a numeric here. I will also do a bit of house keeping such as capitalising the factors (personal preference!).\n\nAffairs_binom&lt;-Affairs_binom %&gt;% \n  mutate(Yearsmarried_fct=factor(case_when(yearsmarried==0.125~\"&lt;1 Year\",\n                                          yearsmarried==0.417~\"&lt;1 Year\",\n                                          yearsmarried==0.75~\"&lt;1 Year\",\n                                          yearsmarried==1.5~\"1-2 Years\",\n                                          yearsmarried==4~\"3-5 Years\",\n                                          yearsmarried==7~\"6-8 Years\",\n                                          yearsmarried==10~\"9-11 Years\",\n                                          yearsmarried==15~\"&gt; 12 Years\"\n                                          ),\n                                 levels=c(\"&lt;1 Year\",\n                                          \"1-2 Years\",\"3-5 Years\",\"6-8 Years\",\n                                          \"9-11 Years\",\"&gt; 12 Years\")),\n         Religiousness_fct=factor(case_when(religiousness==1~\"Anti\",\n                                          religiousness==2~\"Not\",\n                                          religiousness==3~\"Slightly\",\n                                          religiousness==4~\"Somewhat\",\n                                          religiousness==5~\"Very\"\n                                          ),\n                                 levels=c(\"Anti\",\"Not\",\"Slightly\",\"Somewhat\",\"Very\")),\n         Gender=case_when(gender==\"male\"~\"Male\",\n                          gender==\"female\"~\"Female\"),\n         Children=case_when(children==\"yes\"~\"Yes\",\n                            children==\"no\"~\"No\")) %&gt;% \n  dplyr::rename(Yearsmarried=yearsmarried)\n  \n\np1&lt;-ggplot(Affairs_binom,aes(x=Children))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(Affairs_binom,aes(x=Yearsmarried))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np3&lt;-ggplot(Affairs_binom,aes(x=Gender))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n  \np4&lt;-ggplot(Affairs_binom,aes(x=Religiousness_fct))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\nThese look okay, not always totally even but we shall see how the model performs.\n\nglm2&lt;-glm(affairs_bin~Children*Yearsmarried+Gender+Religiousness_fct,data=Affairs_binom, family= \"binomial\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_histogram(aes(x=Residuals))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Count\")\n\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAs earlier, we don’t see the plot for homogeneity of residuals but we do see binned residuals, which show two clear peaks of residuals. There seems to be two clear elements/patterns within our data. Generally, we shouldn’t accept this model diagnostic. One trick to find out where these issues come from can be to plot the residuals against the fixed factors in the model.\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2),\n                  Children= Affairs_binom$Children,\n                  Yearsmarried= Affairs_binom$Yearsmarried,\n                  Gender= Affairs_binom$Gender,\n                  Religiousness_fct= Affairs_binom$Religiousness_fct)\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Children))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Children\")\n\np4&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Yearsmarried))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Years Married\")\n\np5&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Gender))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Gender\")\n\np6&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Religiousness_fct))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Religiousness\")\n\n(p3+p4)/(p5+p6)\n\n\n\n\n\n\n\n\nThere seems to be no clear patterns we can see, so we must have some other issue somewhere, we will continue with caution now but that would not be advisable normally.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = affairs_bin ~ Children * Yearsmarried + Gender + \n    Religiousness_fct, family = \"binomial\", data = Affairs_binom)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -1.61951    0.42272  -3.831 0.000128 ***\nChildrenYes                1.10918    0.40614   2.731 0.006314 ** \nYearsmarried               0.16780    0.05222   3.213 0.001312 ** \nGenderMale                 0.19597    0.19837   0.988 0.323212    \nReligiousness_fctNot      -0.73164    0.35634  -2.053 0.040052 *  \nReligiousness_fctSlightly -0.42674    0.36140  -1.181 0.237685    \nReligiousness_fctSomewhat -1.45482    0.36579  -3.977 6.97e-05 ***\nReligiousness_fctVery     -1.37705    0.44453  -3.098 0.001950 ** \nChildrenYes:Yearsmarried  -0.12986    0.05682  -2.286 0.022282 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 675.38  on 600  degrees of freedom\nResidual deviance: 628.38  on 592  degrees of freedom\nAIC: 646.38\n\nNumber of Fisher Scoring iterations: 4\n\n\nHere we have, for the first time sing our penguins example, multiple levels in a categorical fixed effect. This highlights a behaviour that \\(\\beta\\) values or Estimate values from GLMs show pairwise effect. So each row in this summary where it says yearsmarried_ft then one of the factor levels, it is comparing that factor level to the base level (&lt;1 Year). Likewise, for religiousness_fct, where Anti is our base value and all factor levels are compared to this base level.\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this (this is a long one as there are so many factors):\n\\[ Affair \\:Occurence \\:in \\:Last  \\:Year = Binomial(N, Pr)\\]\n\\[Pr=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Children : Years \\:Married \\\\\n+ \\beta_{2} Anti \\:vs \\:Not \\:Regligious \\\\\n+ \\beta_{3} Anti \\:vs \\:Somewhat \\:Regligious \\\\\n+ \\beta_{4} Anti \\:vs \\:Slightly \\:Regligious \\\\\n+ \\beta_{5} Anti \\:vs \\:Very \\:Regligious \\\\\n+ \\beta_{6} Female \\:vs \\:Male \\:Gender \\\\\n+ \\beta_{7} Years \\:Married \\\\\n+ \\beta_{8} Children \\\\\n+ Intercept\n\\end{aligned}\n\\]\nThis is a lot of \\(\\beta\\) values but thankfully we don’t have to deal with them directly. As before our link function is the logit value.\n\nglm2$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with Children, Years Married, Gender and Religiousness the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the probability of affairs based on those Children, Years Married, Gender and Religiousness factors.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval again we make sure they stay between 0 and 1).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(Children=as.factor(c(\"Yes\",\"No\")),\n                       Yearsmarried=unique(Affairs_binom$Yearsmarried),\n                       Gender=as.factor(c(\"Female\",\"Male\")),\n                       Religiousness_fct=as.factor(c(\"Anti\",\"Not\",\"Slightly\",\"Somewhat\",\"Very\")))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Upr=case_when(Upr&gt;1~1,\n                       TRUE~Upr),\n         Lwr=case_when(Lwr&lt;0~0,\n                       TRUE~Lwr))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Yearsmarried,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Children),\n              alpha=0.6)+\n  geom_line(aes(x=Yearsmarried,\n                 y=response,\n                  colour=Children),\n              alpha=0.6)+\n  facet_grid(Religiousness_fct~Gender)+\n   scale_colour_manual(values=c(\"darkorange\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"darkcyan\"))+\n  labs(x=\"Years Married\",y=\"Probability of an Affair\\nin the Last Year\",\n       fill=\"Children\",\n       colour=\"Children\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe wont plot this model output with the raw values to see how well the model has worked as it will look pretty horrible. This is one of the biggest issues with Binomial models, the raw data are all 0s and 1s so when plotted it is very hard to see any patterns.\nWe can see from these plots that the probability of having had an affair in the last year increases with how many years you have been married especially if you don’t have children. This pattern is less clear or obvious if someone is more religious and there doesn’t appear to be any difference in these trends between Males or Females.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Binomial GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BinomialGLMs.html#data-loading-simple---diamonds",
    "href": "StatisticsTutorials/BinomialGLMs.html#data-loading-simple---diamonds",
    "title": "Binomial GLMs",
    "section": "",
    "text": "Okay let’s grab the data from the yarrr dataset for diamonds to assess what drives the value of diamonds based on their weight, clarity and colour.\n\nlibrary(performance)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n#install.packages(\"yarrr\")\nlibrary(yarrr)\n\nLoading required package: jpeg\nLoading required package: BayesFactor\nLoading required package: coda\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\n************\nWelcome to BayesFactor 0.9.12-4.5. If you have questions, please contact Richard Morey (richarddmorey@gmail.com).\n\nType BFManual() to open the manual.\n************\nLoading required package: circlize\n========================================\ncirclize version 0.4.15\nCRAN page: https://cran.r-project.org/package=circlize\nGithub page: https://github.com/jokergoo/circlize\nDocumentation: https://jokergoo.github.io/circlize_book/book/\n\nIf you use it in published research, please cite:\nGu, Z. circlize implements and enhances circular visualization\n  in R. Bioinformatics 2014.\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(circlize))\n========================================\n\nyarrr v0.1.5. Citation info at citation('yarrr'). Package guide at yarrr.guide()\nEmail me at Nathaniel.D.Phillips.is@gmail.com\n\nAttaching package: 'yarrr'\n\nThe following object is masked from 'package:ggplot2':\n\n    diamonds\n\ndata(diamonds)\nsummary(diamonds)\n\n     weight          clarity           color          value      \n Min.   : 5.360   Min.   :0.4400   Min.   :2.00   Min.   :174.0  \n 1st Qu.: 8.598   1st Qu.:0.8900   1st Qu.:4.00   1st Qu.:184.0  \n Median : 9.805   Median :1.0000   Median :5.00   Median :189.6  \n Mean   : 9.901   Mean   :0.9996   Mean   :4.96   Mean   :189.4  \n 3rd Qu.:11.155   3rd Qu.:1.1200   3rd Qu.:6.00   3rd Qu.:194.9  \n Max.   :14.270   Max.   :1.4400   Max.   :8.00   Max.   :206.4  \n\n\n\n\n\nSo we could model the actual diamond value but for this example we will split the value into high and low values (higher than 190 or lower than 190).\n\ndf_bin&lt;-diamonds %&gt;% \n  mutate(binary=if_else(value&gt;190,1,0))\n\nggplot(df_bin,aes(x=as.factor(binary)))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Binary Value Above 190 or not\",y=\"Count\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe imagine that the colour, weight and clarity will all influence the value of a diamond, thus we shall additively include each of these variables in the model.\nBinary Value (above 190) ~ Clarity + Weight + Colour\n\n\n\nWe created the response variable to be either 1 or 0. We therefore know that it is a binomial distribution but more specifically it is a Bernoulli distribution. For modelling in r the binomial distribution family is what is used.\n\n\n\nOkay so lets look at our fixed effects, we will use density for continuous values (clarity and weight), while bars can show the counts for the integer Colour. Here colour is an ordinal integer, therefore we will model it as such. If we had Colour as a category (such as red, green, blue etc) we would want to convert it to be a factor for modelling as it would be more nominal.\n\np1&lt;-ggplot(df_bin,aes(x=clarity))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Clarity\",y=\"Density\")+\n  theme_classic()\n\np2&lt;-ggplot(df_bin,aes(x=color))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Colour\",y=\"Count\")+\n  theme_classic()\n\np3&lt;-ggplot(df_bin,aes(x=weight))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Weight\",y=\"Denisty\")+\n  theme_classic()\n\n\np1+p2+p3\n\n\n\n\n\n\n\n\nLet’s fit the glm using these our statistical model from above.\n\nglm1&lt;-glm(binary~clarity+color+weight,data=df_bin, family= \"binomial\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_histogram(aes(x=Residuals))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Count\")\n\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nOkay, here we have a different looking plot than before as the response variable is not continuous or integers. Therefore, homogeneity of residuals is less important, but normality of residuals is. Here we see that the qqnorm plot looks fairly good but high values are moving away from normality. Generally the model seems to fit well so we will interpret it.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = binary ~ clarity + color + weight, family = \"binomial\", \n    data = df_bin)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -18.8009     3.4634  -5.428 5.69e-08 ***\nclarity       9.2910     1.9629   4.733 2.21e-06 ***\ncolor        -0.3836     0.2481  -1.547    0.122    \nweight        1.1251     0.1968   5.716 1.09e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 207.52  on 149  degrees of freedom\nResidual deviance: 106.67  on 146  degrees of freedom\nAIC: 114.67\n\nNumber of Fisher Scoring iterations: 6\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[ Value \\:Above \\:190 \\:(or \\:Not) = Binomial(N, Pr)\\]\n\\[Pr=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Weight + \\beta_{2} Colour + \\beta_{3} Clarity+ Intercept\n\\end{aligned}\n\\]\nHere our link function is slightly different again. This time it is a logit. This is effectively a way of converting a value to be a probability.\nWe can check this is the link function r is using here.\n\nglm1$family$link\n\n[1] \"logit\"\n\n\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just Clarity, Color and Weight the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Probability that a Diamond has a value over 190 based on its weight, color and clarity.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval, we make sure these Upper and Lower confidence intervals don’t stray outside of 0 and 1).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\nTo make plotting easier I will set a Low, middle and high value for weight and color as clarity has the highest influence on diamond value.\n\nNewData_1&lt;-expand.grid(weight=c(min(df_bin$weight),mean(df_bin$weight),max(df_bin$weight)),\n                       color=c(2,5,8),\n                       clarity=seq(min(df_bin$clarity),max(df_bin$clarity),length.out=50)\n                       ) %&gt;% \n  mutate(Weight_Group=factor(case_when(weight==min(df_bin$weight)~\"Low\",\n                                weight==mean(df_bin$weight)~\"Mid\",\n                                weight==max(df_bin$weight)~\"High\"\n                                ),levels=c(\"Low\",\"Mid\",\"High\")),\n         Color_Group=factor(case_when(color==2~\"Low Colour\",\n                                color==5~\"Mid Colour\",\n                                color==8~\"High Colour\"\n                                ),levels=c(\"Low Colour\",\"Mid Colour\",\"High Colour\")))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Upr=case_when(Upr&gt;1~1,\n                       TRUE~Upr),\n         Lwr=case_when(Lwr&lt;0~0,\n                       TRUE~Lwr))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=clarity,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Weight_Group),\n              alpha=0.6)+\n  geom_line(aes(x=clarity,\n                 y=response,\n                  colour=Weight_Group))+\n  facet_wrap(~Color_Group,ncol = 1)+\n   scale_colour_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"))+\n  labs(x=\"Clarity\",y=\"Response Variable (Probability of Diamond value over 190)\",\n       fill=\"Weight Group\",colour=\"Weight Group\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nFrom this graph we can see that Higher Weight class leads to a higher probability of a diamond being over 190 in value, likewise higher clarity leads to higher probability of high value. Colour seems to have less effect, although this is hard to see from this graph.\nNow lets plot this model output over the raw values to see how well the model has worked. We will create new columns to show the Colour and Weight Group from the raw data. I will set arbitary cut offs, which could be interquartiles or thirds of the data. Or if there were specific values of interest these could be plotted on their own.\n\nRaw_df_bin&lt;-df_bin %&gt;% \n  mutate(Weight_Group=factor(case_when(weight&lt;=7.5~\"Low\",\n                                weight&gt;7.5 & weight&lt;12.5~\"Mid\",\n                                weight&gt;=12.5~\"High\"\n                                ),levels=c(\"Low\",\"Mid\",\"High\")),\n         Color_Group=factor(case_when(color&lt;=3~\"Low Colour\",\n                                color&gt;3 & color&lt;7~\"Mid Colour\",\n                                color&gt;=7~\"High Colour\"\n                                ),levels=c(\"Low Colour\",\"Mid Colour\",\"High Colour\")))\n\n\nggplot(NewData)+\n  geom_ribbon(aes(x=clarity,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Weight_Group),\n              alpha=0.6)+\n  geom_line(aes(x=clarity,\n                 y=response,\n                  colour=Weight_Group))+\n  geom_point(data=Raw_df_bin,aes(x=clarity,y=binary,colour=Weight_Group))+\n  facet_grid(Weight_Group~Color_Group)+\n   scale_colour_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"))+\n  labs(x=\"Clarity\",y=\"Response Variable (Probability of Diamond value over 190)\",\n       fill=\"Weight Group\",colour=\"Weight Group\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good, with more uncertainty where there are less values to influence the prediction. This is a very simplified model that is not taking into consideration many different factors, such as origin of diamond, the current diamond market etc. It is also a relatively small data set.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Binomial GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BinomialGLMs.html#data-loading-complex---affairs",
    "href": "StatisticsTutorials/BinomialGLMs.html#data-loading-complex---affairs",
    "title": "Binomial GLMs",
    "section": "",
    "text": "Lets create a more complex Binomial model. This data set is the frequency of affairs within the last year with their gender, age, yearsmarried, children, religiousness, education, occupation and self rating of marriage.\n\n#install.packages(\"AER\")\nlibrary(\"AER\")\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\ndata(Affairs, package=\"AER\")\n\nsummary(Affairs)\n\n    affairs          gender         age         yearsmarried    children \n Min.   : 0.000   female:315   Min.   :17.50   Min.   : 0.125   no :171  \n 1st Qu.: 0.000   male  :286   1st Qu.:27.00   1st Qu.: 4.000   yes:430  \n Median : 0.000                Median :32.00   Median : 7.000            \n Mean   : 1.456                Mean   :32.49   Mean   : 8.178            \n 3rd Qu.: 0.000                3rd Qu.:37.00   3rd Qu.:15.000            \n Max.   :12.000                Max.   :57.00   Max.   :15.000            \n religiousness     education       occupation        rating     \n Min.   :1.000   Min.   : 9.00   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:14.00   1st Qu.:3.000   1st Qu.:3.000  \n Median :3.000   Median :16.00   Median :5.000   Median :4.000  \n Mean   :3.116   Mean   :16.17   Mean   :4.195   Mean   :3.932  \n 3rd Qu.:4.000   3rd Qu.:18.00   3rd Qu.:6.000   3rd Qu.:5.000  \n Max.   :5.000   Max.   :20.00   Max.   :7.000   Max.   :5.000  \n\nAffairs_binom&lt;-Affairs %&gt;% \n  mutate(affairs_bin=if_else(affairs&gt;0,1,0))\n\nOur response variable will be a summary of the affairs column for occurrence or not of an affair in the last year where affairs was how often someone engaged in extramarital sexual intercourse during the past year? 0 = none, 1 = once, 2 = twice, 3 = 3 times, 7 = 4–10 times, 12 = monthly, 12 = weekly, 12 = daily.\n\n\nWe will assess the frequency of affairs and whether someones gender, religiousness affects this value, we will also include yearsmarried and whether the pattern of yearsmarried changes depending having children or not.\nThis is a bit more complex model with some interacting fixed effects and additional additive fixed effects and can be written as:\nFrequency of Affairs ~ Children*Yearsmarried + Gender + Religiousness\n\n\n\nAs before there are only integer values for affairs or none in the last year. This type of data could be presented in a different way if we wanted but we will use this scale for now.\n\n\n\nLets check all our fixed effects. For numeric values we can assess their distribution, categorical we can see the number of samples and if it is relatively even. From the way our data has been stored, they are all ordered categories but r has read some as numeric values. This is why we code them as factors first. Gender and children are already factors. We could use the yearsmarried as a categry but we will use a numeric here. I will also do a bit of house keeping such as capitalising the factors (personal preference!).\n\nAffairs_binom&lt;-Affairs_binom %&gt;% \n  mutate(Yearsmarried_fct=factor(case_when(yearsmarried==0.125~\"&lt;1 Year\",\n                                          yearsmarried==0.417~\"&lt;1 Year\",\n                                          yearsmarried==0.75~\"&lt;1 Year\",\n                                          yearsmarried==1.5~\"1-2 Years\",\n                                          yearsmarried==4~\"3-5 Years\",\n                                          yearsmarried==7~\"6-8 Years\",\n                                          yearsmarried==10~\"9-11 Years\",\n                                          yearsmarried==15~\"&gt; 12 Years\"\n                                          ),\n                                 levels=c(\"&lt;1 Year\",\n                                          \"1-2 Years\",\"3-5 Years\",\"6-8 Years\",\n                                          \"9-11 Years\",\"&gt; 12 Years\")),\n         Religiousness_fct=factor(case_when(religiousness==1~\"Anti\",\n                                          religiousness==2~\"Not\",\n                                          religiousness==3~\"Slightly\",\n                                          religiousness==4~\"Somewhat\",\n                                          religiousness==5~\"Very\"\n                                          ),\n                                 levels=c(\"Anti\",\"Not\",\"Slightly\",\"Somewhat\",\"Very\")),\n         Gender=case_when(gender==\"male\"~\"Male\",\n                          gender==\"female\"~\"Female\"),\n         Children=case_when(children==\"yes\"~\"Yes\",\n                            children==\"no\"~\"No\")) %&gt;% \n  dplyr::rename(Yearsmarried=yearsmarried)\n  \n\np1&lt;-ggplot(Affairs_binom,aes(x=Children))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(Affairs_binom,aes(x=Yearsmarried))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np3&lt;-ggplot(Affairs_binom,aes(x=Gender))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n  \np4&lt;-ggplot(Affairs_binom,aes(x=Religiousness_fct))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\nThese look okay, not always totally even but we shall see how the model performs.\n\nglm2&lt;-glm(affairs_bin~Children*Yearsmarried+Gender+Religiousness_fct,data=Affairs_binom, family= \"binomial\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_histogram(aes(x=Residuals))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Count\")\n\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAs earlier, we don’t see the plot for homogeneity of residuals but we do see binned residuals, which show two clear peaks of residuals. There seems to be two clear elements/patterns within our data. Generally, we shouldn’t accept this model diagnostic. One trick to find out where these issues come from can be to plot the residuals against the fixed factors in the model.\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2),\n                  Children= Affairs_binom$Children,\n                  Yearsmarried= Affairs_binom$Yearsmarried,\n                  Gender= Affairs_binom$Gender,\n                  Religiousness_fct= Affairs_binom$Religiousness_fct)\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Children))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Children\")\n\np4&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Yearsmarried))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Years Married\")\n\np5&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Gender))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Gender\")\n\np6&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Religiousness_fct))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Religiousness\")\n\n(p3+p4)/(p5+p6)\n\n\n\n\n\n\n\n\nThere seems to be no clear patterns we can see, so we must have some other issue somewhere, we will continue with caution now but that would not be advisable normally.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = affairs_bin ~ Children * Yearsmarried + Gender + \n    Religiousness_fct, family = \"binomial\", data = Affairs_binom)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -1.61951    0.42272  -3.831 0.000128 ***\nChildrenYes                1.10918    0.40614   2.731 0.006314 ** \nYearsmarried               0.16780    0.05222   3.213 0.001312 ** \nGenderMale                 0.19597    0.19837   0.988 0.323212    \nReligiousness_fctNot      -0.73164    0.35634  -2.053 0.040052 *  \nReligiousness_fctSlightly -0.42674    0.36140  -1.181 0.237685    \nReligiousness_fctSomewhat -1.45482    0.36579  -3.977 6.97e-05 ***\nReligiousness_fctVery     -1.37705    0.44453  -3.098 0.001950 ** \nChildrenYes:Yearsmarried  -0.12986    0.05682  -2.286 0.022282 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 675.38  on 600  degrees of freedom\nResidual deviance: 628.38  on 592  degrees of freedom\nAIC: 646.38\n\nNumber of Fisher Scoring iterations: 4\n\n\nHere we have, for the first time sing our penguins example, multiple levels in a categorical fixed effect. This highlights a behaviour that \\(\\beta\\) values or Estimate values from GLMs show pairwise effect. So each row in this summary where it says yearsmarried_ft then one of the factor levels, it is comparing that factor level to the base level (&lt;1 Year). Likewise, for religiousness_fct, where Anti is our base value and all factor levels are compared to this base level.\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this (this is a long one as there are so many factors):\n\\[ Affair \\:Occurence \\:in \\:Last  \\:Year = Binomial(N, Pr)\\]\n\\[Pr=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Children : Years \\:Married \\\\\n+ \\beta_{2} Anti \\:vs \\:Not \\:Regligious \\\\\n+ \\beta_{3} Anti \\:vs \\:Somewhat \\:Regligious \\\\\n+ \\beta_{4} Anti \\:vs \\:Slightly \\:Regligious \\\\\n+ \\beta_{5} Anti \\:vs \\:Very \\:Regligious \\\\\n+ \\beta_{6} Female \\:vs \\:Male \\:Gender \\\\\n+ \\beta_{7} Years \\:Married \\\\\n+ \\beta_{8} Children \\\\\n+ Intercept\n\\end{aligned}\n\\]\nThis is a lot of \\(\\beta\\) values but thankfully we don’t have to deal with them directly. As before our link function is the logit value.\n\nglm2$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with Children, Years Married, Gender and Religiousness the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the probability of affairs based on those Children, Years Married, Gender and Religiousness factors.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval again we make sure they stay between 0 and 1).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(Children=as.factor(c(\"Yes\",\"No\")),\n                       Yearsmarried=unique(Affairs_binom$Yearsmarried),\n                       Gender=as.factor(c(\"Female\",\"Male\")),\n                       Religiousness_fct=as.factor(c(\"Anti\",\"Not\",\"Slightly\",\"Somewhat\",\"Very\")))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Upr=case_when(Upr&gt;1~1,\n                       TRUE~Upr),\n         Lwr=case_when(Lwr&lt;0~0,\n                       TRUE~Lwr))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Yearsmarried,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Children),\n              alpha=0.6)+\n  geom_line(aes(x=Yearsmarried,\n                 y=response,\n                  colour=Children),\n              alpha=0.6)+\n  facet_grid(Religiousness_fct~Gender)+\n   scale_colour_manual(values=c(\"darkorange\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"darkcyan\"))+\n  labs(x=\"Years Married\",y=\"Probability of an Affair\\nin the Last Year\",\n       fill=\"Children\",\n       colour=\"Children\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe wont plot this model output with the raw values to see how well the model has worked as it will look pretty horrible. This is one of the biggest issues with Binomial models, the raw data are all 0s and 1s so when plotted it is very hard to see any patterns.\nWe can see from these plots that the probability of having had an affair in the last year increases with how many years you have been married especially if you don’t have children. This pattern is less clear or obvious if someone is more religious and there doesn’t appear to be any difference in these trends between Males or Females.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Binomial GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ProblematicGLMs.html#data-loading---lets-go-back-to-penguins",
    "href": "StatisticsTutorials/ProblematicGLMs.html#data-loading---lets-go-back-to-penguins",
    "title": "Common GLM Problems",
    "section": "Data Loading - Lets go Back to Penguins",
    "text": "Data Loading - Lets go Back to Penguins\nWe will use the Penguins data set again, but this time we will not take into consideration the causal implications of Sex or Species for the morphometric bill_depth_mm. This would not make any sense with a dataset where different species and different sexes of these species are present.\n\n#install.packages(\"palmerpenguins\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\nlibrary(performance)\n\ndata(penguins)\n\npenguins_noNAs&lt;-penguins%&gt;% \n  drop_na()\n\n\nStep One - Scienctific Model to Stats Model\n\nHere we will see if the effect of flipper length on the depth of the bill.\nThis is a fairly simple model with one fixed effect and can be written as:\nBill Depth ~ Flipper Length\n\n\nStep Two - Resonse Variable Distribution\nAs before when we modelled flipper length, bill depth is technically Gamma distributed but a linear model will work well.\n\n\nStep Three - Organising Fixed Effects\nOur data are fairly well distributed across the values, although there are two clear peaks. HINT HINT - This might cause issues if we haven’t taken into consideration why there are two peaks (one of which is twice the height of the other).\n\nggplot(penguins_noNAs,aes(x=flipper_length_mm))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Flipper Length (mm)\",y=\"Density\")\n\n\n\n\n\n\n\n\nAs we will pretend these look fine, we shall fit out model with a Gaussian distribution.\n\nlm1&lt;-lm(bill_depth_mm~flipper_length_mm,data=penguins_noNAs)\n\n\n\nStep Four - Assessing Model Functioning\nWe can use the check_model() function to assess the residuals from the performance package.\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm1),\n                  Residuals=resid(lm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAhhh No, this looks pretty bad. Well the qqnorm plot looks good actually, but the residuals vs fitted values has clear patterns that cannot be ignored.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Common GLM Problems"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ProblematicGLMs.html#what-is-wrong-and-how-do-we-fix-it",
    "href": "StatisticsTutorials/ProblematicGLMs.html#what-is-wrong-and-how-do-we-fix-it",
    "title": "Common GLM Problems",
    "section": "What is wrong and How do we fix it?",
    "text": "What is wrong and How do we fix it?\nSo for us to see such strong patterns in our residuals vs fitted plot there are a number of different issues that could be causing this:\n\nWe have used the wrong distribution,\nWe haven’t modelled the structure of the data properly,\nWe have ignored issues in the sampling that created inherent bias in the data,\nOur data have excessive numbers of 0s (either caused by our sampling methods or precision issues).\n\n\n1) Wrong Distribution\nIf we have used the wrong distribution we should remodel with the correct distribution. This can happen especially for poisson models where there is high levels of variance. The poisson distribution expects variance to be proportional to the expected value. When there is significant over dispersion of values we can use a different distribution called the negative binomial distribution.\n\n\n2) Wrong Model Structure\nIf we have modelled the data structure incorrectly, i.e. ignored the difference in the relationship of bill depth and flipper depending on different species, then we need to include that structure in the model. Sometimes this will be some sort of hierarchy, repeat measures or multi-level structure, to model this correctly we will need to use mixed effect or multi-level models (General Linear Mixed Effect Models: GLMMs, we will go through these in future tutorials).\n\n\n3) Inherent Bias in Sampling\nThis is a more complex issue. This could, for example, be that we only sampled male gentoo penguins and female chinstrap penguins, thus our ability to split by sex and species to model the data structure properly will be impossible. In many situations, if we know the bias we could perhaps model within one of the groups, just male gentoo penguins, but this would mean our model was only applicable to this group. Other times we may have a sampling cut off/threshold, which may or may not be acceptable. For example, when measuring penguins there may be ethical issues/sampling protocols of measuring individuals smaller than a certain size, as it could harm their parent-chick relationship, or cause too much distress to the individual. This will effectively mean you miss data below a certain size. Often this can be acceptable as long as the interpretation of the model takes this into consideration.\n\n\n4) Zero or One Inflated Data\nZero (or one) inflation of data can have multiple root causes, such as sampling bias as mentioned above, also subsetting of multivariate datasets e.g. we have catch data for a experimental trawls. From the trawls we get abundance per trawl of all the different species caught, but we are interested in just one species and its abundance. Sometimes this means trawls will have high numbers of zeros for this species. Sometimes zeros will not cause issues but sometimes high numbers of zeros will cause clear patterns in both the residuals vs fitted and qqnorm plots. When this happens we can employ zero inflated or zero adjusted versions of the model we want. This is effectively creating two models in sequence. Firstly, we model the presence or absence with a Bernoulli model, then when there is presence we model all non-zero values with the desired distribution (We will also explore these models in a future tutorial).",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Common GLM Problems"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ProblematicGLMs.html#solution",
    "href": "StatisticsTutorials/ProblematicGLMs.html#solution",
    "title": "Common GLM Problems",
    "section": "Solution",
    "text": "Solution\nWhile we could go down the GLMM route here, we can also model the species into our model to sort the issues we found.\n\nlm2&lt;-lm(bill_depth_mm~flipper_length_mm*species,data=penguins_noNAs)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm2),\n                  Residuals=resid(lm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nThis is a lot better, but again we can see two clear groups of points with different spreads, what other element of our data has two factors that we may have missed?\n\nlm3&lt;-lm(bill_depth_mm~flipper_length_mm*species+sex,data=penguins_noNAs)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm3),\n                  Residuals=resid(lm3))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nThis is better, but not perfect as we still see some patterns, but generally our points are more evenly distributed above and below the 0 line.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Common GLM Problems"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ProblematicGLMs.html#galapagos",
    "href": "StatisticsTutorials/ProblematicGLMs.html#galapagos",
    "title": "Common GLM Problems",
    "section": "Galapagos",
    "text": "Galapagos\nHere we will bring the Galapagos dataset back. When we used this for a poisson model we talked about it having high levels of over dispersion. We ignored it before to give the example, but lets now actually take action.\n\n\nlibrary(faraway)\ndata(gala)\n\n\nglm1&lt;-glm(Species~Elevation+Nearest,data=gala, family= \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nSo how do we the fit a negative binomial model? We need to load another package as the negative binomial distribution in glm() needs to already know the overdispersion, whereas we won’t know it and will want to estimate it within the model! To do this we can use the glm.nb() function from the MASS package. Note of caution: the MASS package has a function called select() this will conflict with the select function from dplyr we use for data manipulation - Be aware!\n\n#install.packages(\"MASS\")\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nglm2&lt;-glm.nb(Species~Elevation+Nearest,data=gala)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\nggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    scale_x_continuous(limits=c(0,100))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\nWarning: Removed 7 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs we can see this is a better residuals check. As mentioned before when we used this data set there are very few data points so we are very unlikely to get a perfect model fit (At least not with a simple glm). We can still see some very large fitted values that might be worth investigating but when we zoom in we can see the other residuals seem pretty good.",
    "crumbs": [
      "Home",
      "Statistics Tutorials: GLMs",
      "Common GLM Problems"
    ]
  },
  {
    "objectID": "index_Mobile.html",
    "href": "index_Mobile.html",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "",
    "text": "Bede Ffinian Rowe Davies Post-Doctoral Researcher bedeffinian@gmail.com Laboratoire ISOMer, UFR Sciences et Techniques Nantes University, France."
  },
  {
    "objectID": "index_Mobile.html#projects",
    "href": "index_Mobile.html#projects",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "Projects",
    "text": "Projects\nI have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\n\n\n\n\n\n\n\n\n\n\n\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation."
  },
  {
    "objectID": "Blogs.html",
    "href": "Blogs.html",
    "title": "Travel Blogs",
    "section": "",
    "text": "A Greenlandic Tale\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts1/Functional.html",
    "href": "posts1/Functional.html",
    "title": "A Decade Implementing Ecosystem Approach to Fisheries Management Improves Diversity of Taxa and Traits within a Marine Protected Area in the UK.",
    "section": "",
    "text": "Davies et al., 2022\nAim: Ecosystem Approach to Fisheries Management has highlighted the importance of studying ecosystem functions and services, and the biological traits that drive them. Yet, ecosystem services and the associated benefits that they provide are rarely the motive for creating marine protected area (MPA). Therefore, many MPA monitoring projects do not explicitly study these functions and services or the underlying biological traits linked to them. Location: Lyme Bay MPA, located in the SW of England, was established in 2008 to protect the reef biodiversity across a 206 km\\(^2\\) area, which includes rocky reef habitats, pebbly sand and soft muddy sediments. Mobile demersal fishing was excluded across the whole site to allow the recovery of the reef habitats. Methods: Using a combination of towed underwater video and Baited Remote Underwater Video Systems changes in diversity (taxonomic and trait), and traits affected by mobile demersal fishing were assessed in Lyme Bay MPA over 10 years. Results: There was a consistent increase in the number of taxa and the trait diversity they provide within the MPA as well as an increase in functional redundancy, which may increase community resilience to perturbations. Outside of the MPA there was an increase in the abundance of mobile species, while the MPA showed an increase in filter feeders. Main conclusions: The MPA showed a trend towards more diverse and potentially resilient rocky reef habitats. This study constitutes a novel MPA assessment using multiple sampling methods to encompass a wide range of taxa. It also reinforces the importance of effective MPA monitoring, which has demonstrated changes in trait diversity and trait composition driven by changes in taxonomic diversity."
  },
  {
    "objectID": "posts1/SpectralLibrary.html",
    "href": "posts1/SpectralLibrary.html",
    "title": "Multi- and hyperspectral classification of soft-bottom intertidal vegetation using a spectral library for coastal biodiversity remote sensing.",
    "section": "",
    "text": "Davies et al., 2023\n\nAbstract\nMonitoring biodiversity and how anthropogenic pressures impact this is critical, especially as anthropogenically driven climate change continues to affect all ecosystems. Intertidal areas are exposed to particularly high levels of pressures owing to increased population density in coastal areas. Traditional methods of monitoring intertidal areas do not provide datasets with full coverage in a cost-effective or timely manner, and so the use of remote sensing to monitor these areas is becoming more common. Monitoring of ecologically important monospecific habitats, such as seagrass beds, using remote sensing techniques is well documented. However, the ability for multispectral data to distinguish efficiently and accurately between classes of vegetation with similar pigment composition, such as seagrass and green algae, has proved difficult, often requiring hyperspectral data. A machine learning approach was used to differentiate between soft-bottom intertidal vegetation classes when exposed at low tide, comparing 6 different multi- and hyperspectral remote and in situ sensors. For the library of 366 spectra, collected across Northern Europe, high accuracy (&gt;80%) was found across all spectral resolutions. While a higher spectral resolution resulted in higher accuracy, there was no discernible increase in accuracy above 10 spectral bands (95%: Sentinel-2 MSI sensor with a spatial resolution of 20 m). This work highlights the ability of multispectral sensors to discriminate intertidal vegetation types, while also showing the most important wavelengths for this discrimination (∼530 and ∼ 730 nm), giving recommendations for spectral ranges of future satellite missions. The ability for multispectral sensors to aid in accurate and rapid intertidal vegetation classification at the taxonomic resolution of classes, could be a significant contribution for future sustainable and effective ecosystem management."
  },
  {
    "objectID": "consultancy_Mobile.html",
    "href": "consultancy_Mobile.html",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\n\n\n\n\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects."
  },
  {
    "objectID": "consultancy_Mobile.html#statistical-experience",
    "href": "consultancy_Mobile.html#statistical-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\n\n\n\n\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects."
  },
  {
    "objectID": "consultancy_Mobile.html#consultancy-fieldwork-experience",
    "href": "consultancy_Mobile.html#consultancy-fieldwork-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "Consultancy Fieldwork Experience",
    "text": "Consultancy Fieldwork Experience\nI have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp."
  }
]