[
  {
    "objectID": "index_Desktop.html",
    "href": "index_Desktop.html",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "",
    "text": "Bede Ffinian Rowe Davies Post-Doctoral Researcher bedeffinian@gmail.com Laboratoire ISOMer, UFR Sciences et Techniques Nantes University, France."
  },
  {
    "objectID": "index_Desktop.html#projects",
    "href": "index_Desktop.html#projects",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "Projects",
    "text": "Projects\nI have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation."
  },
  {
    "objectID": "posts1/Baited.html",
    "href": "posts1/Baited.html",
    "title": "Ecosystem Approach to Fisheries Management works— How switching from Mbile to Static Fishing Gear Improves Populations of Fished and Non-Fished Species Inside a Marine Protected Area.",
    "section": "",
    "text": "Link here: Davies et al., 2021\nDesignated using a Statutory Instrument in 2008, Lyme Bay marine- protected area (MPA) is the UK’s first and largest example of an ambitious, whole-site approach to management, to recover and protect reef biodiversity. The whole-site approach applies consistent management, in this case excluding bottom towed fishing, across the full 206 km2 extent of the MPA, thus protecting a mosaic of reef-associated habitats from regular damage, while still allowing less destructive fishing methods, such as static gear, rod and line, and diving.2. To assess the effectiveness of this management strategy for mobile taxa and the sustainability for those taxa that continue to be targeted, Exploited and Non- Exploited species’ populations were compared inside the MPA, relative to open control sites spanning 11 of the 12 years of designation. baited remote underwater video systems (BRUVs) were deployed annually to assess mobile benthic and demersal fauna.3. Overall, the number of taxa significantly increased in the MPA relative to the open controls while total abundance increased in both treatments.4. Exploited fish showed increases in number of taxa (430%) and total abundance (370%) inside the MPA over 11 years.5. Likewise, but to a lesser degree in the open controls, number of taxa of commercially Exploited fish increased over time, potentially showing ‘spillover’ effects from the MPA.6. Non-Exploited fish did not show such changes. Regardless of constituting the majority of the fishery value, highly valuable Exploited invertebrates showed no significant changes over time.7. Synthesis and applications. The Lyme Bay marine- protected area shows importance of protecting a whole site, comprising mosaics of different benthic habitats, through protection of sessile organisms that contribute to essential fish habitats."
  },
  {
    "objectID": "posts1/SpectralLibrary.html",
    "href": "posts1/SpectralLibrary.html",
    "title": "Multi- and hyperspectral classification of soft-bottom intertidal vegetation using a spectral library for coastal biodiversity remote sensing.",
    "section": "",
    "text": "Link here: Davies et al., 2023\n\nAbstract\nMonitoring biodiversity and how anthropogenic pressures impact this is critical, especially as anthropogenically driven climate change continues to affect all ecosystems. Intertidal areas are exposed to particularly high levels of pressures owing to increased population density in coastal areas. Traditional methods of monitoring intertidal areas do not provide datasets with full coverage in a cost-effective or timely manner, and so the use of remote sensing to monitor these areas is becoming more common. Monitoring of ecologically important monospecific habitats, such as seagrass beds, using remote sensing techniques is well documented. However, the ability for multispectral data to distinguish efficiently and accurately between classes of vegetation with similar pigment composition, such as seagrass and green algae, has proved difficult, often requiring hyperspectral data. A machine learning approach was used to differentiate between soft-bottom intertidal vegetation classes when exposed at low tide, comparing 6 different multi- and hyperspectral remote and in situ sensors. For the library of 366 spectra, collected across Northern Europe, high accuracy (&gt;80%) was found across all spectral resolutions. While a higher spectral resolution resulted in higher accuracy, there was no discernible increase in accuracy above 10 spectral bands (95%: Sentinel-2 MSI sensor with a spatial resolution of 20 m). This work highlights the ability of multispectral sensors to discriminate intertidal vegetation types, while also showing the most important wavelengths for this discrimination (∼530 and ∼ 730 nm), giving recommendations for spectral ranges of future satellite missions. The ability for multispectral sensors to aid in accurate and rapid intertidal vegetation classification at the taxonomic resolution of classes, could be a significant contribution for future sustainable and effective ecosystem management."
  },
  {
    "objectID": "posts1/TrendsICECREAMS.html",
    "href": "posts1/TrendsICECREAMS.html",
    "title": "Intertidal seagrass extent from Sentinel-2 time-series show distinct trajectories in Western Europe.",
    "section": "",
    "text": "Link here: Davies et al., 2024\n\nAbstract\nIntertidal areas, which emerge during low tide, form a vital link between terrestrial and marine environments. Seagrasses, a well-studied intertidal habitat, provide a multitude of different ecosystem goods and services. However, owing to their relatively high exposure to anthropogenic impacts, seagrasss meadows and other intertidal habitats have seen extensive declines. Remote sensing methods that can capture the spatial and temporal variation of marine habitats are essential to best assess the trajectories of seagrass ecosystems. An advanced machine learning method has been developed to map intertidal vegetation from satellite-derived surface reflectance at a 12-band multispectral resolution and distinguish between similarly pigmented intertidal macrophytes, such as seagrass and green algae. The Intertidal Classification of Europe: Categorising Reflectance of Emerged Areas of Marine vegetation with Sentinel-2 (ICE CREAMS v1.0), a neural network model trained on over 300,000 Sentinel-2 pixels to identify different intertidal habitats, was applied to the open-access long term archive of systematically collected Sentinel-2 imagery to provide 7 years (2017–2023) worth of intertidal seagrass dynamics in 6 sites across Western Europe (471 Sentinel-2 Images). A combination of independently collected visually inspected Uncrewed Aerial Vehicle imagery and in situ quadrat images were used to validate ICE CREAMS. Having achieved a high seagrass classification accuracy (0.82 over 12,000 pixels) and consistent conversion into cover (19% RMSD), the ICE CREAMS model outputs provided evidence of site specific variation in trajectories of seagrass extent, when appropriate consideration of intra-annual variation has been considered. Inter-annual dynamics of sites showed some instances of consistent change, some indicated stability, while others indicated instability over time, characterised by increases and decreases across the time-series in seagrass coverage. This methological pipeline has helped to create up-to-date monitoring data that, with the planned continuation of the Sentinel missions, will allow almost real-time monitoring of these habitats into the future. This process, and the data it provides, could aid management practitioners from regional to international levels, with the ability to monitor intertidal seagrass meadows at both high spatial and temporal resolution, over continental scales. The implementation of Earth Observation for high-resolution monitoring of intertidal seagrasses could therefore allow for gap-filling seagrass datasets, and sustain specific and rapid management measures."
  },
  {
    "objectID": "posts1/Functional.html",
    "href": "posts1/Functional.html",
    "title": "A Decade Implementing Ecosystem Approach to Fisheries Management Improves Diversity of Taxa and Traits within a Marine Protected Area in the UK.",
    "section": "",
    "text": "Link here: Davies et al., 2022\nAim: Ecosystem Approach to Fisheries Management has highlighted the importance of studying ecosystem functions and services, and the biological traits that drive them. Yet, ecosystem services and the associated benefits that they provide are rarely the motive for creating marine protected area (MPA). Therefore, many MPA monitoring projects do not explicitly study these functions and services or the underlying biological traits linked to them. Location: Lyme Bay MPA, located in the SW of England, was established in 2008 to protect the reef biodiversity across a 206 km\\(^2\\) area, which includes rocky reef habitats, pebbly sand and soft muddy sediments. Mobile demersal fishing was excluded across the whole site to allow the recovery of the reef habitats. Methods: Using a combination of towed underwater video and Baited Remote Underwater Video Systems changes in diversity (taxonomic and trait), and traits affected by mobile demersal fishing were assessed in Lyme Bay MPA over 10 years. Results: There was a consistent increase in the number of taxa and the trait diversity they provide within the MPA as well as an increase in functional redundancy, which may increase community resilience to perturbations. Outside of the MPA there was an increase in the abundance of mobile species, while the MPA showed an increase in filter feeders. Main conclusions: The MPA showed a trend towards more diverse and potentially resilient rocky reef habitats. This study constitutes a novel MPA assessment using multiple sampling methods to encompass a wide range of taxa. It also reinforces the importance of effective MPA monitoring, which has demonstrated changes in trait diversity and trait composition driven by changes in taxonomic diversity."
  },
  {
    "objectID": "Blogs.html",
    "href": "Blogs.html",
    "title": "Travel Blogs",
    "section": "",
    "text": "A Greenlandic Tale\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index_Mobile.html",
    "href": "index_Mobile.html",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "",
    "text": "Bede Ffinian Rowe Davies Post-Doctoral Researcher bedeffinian@gmail.com Laboratoire ISOMer, UFR Sciences et Techniques Nantes University, France."
  },
  {
    "objectID": "index_Mobile.html#projects",
    "href": "index_Mobile.html#projects",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "Projects",
    "text": "Projects\nI have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\n\n\n\n\n\n\n\n\n\n\n\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation."
  },
  {
    "objectID": "IntermediateRTutorials/ComplexGIS.html",
    "href": "IntermediateRTutorials/ComplexGIS.html",
    "title": "Complex GIS in R",
    "section": "",
    "text": "So now we know how to read in, create and download different sources of Vector data and Rasters, use and combine these data to produce maps. Now lets look at using some of these skills but combining different data sources, converting between different resolutions and dimensions, converting between data types and then create summary statistics within spatial limits.\n\n\nThrough my work, I often have a habitat classification of an area that has a resolution and dimension of the original imagery and then we want to compare this classification to other environmental data, such as temperature, solar radiation or salinity. However, very rarely will the open access environmental data align with the habitat classification. So we will look at habitat classification of an area with a specific (maybe illogical) resolution and dimension, then we will relate these environmental data to then split the environmental data by its classification.\n\n\n\n\n\nWe could create our own classification model and use the output of that but to stick to our objective we will download a classification from r package. This will come from the geodata package, where there is an example of the CORINE landcover classification of the Island São Miguel from the Azores. This has a lot of complicated groups (23 to be exact) so we will combine them into more general groupings. We can use tidyterra to do this,which mimics tidyverse methods.\n\nlibrary(exactextractr)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(tidyverse)\n\n\nclassification &lt;- rast(system.file('sao_miguel/clc2018_v2020_20u1.tif',\n                     package = 'exactextractr')) \n\nprint(unique(classification$LABEL3))\n\n                                                                                   LABEL3\n1                                                                 Continuous urban fabric\n2                                                              Discontinuous urban fabric\n3                                                          Industrial or commercial units\n4                                                                              Port areas\n5                                                                                Airports\n6                                                                Mineral extraction sites\n7                                                                              Dump sites\n8                                                                      Construction sites\n9                                                                       Green urban areas\n10                                                           Sport and leisure facilities\n11                                                              Non-irrigated arable land\n12                                                      Fruit trees and berry plantations\n13                                                                               Pastures\n14                                                           Complex cultivation patterns\n15 Land principally occupied by agriculture, with significant areas of natural vegetation\n16                                                                    Broad-leaved forest\n17                                                                      Coniferous forest\n18                                                                           Mixed forest\n19                                                                     Natural grasslands\n20                                                                    Moors and heathland\n21                                                            Transitional woodland-shrub\n22                                                                           Water bodies\n23                                                                          Sea and ocean\n\nGrouped_Classification&lt;-classification%&gt;% \n  tidyterra::mutate(LABEL3=case_when(\n    LABEL3%in%c(\"Continuous urban fabric\",\n                  \"Discontinuous urban fabric\",\n                  \"Industrial or commercial units\",\n                  \"Port areas\",\n                  \"Airports\",\n                  \"Mineral extraction sites\",\n                  \"Dump sites\",\n                  \"Construction sites\",\n                  \"Green urban areas\",\n                  \"Sport and leisure facilities\")~\"Urban\",\n      LABEL3%in%c(\"Broad-leaved forest\",\n                  \"Coniferous forest\",\n                  \"Mixed forest\",\n                  \"Natural grasslands\",\n                  \"Moors and heathland\",\n                  \"Transitional woodland-shrub\")~\"Wild Vegetation\",\n      LABEL3%in%c(\"Water bodies\",\n                  \"Sea and ocean\")~\"Ocean/Water Body\",\n      TRUE~\"Farmland\")\n                    )\n\n\nggplot()+\n  geom_spatraster(data=Grouped_Classification,\n                  maxcell = 5e+7\n                  )+\n  labs(title=\"São Miguel: Landcover Classification\")+\n  scale_fill_manual(name=\"\",\n                    values = c(\"#e86a28\",\"#008B8B\", \"#fde825\", \"#9fcb41\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nWe have already previously downloaded climate data from the geodata package, we can do this again for the same extent as our Habitat data, we will download temperature, elevation, precipitation and population density. Again as before some data will be a layer for each month, so we will average across the layers.\n\nlibrary(geodata)\nlibrary(patchwork)\n\navgtemp &lt;- worldclim_tile(var=\"tavg\",\n                          lon=-25.5,lat=37.8,\n                          path=tempdir())\n\navgtemp_SM&lt;-avgtemp %&gt;% \n  crop(Grouped_Classification)%&gt;% \n  mean()\n\np1&lt;-ggplot()+\n  geom_spatraster(data=avgtemp_SM)+\n  labs(title=\"Average Temperature (°C)\",fill=\"\")+\n  scale_fill_whitebox_c(\"muted\",na.value = NA)+\n  theme_classic()\n\n\nElevation_SM&lt;-elevation_3s(lon=-25.5,lat=37.8, \n                           path=tempdir())%&gt;% \n  crop(Grouped_Classification)\n\np2&lt;-ggplot()+\n  geom_spatraster(data=Elevation_SM)+\n  labs(title=\"Elevation (m)\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA)+\n  theme_classic()\n\navgprec_SM &lt;- worldclim_tile(var=\"prec\",lon=-25.5,lat=37.8,\n                             path=tempdir())%&gt;% \n  crop(Grouped_Classification) %&gt;% \n  mean()\n\np3&lt;-ggplot()+\n  geom_spatraster(data=avgprec_SM)+\n  labs(title=\"Total Precipitation (mm)\",fill=\"\")+\n  scale_fill_hypso_c(\"colombia\",na.value = NA)+\n  theme_classic()\n\n\npop_density &lt;- rast(system.file('sao_miguel/gpw_v411_2020_density_2020.tif',\n                                  package = 'exactextractr'))\n\np4&lt;-ggplot()+\n  geom_spatraster(data=pop_density)+\n  labs(title=\"Population density\",fill=\"\")+\n  scale_fill_whitebox_c(\"soft\",na.value = NA)+\n  theme_classic()\n\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\n\n\n\nBefore we combine all these data together and start assessing potential patterns we will want to make sure our data are at comparable coordinate reference systems, resolutions and extents.\n\ncrs(Grouped_Classification,describe=T)\ncrs(avgtemp_SM,describe=T)\ncrs(Elevation_SM,describe=T)\ncrs(avgprec_SM,describe=T)\ncrs(pop_density,describe=T)\n\nres(Grouped_Classification)\nres(avgtemp_SM)\nres(Elevation_SM)\nres(avgprec_SM)\nres(pop_density)\n\next(Grouped_Classification)\next(avgtemp_SM)\next(Elevation_SM)\next(avgprec_SM)\next(pop_density)\n\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n     name authority code area         extent\n1 unknown      &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA, NA, NA, NA\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n[1] 0.0002083333 0.0002083333\n[1] 0.008333333 0.008333333\n[1] 0.0008333333 0.0008333333\n[1] 0.008333333 0.008333333\n[1] 0.008333333 0.008333333\nSpatExtent : -25.9, -25.100000000128, 37.600000000064, 38 (xmin, xmax, ymin, ymax)\nSpatExtent : -25.9, -25.1, 37.6, 38 (xmin, xmax, ymin, ymax)\nSpatExtent : -25.9, -25.1, 37.6, 38 (xmin, xmax, ymin, ymax)\nSpatExtent : -25.9, -25.1, 37.6, 38 (xmin, xmax, ymin, ymax)\nSpatExtent : -25.9000000000001, -25.1000000000001, 37.5999999999999, 37.9999999999999 (xmin, xmax, ymin, ymax)\n\n\n\n\n\n\nThe elevation raster has no CRS so we need to set it. We can set it using the crs of one of the other rasters.\n\ncrs(Elevation_SM)&lt;-crs(avgtemp_SM)\n\ncrs(Elevation_SM,describe=T)\n\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n\n\n\n\n\nCombining the resolution and the extents we can work out the dimension, as the difference in the extent divided by the resolution will return the dimensions. Therefore, making the extents the same, then setting the resolutions to the same will make dimensions be the same likewise. Thus, allowing us to combine the rasters together and inspect each element as a new band or layer. We can achieve this using the resample function from terra.\n\n\n\nOkay so temperature, precipitation and population density are at the same resolution but the classification and the elevation are different to all others. To combine them well we will want to resample our data all to the same scale. This could be resampling all low resolutions rasters to a higher resolution to match the the highest resolution raster, the classification, or perhaps we might want to resample to the lowest resolution, the temperature and precipitation. We will attempt both and see how it affects our analyses later on.\n\n\nSo our temperature raster is the lowest resolution (same as Precipitation) so we want to use the resample function. But we need to decide how to combine multiple values into one, we might take the mean, median, max, min, quartiles, sum or mode. The default for a continuous value is bilinear interpolation, but cubic, cubic spline, lanczos window among others are available in terra. We should chose a method that works best for our objective. For the elevation we can see what the differences will be for multiple different methods. We will set the scales to all be the same to highlight differences.\n\nElevation_Low_max&lt;-resample(Elevation_SM,avgtemp_SM,method=\"max\")\nElevation_Low_min&lt;-resample(Elevation_SM,avgtemp_SM,method=\"min\")\nElevation_Low_average&lt;-resample(Elevation_SM,avgtemp_SM,method=\"average\")\nElevation_Low_bilinear&lt;-resample(Elevation_SM,avgtemp_SM,method=\"bilinear\")\nElevation_Low_cubic&lt;-resample(Elevation_SM,avgtemp_SM,method=\"cubic\")\n\np5&lt;-ggplot()+\n  geom_spatraster(data=Elevation_SM)+\n  labs(title=\"Elevation (m) Original Res\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np6&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_max)+\n  labs(title=\"Low Resolution Max\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np7&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_min)+\n  labs(title=\"Low Resolution Min\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np8&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_average)+\n  labs(title=\"Low Resolution Average\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np9&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_bilinear)+\n  labs(title=\"Low Resolution Bilinear\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np10&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_cubic)+\n  labs(title=\"Low Resolution Cubic\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\n\n(p5+p6)/(p7+p8)/(p9+p10)+plot_layout(guides=\"collect\")\n\n\n\n\n\n\n\n\nOkay this seems pretty similar but the biggest difference come from min, max and average, while bilinear, cubic and average all appear very similar. For this reason we will use bilinear for the elevation. However, for the classification we have a numeric value, but it is categorical. Therefore we will use the mode value. But lets have a look at how the others act too just to be curious.\n\nGrouped_Classification_Low_max&lt;-resample(Grouped_Classification,\n                                         avgtemp_SM,method=\"max\")\nGrouped_Classification_Low_min&lt;-resample(Grouped_Classification,\n                                         avgtemp_SM,method=\"min\")\nGrouped_Classification_Low_average&lt;-resample(Grouped_Classification,\n                                             avgtemp_SM,method=\"average\")\nGrouped_Classification_Low_bilinear&lt;-resample(Grouped_Classification,\n                                              avgtemp_SM,method=\"bilinear\")\nGrouped_Classification_Low_mode&lt;-resample(Grouped_Classification,\n                                          avgtemp_SM,method=\"mode\")\n\np11&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification,\n                  maxcell = 5e+7)+\n  labs(title=\"Classification: Original Res\",fill=\"\")+\n  scale_fill_manual(name=\"\",\n                    values = c(\"#e86a28\",\n                               \"#008B8B\", \n                               \"#fde825\", \n                               \"#9fcb41\"))+\n  theme_classic()\n\np12&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_max)+\n  labs(title=\"Low Resolution Max\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\np13&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_min)+\n  labs(title=\"Low Resolution Min\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\n\np14&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_average)+\n  labs(title=\"Low Resolution Average\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\n\np15&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_bilinear)+\n  labs(title=\"Low Resolution Bilinear\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\n\np16&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_mode)+\n  labs(title=\"Low Resolution Mode\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\n\n\n(p11+p12)/(p13+p14)/(p15+p16)+plot_layout(guides=\"collect\")\n\n\n\n\n\n\n\n\nTo make sure all extents and resolutions are the same we will resample the other rasters with the same base level/raster: temperature. This shouldn’t change anything, just sorts any odd extents or dimensions, especially small unrounded values. The method of resample won’t matter so we will leave bilinear as default. We also want to change the classification back to be a category not a number.\n\navgprec_Low&lt;-resample(avgprec_SM,avgtemp_SM)\npop_density_Low&lt;-resample(pop_density,avgtemp_SM)\n\nGrouped_Classification_Low_mode&lt;-Grouped_Classification_Low_mode%&gt;% \n  tidyterra::mutate(LABEL3=case_when(LABEL3==1~\"Farmland\",\n                                     LABEL3==2~\"Ocean/Water Body\",\n                                     LABEL3==3~\"Urban\",\n                                     LABEL3==4~\"Wild Vegetation\"))\n\n\n\n\nConversely we can increase the resolution of all rasters to be the highest level. Our Classification raster is the highest so we want to use the resample function as above but this time it repeats values at higher resolution rather than combining them into one low resolution value. Again, this means the default method is fine, as all methods should return the same result.\n\nElevation_High&lt;-resample(Elevation_SM,Grouped_Classification)\nTemperature_High&lt;-resample(avgtemp_SM,Grouped_Classification)\nPrecipitation_High&lt;-resample(avgprec_SM,Grouped_Classification)\nPopulation_High&lt;-resample(pop_density,Grouped_Classification)\n\n\np17&lt;-ggplot()+\n  geom_spatraster(data=Elevation_High,\n                   maxcell = 5e+7)+\n  labs(title=\"Elevation High Res\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA)+\n  theme_classic()\n\np18&lt;-ggplot()+\n  geom_spatraster(data=Temperature_High,\n                  maxcell = 5e+7)+\n  labs(title=\"Temperature High Res\",fill=\"\")+\n  scale_fill_whitebox_c(\"muted\",na.value = NA)+\n  theme_classic()\n\np19&lt;-ggplot()+\n  geom_spatraster(data=Precipitation_High,\n                  maxcell = 5e+7)+\n  labs(title=\"Precipitation High Res\",fill=\"\")+\n  scale_fill_hypso_c(\"colombia\",na.value = NA)+\n  theme_classic()\n\n\np20&lt;-ggplot()+\n  geom_spatraster(data=Population_High,\n                  maxcell = 5e+7)+\n  labs(title=\"Popultation High Res\",fill=\"\")+\n  scale_fill_whitebox_c(\"soft\",na.value = NA)+\n  theme_classic()\n\n\n\n(p17+p18)/(p19+p20)\n\n\n\n\n\n\n\n\nThis doesn’t really change the appearance of our plots but we can see the resolution is now higher.\n\n\n\n\nWe can just use the c() function to add layers of a raster together into one multi layer or band raster. When we have a multilayer raster like this but the bands have different scales, plotting them with ggplot and tidyterra doesn’t really work easily, but thankfully the base plot function works reasonably well just for quick inspection.\n\nCombined_HighRes&lt;-c(Grouped_Classification,\n                    Elevation_High,\n                    Temperature_High,\n                    Precipitation_High,\n                    Population_High)\n\nnames(Combined_HighRes)&lt;-c(\"Classification\",\n                           \"Elevation\",\n                           \"Temperature\",\n                           \"Precipitation\",\n                           \"Population\")\n\nplot(Combined_HighRes)\n\n\n\n\n\n\n\nCombined_LowRes&lt;-c(Grouped_Classification_Low_mode,\n                   Elevation_Low_bilinear,\n                   avgtemp_SM,\n                   avgprec_Low,\n                   pop_density_Low)\n\nnames(Combined_LowRes)&lt;-c(\"Classification\",\n                          \"Elevation\",\n                          \"Temperature\",\n                          \"Precipitation\",\n                          \"Population\")\n\nplot(Combined_LowRes)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor many of the analysis methods, formal and informal, we will probably want our data to be in a tibble format. Then we can look at how to compare the different classes. Firstly, most of our environmental data is terrestrial so we can remove all the ocean areas. Then compare environmental data across classes and if the different spatial resolutions change this relationship.\n\nLowRes_df&lt;-as.data.frame(Combined_LowRes,xy=T) %&gt;% \n  mutate(Res=\"Low\")\n\nHighRes_df&lt;-as.data.frame(Combined_HighRes,xy=T)%&gt;% \n  mutate(Res=\"High\")\n\nHighRes_df %&gt;% \n  bind_rows(LowRes_df) %&gt;% \n  drop_na() %&gt;% \n  pivot_longer(-c(x,y,Classification,Res),\n               names_to = \"Metric\",values_to = \"value\")%&gt;% \n  ggplot(aes(x=Classification,y=value,fill=Res))+\n  geom_boxplot()+\n  facet_wrap(~Metric,scales = \"free\")+\n  scale_fill_manual(name=\"Resolution\",\n                    values=c(\"#36b779\",\"#2d85c5\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nGenerally the same patterns are in high and low (not surprisingly). To give a little example analysis we could create a model based on the low resolution environmental data, and see if we can accurately predict the high resolution classification. This won’t be the best or most effective way at doing this task, but an example! DO NOT EMULATE THIS EXCEPT AS A LEARNING EXERCISE (Check other tutorials for applying models correctly, what steps to take and best practices). For example, if you look at temperature, elevation, precipitation for example they are clearly correlated so would require more complex modelling to do correctly.\n\nlowres_Model&lt;-nnet::multinom(Classification~Elevation+\n                         Temperature+\n                         Precipitation+\n                         Population,\n                       data=LowRes_df)\n\n# weights:  24 (15 variable)\ninitial  value 1497.197910 \niter  10 value 888.197723\niter  20 value 657.680242\niter  30 value 644.902880\niter  40 value 644.384735\niter  40 value 644.384732\nfinal  value 644.383545 \nconverged\n\nHighRes_df$Pred&lt;-predict(lowres_Model,\n                         newdata=HighRes_df,\"class\")\n\nSo now we have a prediction and a “true” class at high resolution, lets convert back to raster and compare the true and predicted maps. To do this we convert our dataframe to an sf object, create an empty rast() with the same resolution, crs and extent as our original high resolution rast, then fill the empty raster with our predictions from the model, we use a min function but actually it should not summarise at all. Then we can stack the true and the predicted rasters together to visualise the difference.\n\nHighRes_sf&lt;-st_as_sf(HighRes_df,coords=c(\"x\",\"y\"))\n\n\nHighRes_Pred_raster &lt;- rast(resolution = res(Grouped_Classification),\n                               crs = crs(Grouped_Classification),\n                               ext = ext(Grouped_Classification)) %&gt;% \n  rasterize(vect(HighRes_sf), ., field = \"Pred\", fun = \"min\")%&gt;% \n  tidyterra::mutate(Pred_min=case_when(\n    Pred_min==0~\"Farmland\",\n    Pred_min==1~\"Ocean/Water Body\",\n    Pred_min==2~\"Urban\",\n    Pred_min==3~\"Wild Vegetation\"))\n\n\nHighRes_True_Pred_raster&lt;-c(Grouped_Classification,\n                            HighRes_Pred_raster)\n\nnames(HighRes_True_Pred_raster)&lt;-c(\"'True'\",\"Prediction\")\n\nggplot()+\n  geom_spatraster(data=HighRes_True_Pred_raster,\n                  maxcell = 5e+7\n                  )+\n  labs(title=\"São Miguel: Landcover Classification\")+\n  facet_wrap(~lyr)+\n  scale_fill_manual(name=\"\",\n                    values = c(\"#e86a28\",\n                               \"#008B8B\", \n                               \"#fde825\", \n                               \"#9fcb41\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nActually not a very good prediction, but that is not surprising given the model that was used and the little input data used. We will go through some remote sensing tutorials and then we can apply Machine Learning methods alongside spatial data skills to create some prediction models that will hopefully do a lot better.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Complex GIS in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ComplexGIS.html#complex-gis",
    "href": "IntermediateRTutorials/ComplexGIS.html#complex-gis",
    "title": "Complex GIS in R",
    "section": "",
    "text": "So now we know how to read in, create and download different sources of Vector data and Rasters, use and combine these data to produce maps. Now lets look at using some of these skills but combining different data sources, converting between different resolutions and dimensions, converting between data types and then create summary statistics within spatial limits.\n\n\nThrough my work, I often have a habitat classification of an area that has a resolution and dimension of the original imagery and then we want to compare this classification to other environmental data, such as temperature, solar radiation or salinity. However, very rarely will the open access environmental data align with the habitat classification. So we will look at habitat classification of an area with a specific (maybe illogical) resolution and dimension, then we will relate these environmental data to then split the environmental data by its classification.\n\n\n\n\n\nWe could create our own classification model and use the output of that but to stick to our objective we will download a classification from r package. This will come from the geodata package, where there is an example of the CORINE landcover classification of the Island São Miguel from the Azores. This has a lot of complicated groups (23 to be exact) so we will combine them into more general groupings. We can use tidyterra to do this,which mimics tidyverse methods.\n\nlibrary(exactextractr)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(tidyverse)\n\n\nclassification &lt;- rast(system.file('sao_miguel/clc2018_v2020_20u1.tif',\n                     package = 'exactextractr')) \n\nprint(unique(classification$LABEL3))\n\n                                                                                   LABEL3\n1                                                                 Continuous urban fabric\n2                                                              Discontinuous urban fabric\n3                                                          Industrial or commercial units\n4                                                                              Port areas\n5                                                                                Airports\n6                                                                Mineral extraction sites\n7                                                                              Dump sites\n8                                                                      Construction sites\n9                                                                       Green urban areas\n10                                                           Sport and leisure facilities\n11                                                              Non-irrigated arable land\n12                                                      Fruit trees and berry plantations\n13                                                                               Pastures\n14                                                           Complex cultivation patterns\n15 Land principally occupied by agriculture, with significant areas of natural vegetation\n16                                                                    Broad-leaved forest\n17                                                                      Coniferous forest\n18                                                                           Mixed forest\n19                                                                     Natural grasslands\n20                                                                    Moors and heathland\n21                                                            Transitional woodland-shrub\n22                                                                           Water bodies\n23                                                                          Sea and ocean\n\nGrouped_Classification&lt;-classification%&gt;% \n  tidyterra::mutate(LABEL3=case_when(\n    LABEL3%in%c(\"Continuous urban fabric\",\n                  \"Discontinuous urban fabric\",\n                  \"Industrial or commercial units\",\n                  \"Port areas\",\n                  \"Airports\",\n                  \"Mineral extraction sites\",\n                  \"Dump sites\",\n                  \"Construction sites\",\n                  \"Green urban areas\",\n                  \"Sport and leisure facilities\")~\"Urban\",\n      LABEL3%in%c(\"Broad-leaved forest\",\n                  \"Coniferous forest\",\n                  \"Mixed forest\",\n                  \"Natural grasslands\",\n                  \"Moors and heathland\",\n                  \"Transitional woodland-shrub\")~\"Wild Vegetation\",\n      LABEL3%in%c(\"Water bodies\",\n                  \"Sea and ocean\")~\"Ocean/Water Body\",\n      TRUE~\"Farmland\")\n                    )\n\n\nggplot()+\n  geom_spatraster(data=Grouped_Classification,\n                  maxcell = 5e+7\n                  )+\n  labs(title=\"São Miguel: Landcover Classification\")+\n  scale_fill_manual(name=\"\",\n                    values = c(\"#e86a28\",\"#008B8B\", \"#fde825\", \"#9fcb41\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nWe have already previously downloaded climate data from the geodata package, we can do this again for the same extent as our Habitat data, we will download temperature, elevation, precipitation and population density. Again as before some data will be a layer for each month, so we will average across the layers.\n\nlibrary(geodata)\nlibrary(patchwork)\n\navgtemp &lt;- worldclim_tile(var=\"tavg\",\n                          lon=-25.5,lat=37.8,\n                          path=tempdir())\n\navgtemp_SM&lt;-avgtemp %&gt;% \n  crop(Grouped_Classification)%&gt;% \n  mean()\n\np1&lt;-ggplot()+\n  geom_spatraster(data=avgtemp_SM)+\n  labs(title=\"Average Temperature (°C)\",fill=\"\")+\n  scale_fill_whitebox_c(\"muted\",na.value = NA)+\n  theme_classic()\n\n\nElevation_SM&lt;-elevation_3s(lon=-25.5,lat=37.8, \n                           path=tempdir())%&gt;% \n  crop(Grouped_Classification)\n\np2&lt;-ggplot()+\n  geom_spatraster(data=Elevation_SM)+\n  labs(title=\"Elevation (m)\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA)+\n  theme_classic()\n\navgprec_SM &lt;- worldclim_tile(var=\"prec\",lon=-25.5,lat=37.8,\n                             path=tempdir())%&gt;% \n  crop(Grouped_Classification) %&gt;% \n  mean()\n\np3&lt;-ggplot()+\n  geom_spatraster(data=avgprec_SM)+\n  labs(title=\"Total Precipitation (mm)\",fill=\"\")+\n  scale_fill_hypso_c(\"colombia\",na.value = NA)+\n  theme_classic()\n\n\npop_density &lt;- rast(system.file('sao_miguel/gpw_v411_2020_density_2020.tif',\n                                  package = 'exactextractr'))\n\np4&lt;-ggplot()+\n  geom_spatraster(data=pop_density)+\n  labs(title=\"Population density\",fill=\"\")+\n  scale_fill_whitebox_c(\"soft\",na.value = NA)+\n  theme_classic()\n\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\n\n\n\nBefore we combine all these data together and start assessing potential patterns we will want to make sure our data are at comparable coordinate reference systems, resolutions and extents.\n\ncrs(Grouped_Classification,describe=T)\ncrs(avgtemp_SM,describe=T)\ncrs(Elevation_SM,describe=T)\ncrs(avgprec_SM,describe=T)\ncrs(pop_density,describe=T)\n\nres(Grouped_Classification)\nres(avgtemp_SM)\nres(Elevation_SM)\nres(avgprec_SM)\nres(pop_density)\n\next(Grouped_Classification)\next(avgtemp_SM)\next(Elevation_SM)\next(avgprec_SM)\next(pop_density)\n\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n     name authority code area         extent\n1 unknown      &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA, NA, NA, NA\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n[1] 0.0002083333 0.0002083333\n[1] 0.008333333 0.008333333\n[1] 0.0008333333 0.0008333333\n[1] 0.008333333 0.008333333\n[1] 0.008333333 0.008333333\nSpatExtent : -25.9, -25.100000000128, 37.600000000064, 38 (xmin, xmax, ymin, ymax)\nSpatExtent : -25.9, -25.1, 37.6, 38 (xmin, xmax, ymin, ymax)\nSpatExtent : -25.9, -25.1, 37.6, 38 (xmin, xmax, ymin, ymax)\nSpatExtent : -25.9, -25.1, 37.6, 38 (xmin, xmax, ymin, ymax)\nSpatExtent : -25.9000000000001, -25.1000000000001, 37.5999999999999, 37.9999999999999 (xmin, xmax, ymin, ymax)\n\n\n\n\n\n\nThe elevation raster has no CRS so we need to set it. We can set it using the crs of one of the other rasters.\n\ncrs(Elevation_SM)&lt;-crs(avgtemp_SM)\n\ncrs(Elevation_SM,describe=T)\n\n    name authority code  area             extent\n1 WGS 84      EPSG 4326 World -180, 180, 90, -90\n\n\n\n\n\nCombining the resolution and the extents we can work out the dimension, as the difference in the extent divided by the resolution will return the dimensions. Therefore, making the extents the same, then setting the resolutions to the same will make dimensions be the same likewise. Thus, allowing us to combine the rasters together and inspect each element as a new band or layer. We can achieve this using the resample function from terra.\n\n\n\nOkay so temperature, precipitation and population density are at the same resolution but the classification and the elevation are different to all others. To combine them well we will want to resample our data all to the same scale. This could be resampling all low resolutions rasters to a higher resolution to match the the highest resolution raster, the classification, or perhaps we might want to resample to the lowest resolution, the temperature and precipitation. We will attempt both and see how it affects our analyses later on.\n\n\nSo our temperature raster is the lowest resolution (same as Precipitation) so we want to use the resample function. But we need to decide how to combine multiple values into one, we might take the mean, median, max, min, quartiles, sum or mode. The default for a continuous value is bilinear interpolation, but cubic, cubic spline, lanczos window among others are available in terra. We should chose a method that works best for our objective. For the elevation we can see what the differences will be for multiple different methods. We will set the scales to all be the same to highlight differences.\n\nElevation_Low_max&lt;-resample(Elevation_SM,avgtemp_SM,method=\"max\")\nElevation_Low_min&lt;-resample(Elevation_SM,avgtemp_SM,method=\"min\")\nElevation_Low_average&lt;-resample(Elevation_SM,avgtemp_SM,method=\"average\")\nElevation_Low_bilinear&lt;-resample(Elevation_SM,avgtemp_SM,method=\"bilinear\")\nElevation_Low_cubic&lt;-resample(Elevation_SM,avgtemp_SM,method=\"cubic\")\n\np5&lt;-ggplot()+\n  geom_spatraster(data=Elevation_SM)+\n  labs(title=\"Elevation (m) Original Res\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np6&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_max)+\n  labs(title=\"Low Resolution Max\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np7&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_min)+\n  labs(title=\"Low Resolution Min\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np8&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_average)+\n  labs(title=\"Low Resolution Average\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np9&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_bilinear)+\n  labs(title=\"Low Resolution Bilinear\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\np10&lt;-ggplot()+\n  geom_spatraster(data=Elevation_Low_cubic)+\n  labs(title=\"Low Resolution Cubic\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA,\n                     limits=c(0,1000))+\n  theme_classic()\n\n\n(p5+p6)/(p7+p8)/(p9+p10)+plot_layout(guides=\"collect\")\n\n\n\n\n\n\n\n\nOkay this seems pretty similar but the biggest difference come from min, max and average, while bilinear, cubic and average all appear very similar. For this reason we will use bilinear for the elevation. However, for the classification we have a numeric value, but it is categorical. Therefore we will use the mode value. But lets have a look at how the others act too just to be curious.\n\nGrouped_Classification_Low_max&lt;-resample(Grouped_Classification,\n                                         avgtemp_SM,method=\"max\")\nGrouped_Classification_Low_min&lt;-resample(Grouped_Classification,\n                                         avgtemp_SM,method=\"min\")\nGrouped_Classification_Low_average&lt;-resample(Grouped_Classification,\n                                             avgtemp_SM,method=\"average\")\nGrouped_Classification_Low_bilinear&lt;-resample(Grouped_Classification,\n                                              avgtemp_SM,method=\"bilinear\")\nGrouped_Classification_Low_mode&lt;-resample(Grouped_Classification,\n                                          avgtemp_SM,method=\"mode\")\n\np11&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification,\n                  maxcell = 5e+7)+\n  labs(title=\"Classification: Original Res\",fill=\"\")+\n  scale_fill_manual(name=\"\",\n                    values = c(\"#e86a28\",\n                               \"#008B8B\", \n                               \"#fde825\", \n                               \"#9fcb41\"))+\n  theme_classic()\n\np12&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_max)+\n  labs(title=\"Low Resolution Max\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\np13&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_min)+\n  labs(title=\"Low Resolution Min\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\n\np14&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_average)+\n  labs(title=\"Low Resolution Average\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\n\np15&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_bilinear)+\n  labs(title=\"Low Resolution Bilinear\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\n\np16&lt;-ggplot()+\n  geom_spatraster(data=Grouped_Classification_Low_mode)+\n  labs(title=\"Low Resolution Mode\",fill=\"\")+\n  scale_fill_gradientn(name=\"\",\n                       colors = c(\"#e86a28\",\n                                  \"#008B8B\", \n                                  \"#fde825\", \n                                  \"#9fcb41\"))+\n  theme_classic()\n\n\n\n(p11+p12)/(p13+p14)/(p15+p16)+plot_layout(guides=\"collect\")\n\n\n\n\n\n\n\n\nTo make sure all extents and resolutions are the same we will resample the other rasters with the same base level/raster: temperature. This shouldn’t change anything, just sorts any odd extents or dimensions, especially small unrounded values. The method of resample won’t matter so we will leave bilinear as default. We also want to change the classification back to be a category not a number.\n\navgprec_Low&lt;-resample(avgprec_SM,avgtemp_SM)\npop_density_Low&lt;-resample(pop_density,avgtemp_SM)\n\nGrouped_Classification_Low_mode&lt;-Grouped_Classification_Low_mode%&gt;% \n  tidyterra::mutate(LABEL3=case_when(LABEL3==1~\"Farmland\",\n                                     LABEL3==2~\"Ocean/Water Body\",\n                                     LABEL3==3~\"Urban\",\n                                     LABEL3==4~\"Wild Vegetation\"))\n\n\n\n\nConversely we can increase the resolution of all rasters to be the highest level. Our Classification raster is the highest so we want to use the resample function as above but this time it repeats values at higher resolution rather than combining them into one low resolution value. Again, this means the default method is fine, as all methods should return the same result.\n\nElevation_High&lt;-resample(Elevation_SM,Grouped_Classification)\nTemperature_High&lt;-resample(avgtemp_SM,Grouped_Classification)\nPrecipitation_High&lt;-resample(avgprec_SM,Grouped_Classification)\nPopulation_High&lt;-resample(pop_density,Grouped_Classification)\n\n\np17&lt;-ggplot()+\n  geom_spatraster(data=Elevation_High,\n                   maxcell = 5e+7)+\n  labs(title=\"Elevation High Res\",fill=\"\")+\n  scale_fill_hypso_c(\"dem_poster\",na.value = NA)+\n  theme_classic()\n\np18&lt;-ggplot()+\n  geom_spatraster(data=Temperature_High,\n                  maxcell = 5e+7)+\n  labs(title=\"Temperature High Res\",fill=\"\")+\n  scale_fill_whitebox_c(\"muted\",na.value = NA)+\n  theme_classic()\n\np19&lt;-ggplot()+\n  geom_spatraster(data=Precipitation_High,\n                  maxcell = 5e+7)+\n  labs(title=\"Precipitation High Res\",fill=\"\")+\n  scale_fill_hypso_c(\"colombia\",na.value = NA)+\n  theme_classic()\n\n\np20&lt;-ggplot()+\n  geom_spatraster(data=Population_High,\n                  maxcell = 5e+7)+\n  labs(title=\"Popultation High Res\",fill=\"\")+\n  scale_fill_whitebox_c(\"soft\",na.value = NA)+\n  theme_classic()\n\n\n\n(p17+p18)/(p19+p20)\n\n\n\n\n\n\n\n\nThis doesn’t really change the appearance of our plots but we can see the resolution is now higher.\n\n\n\n\nWe can just use the c() function to add layers of a raster together into one multi layer or band raster. When we have a multilayer raster like this but the bands have different scales, plotting them with ggplot and tidyterra doesn’t really work easily, but thankfully the base plot function works reasonably well just for quick inspection.\n\nCombined_HighRes&lt;-c(Grouped_Classification,\n                    Elevation_High,\n                    Temperature_High,\n                    Precipitation_High,\n                    Population_High)\n\nnames(Combined_HighRes)&lt;-c(\"Classification\",\n                           \"Elevation\",\n                           \"Temperature\",\n                           \"Precipitation\",\n                           \"Population\")\n\nplot(Combined_HighRes)\n\n\n\n\n\n\n\nCombined_LowRes&lt;-c(Grouped_Classification_Low_mode,\n                   Elevation_Low_bilinear,\n                   avgtemp_SM,\n                   avgprec_Low,\n                   pop_density_Low)\n\nnames(Combined_LowRes)&lt;-c(\"Classification\",\n                          \"Elevation\",\n                          \"Temperature\",\n                          \"Precipitation\",\n                          \"Population\")\n\nplot(Combined_LowRes)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor many of the analysis methods, formal and informal, we will probably want our data to be in a tibble format. Then we can look at how to compare the different classes. Firstly, most of our environmental data is terrestrial so we can remove all the ocean areas. Then compare environmental data across classes and if the different spatial resolutions change this relationship.\n\nLowRes_df&lt;-as.data.frame(Combined_LowRes,xy=T) %&gt;% \n  mutate(Res=\"Low\")\n\nHighRes_df&lt;-as.data.frame(Combined_HighRes,xy=T)%&gt;% \n  mutate(Res=\"High\")\n\nHighRes_df %&gt;% \n  bind_rows(LowRes_df) %&gt;% \n  drop_na() %&gt;% \n  pivot_longer(-c(x,y,Classification,Res),\n               names_to = \"Metric\",values_to = \"value\")%&gt;% \n  ggplot(aes(x=Classification,y=value,fill=Res))+\n  geom_boxplot()+\n  facet_wrap(~Metric,scales = \"free\")+\n  scale_fill_manual(name=\"Resolution\",\n                    values=c(\"#36b779\",\"#2d85c5\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nGenerally the same patterns are in high and low (not surprisingly). To give a little example analysis we could create a model based on the low resolution environmental data, and see if we can accurately predict the high resolution classification. This won’t be the best or most effective way at doing this task, but an example! DO NOT EMULATE THIS EXCEPT AS A LEARNING EXERCISE (Check other tutorials for applying models correctly, what steps to take and best practices). For example, if you look at temperature, elevation, precipitation for example they are clearly correlated so would require more complex modelling to do correctly.\n\nlowres_Model&lt;-nnet::multinom(Classification~Elevation+\n                         Temperature+\n                         Precipitation+\n                         Population,\n                       data=LowRes_df)\n\n# weights:  24 (15 variable)\ninitial  value 1497.197910 \niter  10 value 888.197723\niter  20 value 657.680242\niter  30 value 644.902880\niter  40 value 644.384735\niter  40 value 644.384732\nfinal  value 644.383545 \nconverged\n\nHighRes_df$Pred&lt;-predict(lowres_Model,\n                         newdata=HighRes_df,\"class\")\n\nSo now we have a prediction and a “true” class at high resolution, lets convert back to raster and compare the true and predicted maps. To do this we convert our dataframe to an sf object, create an empty rast() with the same resolution, crs and extent as our original high resolution rast, then fill the empty raster with our predictions from the model, we use a min function but actually it should not summarise at all. Then we can stack the true and the predicted rasters together to visualise the difference.\n\nHighRes_sf&lt;-st_as_sf(HighRes_df,coords=c(\"x\",\"y\"))\n\n\nHighRes_Pred_raster &lt;- rast(resolution = res(Grouped_Classification),\n                               crs = crs(Grouped_Classification),\n                               ext = ext(Grouped_Classification)) %&gt;% \n  rasterize(vect(HighRes_sf), ., field = \"Pred\", fun = \"min\")%&gt;% \n  tidyterra::mutate(Pred_min=case_when(\n    Pred_min==0~\"Farmland\",\n    Pred_min==1~\"Ocean/Water Body\",\n    Pred_min==2~\"Urban\",\n    Pred_min==3~\"Wild Vegetation\"))\n\n\nHighRes_True_Pred_raster&lt;-c(Grouped_Classification,\n                            HighRes_Pred_raster)\n\nnames(HighRes_True_Pred_raster)&lt;-c(\"'True'\",\"Prediction\")\n\nggplot()+\n  geom_spatraster(data=HighRes_True_Pred_raster,\n                  maxcell = 5e+7\n                  )+\n  labs(title=\"São Miguel: Landcover Classification\")+\n  facet_wrap(~lyr)+\n  scale_fill_manual(name=\"\",\n                    values = c(\"#e86a28\",\n                               \"#008B8B\", \n                               \"#fde825\", \n                               \"#9fcb41\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nActually not a very good prediction, but that is not surprising given the model that was used and the little input data used. We will go through some remote sensing tutorials and then we can apply Machine Learning methods alongside spatial data skills to create some prediction models that will hopefully do a lot better.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Complex GIS in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ForLoops.html",
    "href": "IntermediateRTutorials/ForLoops.html",
    "title": "Loops in R",
    "section": "",
    "text": "Often in R (or other programming languages) we want to repeat a task many times. There are a few options we might use to carry out the same task multiple times: copy and paste the code with minor changes, write a loop that repeats the task for us, map (not the GIS sense but is programming language to alter one thing by another) a function across a list or write a whole function to carry out our task them map that function across our list. Generally, we will copy and paste code when it is only a couple times we repeat the task, map a function over a list when the function to be applied is simple, loop when it is a complex task we want to repeat many times and write a bespoke function when it is a complex process we will carry out many times and we will carry out this process often in time generally.\n\n\n\nSo we want to carry out a task multiple times with a slight difference and we don’t want to copy and paste the code lots of times. For example reading in many csv files but there is a different one for each day of a month or we have a 10 locations and want to create and save a map (GIS sense) for each but all in the same style. To achieve this we can use loops. Many of the examples for loops could also be achieved with the apply functions or map functions but each repeats some task with a rule of systematically changing some element of the task each time. Depending on the task or personal preference will dictate which to use. We will first talk about For loops.\nWe set up a for loop by saying:\nfor (variable in sequence){expression}.\nThis is potentially easier to understand in code. We will tell r that for every number from 1 to 7 we want it to calculate and print the square of this value.\nThe curly brackets aren’t needed when the expression is on the same line, but for easier reading we will normally use curly brackets and have on multiple lines.\n\nfor (i in 1:7){\n  \n  x&lt;-i*i\n  \n  print(paste0(i, \" Squared is equal to \", x))\n  \n}\n\n[1] \"1 Squared is equal to 1\"\n[1] \"2 Squared is equal to 4\"\n[1] \"3 Squared is equal to 9\"\n[1] \"4 Squared is equal to 16\"\n[1] \"5 Squared is equal to 25\"\n[1] \"6 Squared is equal to 36\"\n[1] \"7 Squared is equal to 49\"\n\n\nSo we can see that the code on its own is first creating some object called x that stores the square of i and then we print a statement saying “i squared is equal to x”. For each loop we replace i with the values 1 to 7 in ascending order. We could change this to be random values that we might want. i is often the first replacement used but it could be anything (letter or phrase).\n\nfor (ThisNumber in c(10,2,33,8,152)){\n  \n  SquareNumber&lt;-ThisNumber*ThisNumber\n  \n  print(paste0(SquareNumber ,\" is \",ThisNumber, \" squared\"))\n  \n}\n\n[1] \"100 is 10 squared\"\n[1] \"4 is 2 squared\"\n[1] \"1089 is 33 squared\"\n[1] \"64 is 8 squared\"\n[1] \"23104 is 152 squared\"\n\n\n\n\n\nWithin our for loop me might want to have some logical statement that means we apply a different process depending on the sequence value. For example, we might want to print the square of all numbers up to 10 and then print just the raw value after that. To do this we can create an if else statement. Where we have an if statement, which when it is true we do one then, else we do something else.\n\nfor (j in 1:20){\n  \n  if (j&lt;11){\n    \n    print(paste0(j*j ,\" is \",j, \" squared\"))\n    \n  }else {\n    \n    print(paste0(j ,\" is not squared\"))\n    \n    }\n  \n}\n\n[1] \"1 is 1 squared\"\n[1] \"4 is 2 squared\"\n[1] \"9 is 3 squared\"\n[1] \"16 is 4 squared\"\n[1] \"25 is 5 squared\"\n[1] \"36 is 6 squared\"\n[1] \"49 is 7 squared\"\n[1] \"64 is 8 squared\"\n[1] \"81 is 9 squared\"\n[1] \"100 is 10 squared\"\n[1] \"11 is not squared\"\n[1] \"12 is not squared\"\n[1] \"13 is not squared\"\n[1] \"14 is not squared\"\n[1] \"15 is not squared\"\n[1] \"16 is not squared\"\n[1] \"17 is not squared\"\n[1] \"18 is not squared\"\n[1] \"19 is not squared\"\n[1] \"20 is not squared\"\n\n\n\n\n\nFor some more complex analyses we might want to save a df for each loop that we carry out. One easy way to do this is save each df as an element of a list object. To do this we create an empty list object. Lets create a random data column with some other groupings. Lets create random data that has a mean the same as the iteration of the for loop and then we can try find the mean of a specific iteration to see that it has a close mean to the iteration value.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ stringr   1.5.1\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nList_to_fill&lt;-list()\n\nfor (i in 1:100){\n  \n  df&lt;-data.frame(\n    response = rnorm(300,mean=i,sd=4),\n    group1 = c(\"a\",\"b\",\"c\"),\n    group2 = c(\"d\",\"e\",\"f\")\n  )\n  \n  List_to_fill[[i]]&lt;-df\n  \n}\n\ndf_25&lt;-List_to_fill[[25]]\n\nmean(df_25$response)\n\n[1] 24.90474\n\n\n\n\n\nSo we might, because we just created it, a list of elements that we want to apply a loop over. So lets print all mean values that are tens (10, 20, 30, etc) and plot the data of the 50th iteration. We know there are 100 dfs in our list but we don’t need to to set the for loop going. We will use the %in% sign to say “is one of” then create a vector of all possible tens between 10 and the length of our list. For some other examples later on we will also save every 10 dataframe as a csv.\nYou will notice we use an if without and else. R will just carry out the true statements and do nothing for the else statements. To plot a graph from within a for loop we have to explicitly use the print() function. We want our computer to know that Datafrme_100 is our last df and not our second one so we will add leading zeros to make sure the ordering works.\n\nTenSequence&lt;-seq(10,round(length(List_to_fill)),by=10)\n\nTenSequence\n\n [1]  10  20  30  40  50  60  70  80  90 100\n\nNewList&lt;-list()\n\nfor ( i in 1:length(List_to_fill)){\n  \n  if( i %in% TenSequence){\n    Mean_Response&lt;-mean(List_to_fill[[i]]$response)\n    print(Mean_Response)\n    write_csv(List_to_fill[[i]],paste0(\"loop_data/Dataframe_\",str_pad(i,3,pad=\"0\"),\".csv\"))\n  }\n  \n  if( i == length(List_to_fill)){\n    \n    df_100&lt;-List_to_fill[[i]]\n    \n    print(\n      ggplot(df_100)+\n      geom_density(aes(x=response,colour=group1))+\n      theme_classic()+\n      labs(x=paste0(\"Response variable for i = \", i),\n         y=paste0(\"Density distribution of Response fr i = \", i))\n      )\n    \n  }\n  \n  NewList[[i]]&lt;-List_to_fill[[i]]$response\n  \n}\n\n[1] 10.36748\n[1] 19.93955\n[1] 30.4026\n[1] 40.13306\n[1] 49.78063\n[1] 60.10767\n[1] 69.72728\n[1] 79.94347\n[1] 90.00314\n[1] 99.8032\n\n\n\n\n\n\n\n\n\n\n\n\nSo we can create a new list where we have applied (mapped) a function to every element of the list object we have. This is very similar to for looping, but imaging if we create a list of directory locations for all csv files we want to read into r, we can then apply read_csv() across this list of directories and read in all dfs into a new list object. Lets find out the max value in each of our random distributions, remember the means increased and the standard error stayed consistent. Then lets read back all the csv files we save earlier. The first element in map is the list of directories of files, the second is the function to apply to this list, then we can add arguments to this function as a third argument, for example the way to stop printing column information with read_csv by putting col_types=cols().\n\nMaxValues&lt;-map_dbl(NewList,max)\n\nggplot(mapping=aes(x=1:length(MaxValues),y=MaxValues))+\n  geom_point()+\n  theme_classic()\n\n\n\n\n\n\n\nVector_of_File_Locations&lt;-list.files(\"loop_data/\",full.names = T)\n\nList_of_dfs&lt;-map(Vector_of_File_Locations,\n                 read_csv, \n                 col_types = cols())\n\n\nhead(List_of_dfs[[1]])\n\n# A tibble: 6 × 3\n  response group1 group2\n     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n1     7.27 a      d     \n2     9.02 b      e     \n3    11.5  c      f     \n4     7.51 a      d     \n5    13.2  b      e     \n6    13.0  c      f     \n\n\nSo we now have a list of dataframes, maybe we would want to combine them but keep the information of which iteration they originally came from. To do this we can apply mutate to add a new column and then add them together with bind_rows(). Being part of the tidyverse we can start to use pipes to lay out our code more nicely. As we want to include information about how far along the list we are we will use imap before using map_df to bind the dfs together and return a df.\n\nDF&lt;-List_of_dfs %&gt;% \n  imap(~.x %&gt;% mutate(ID = as.factor(.y))) %&gt;% \n  map_df(bind_rows)\n  \nggplot(DF)+\n  geom_density(aes(x=response,colour=ID,fill=ID),\n               alpha=0.7)+\n  scale_colour_viridis_d()+\n  scale_fill_viridis_d()+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nSo we can apply already made functions to lists/vectors using map but maybe what we want to do is more complex than a couple functions but we will be doing this really often so we dont want to write a for loop every time we want to do it. This is where we can create out our function that we will save in our global environment and use when we want.\nThe layout of function writing is creating a name, this should not be the same as other functions from packages we will use, define what inputs the function will take (arguments) then code to show how the inputs will be processed.\nFunctionName &lt;- function(argument1,argument2,…){The process we want our function to do}\nAt its simplest it could be taking a vector and providing the mean. For Example:\n\nNewMean&lt;- function(vector){\n  \n  sum_of_vector&lt;-sum(vector)\n  N_vector&lt;-length(vector)\n  Average &lt;- sum_of_vector/N_vector\n  return(Average)\n  \n}\n\nVector_of_Numbers&lt;-c(2,3,6,4,2,2,4,455,6,777,8,8,9,5)\n\nNewMean(Vector_of_Numbers)\n\n[1] 92.21429\n\n\nSo now we have our bespoke function we can map it over a list of vectors to give a mean value for each vector in the list!\n\nmap_dbl(NewList,NewMean)\n\n  [1]  0.5214515  2.0179997  2.7349912  4.0123729  4.9755988  6.0041771\n  [7]  6.8691304  8.2631078  8.8711523 10.3674789 11.0093459 11.8649303\n [13] 13.0470201 13.8633256 14.9915393 15.9014576 16.9110091 18.1242437\n [19] 19.1040508 19.9395497 20.7747025 22.4479201 23.1566201 23.8133111\n [25] 24.9047370 25.7758256 26.8396820 28.2881186 29.1104265 30.4026036\n [31] 30.8958319 31.8929704 33.1412271 34.2243564 34.7783360 35.7962329\n [37] 36.4030056 37.8922454 39.0394889 40.1330569 41.1846061 42.4162350\n [43] 42.6296000 43.6898546 44.9472961 45.4498889 46.8415029 47.6696720\n [49] 49.2711117 49.7806280 50.7993460 52.2251594 52.8410748 53.6879532\n [55] 55.0303707 56.3160863 57.0412353 57.9503410 59.3288498 60.1076732\n [61] 60.7913924 62.6105341 62.8846831 63.7208229 64.9659304 65.8981619\n [67] 67.2200312 67.8072290 69.1879936 69.7272831 71.0474479 72.0917500\n [73] 73.0701974 73.8670266 74.6634460 76.1995099 76.8326152 77.9235796\n [79] 79.0114163 79.9434696 81.1518071 82.0465290 82.7513367 84.0554713\n [85] 84.6188140 85.8771705 86.6407995 88.4460343 88.6255059 90.0031396\n [91] 90.9752420 92.1510657 93.0900721 94.1502352 95.2476322 95.8953304\n [97] 96.6317660 97.8135420 98.7293428 99.8031986\n\n\nThis is quite simplistic but it could be quite a complex function, for example some way of plotting distributions. As column selection in ggplot2 is not using characters like other packages, a double curly brackets syntax is used to show that it is the name of a column inside the data and not another dataframe.\n\nDensity_Plot_New&lt;-function(data,response_colname,group_colname){\n  \n  ggplot(data)+\n    geom_density(aes(x={{response_colname}},\n                     fill={{group_colname}},\n                     colour={{group_colname}}),\n               alpha=0.7)+\n    scale_colour_viridis_d()+\n    scale_fill_viridis_d()+\n    coord_cartesian(xlim=c(-10,160))+\n    theme_classic()\n  \n}\n\nList_of_dfs %&gt;% \n  imap(~.x %&gt;% mutate(ID = as.factor(.y),\n                      response = case_when(group1==\"a\"~response*0.5,\n                                           group1==\"b\"~response,\n                                           group1==\"c\"~response*1.5))) %&gt;% \n  map(Density_Plot_New,response_colname=response,group_colname=group1)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Loops in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ForLoops.html#repeating-tasks",
    "href": "IntermediateRTutorials/ForLoops.html#repeating-tasks",
    "title": "Loops in R",
    "section": "",
    "text": "Often in R (or other programming languages) we want to repeat a task many times. There are a few options we might use to carry out the same task multiple times: copy and paste the code with minor changes, write a loop that repeats the task for us, map (not the GIS sense but is programming language to alter one thing by another) a function across a list or write a whole function to carry out our task them map that function across our list. Generally, we will copy and paste code when it is only a couple times we repeat the task, map a function over a list when the function to be applied is simple, loop when it is a complex task we want to repeat many times and write a bespoke function when it is a complex process we will carry out many times and we will carry out this process often in time generally.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Loops in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ForLoops.html#loops",
    "href": "IntermediateRTutorials/ForLoops.html#loops",
    "title": "Loops in R",
    "section": "",
    "text": "So we want to carry out a task multiple times with a slight difference and we don’t want to copy and paste the code lots of times. For example reading in many csv files but there is a different one for each day of a month or we have a 10 locations and want to create and save a map (GIS sense) for each but all in the same style. To achieve this we can use loops. Many of the examples for loops could also be achieved with the apply functions or map functions but each repeats some task with a rule of systematically changing some element of the task each time. Depending on the task or personal preference will dictate which to use. We will first talk about For loops.\nWe set up a for loop by saying:\nfor (variable in sequence){expression}.\nThis is potentially easier to understand in code. We will tell r that for every number from 1 to 7 we want it to calculate and print the square of this value.\nThe curly brackets aren’t needed when the expression is on the same line, but for easier reading we will normally use curly brackets and have on multiple lines.\n\nfor (i in 1:7){\n  \n  x&lt;-i*i\n  \n  print(paste0(i, \" Squared is equal to \", x))\n  \n}\n\n[1] \"1 Squared is equal to 1\"\n[1] \"2 Squared is equal to 4\"\n[1] \"3 Squared is equal to 9\"\n[1] \"4 Squared is equal to 16\"\n[1] \"5 Squared is equal to 25\"\n[1] \"6 Squared is equal to 36\"\n[1] \"7 Squared is equal to 49\"\n\n\nSo we can see that the code on its own is first creating some object called x that stores the square of i and then we print a statement saying “i squared is equal to x”. For each loop we replace i with the values 1 to 7 in ascending order. We could change this to be random values that we might want. i is often the first replacement used but it could be anything (letter or phrase).\n\nfor (ThisNumber in c(10,2,33,8,152)){\n  \n  SquareNumber&lt;-ThisNumber*ThisNumber\n  \n  print(paste0(SquareNumber ,\" is \",ThisNumber, \" squared\"))\n  \n}\n\n[1] \"100 is 10 squared\"\n[1] \"4 is 2 squared\"\n[1] \"1089 is 33 squared\"\n[1] \"64 is 8 squared\"\n[1] \"23104 is 152 squared\"",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Loops in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ForLoops.html#if-else",
    "href": "IntermediateRTutorials/ForLoops.html#if-else",
    "title": "Loops in R",
    "section": "",
    "text": "Within our for loop me might want to have some logical statement that means we apply a different process depending on the sequence value. For example, we might want to print the square of all numbers up to 10 and then print just the raw value after that. To do this we can create an if else statement. Where we have an if statement, which when it is true we do one then, else we do something else.\n\nfor (j in 1:20){\n  \n  if (j&lt;11){\n    \n    print(paste0(j*j ,\" is \",j, \" squared\"))\n    \n  }else {\n    \n    print(paste0(j ,\" is not squared\"))\n    \n    }\n  \n}\n\n[1] \"1 is 1 squared\"\n[1] \"4 is 2 squared\"\n[1] \"9 is 3 squared\"\n[1] \"16 is 4 squared\"\n[1] \"25 is 5 squared\"\n[1] \"36 is 6 squared\"\n[1] \"49 is 7 squared\"\n[1] \"64 is 8 squared\"\n[1] \"81 is 9 squared\"\n[1] \"100 is 10 squared\"\n[1] \"11 is not squared\"\n[1] \"12 is not squared\"\n[1] \"13 is not squared\"\n[1] \"14 is not squared\"\n[1] \"15 is not squared\"\n[1] \"16 is not squared\"\n[1] \"17 is not squared\"\n[1] \"18 is not squared\"\n[1] \"19 is not squared\"\n[1] \"20 is not squared\"",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Loops in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ForLoops.html#saving-objects-from-loops",
    "href": "IntermediateRTutorials/ForLoops.html#saving-objects-from-loops",
    "title": "Loops in R",
    "section": "",
    "text": "For some more complex analyses we might want to save a df for each loop that we carry out. One easy way to do this is save each df as an element of a list object. To do this we create an empty list object. Lets create a random data column with some other groupings. Lets create random data that has a mean the same as the iteration of the for loop and then we can try find the mean of a specific iteration to see that it has a close mean to the iteration value.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ stringr   1.5.1\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nList_to_fill&lt;-list()\n\nfor (i in 1:100){\n  \n  df&lt;-data.frame(\n    response = rnorm(300,mean=i,sd=4),\n    group1 = c(\"a\",\"b\",\"c\"),\n    group2 = c(\"d\",\"e\",\"f\")\n  )\n  \n  List_to_fill[[i]]&lt;-df\n  \n}\n\ndf_25&lt;-List_to_fill[[25]]\n\nmean(df_25$response)\n\n[1] 24.90474",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Loops in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ForLoops.html#looping-over-elements-of-a-list",
    "href": "IntermediateRTutorials/ForLoops.html#looping-over-elements-of-a-list",
    "title": "Loops in R",
    "section": "",
    "text": "So we might, because we just created it, a list of elements that we want to apply a loop over. So lets print all mean values that are tens (10, 20, 30, etc) and plot the data of the 50th iteration. We know there are 100 dfs in our list but we don’t need to to set the for loop going. We will use the %in% sign to say “is one of” then create a vector of all possible tens between 10 and the length of our list. For some other examples later on we will also save every 10 dataframe as a csv.\nYou will notice we use an if without and else. R will just carry out the true statements and do nothing for the else statements. To plot a graph from within a for loop we have to explicitly use the print() function. We want our computer to know that Datafrme_100 is our last df and not our second one so we will add leading zeros to make sure the ordering works.\n\nTenSequence&lt;-seq(10,round(length(List_to_fill)),by=10)\n\nTenSequence\n\n [1]  10  20  30  40  50  60  70  80  90 100\n\nNewList&lt;-list()\n\nfor ( i in 1:length(List_to_fill)){\n  \n  if( i %in% TenSequence){\n    Mean_Response&lt;-mean(List_to_fill[[i]]$response)\n    print(Mean_Response)\n    write_csv(List_to_fill[[i]],paste0(\"loop_data/Dataframe_\",str_pad(i,3,pad=\"0\"),\".csv\"))\n  }\n  \n  if( i == length(List_to_fill)){\n    \n    df_100&lt;-List_to_fill[[i]]\n    \n    print(\n      ggplot(df_100)+\n      geom_density(aes(x=response,colour=group1))+\n      theme_classic()+\n      labs(x=paste0(\"Response variable for i = \", i),\n         y=paste0(\"Density distribution of Response fr i = \", i))\n      )\n    \n  }\n  \n  NewList[[i]]&lt;-List_to_fill[[i]]$response\n  \n}\n\n[1] 10.36748\n[1] 19.93955\n[1] 30.4026\n[1] 40.13306\n[1] 49.78063\n[1] 60.10767\n[1] 69.72728\n[1] 79.94347\n[1] 90.00314\n[1] 99.8032",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Loops in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ForLoops.html#map",
    "href": "IntermediateRTutorials/ForLoops.html#map",
    "title": "Loops in R",
    "section": "",
    "text": "So we can create a new list where we have applied (mapped) a function to every element of the list object we have. This is very similar to for looping, but imaging if we create a list of directory locations for all csv files we want to read into r, we can then apply read_csv() across this list of directories and read in all dfs into a new list object. Lets find out the max value in each of our random distributions, remember the means increased and the standard error stayed consistent. Then lets read back all the csv files we save earlier. The first element in map is the list of directories of files, the second is the function to apply to this list, then we can add arguments to this function as a third argument, for example the way to stop printing column information with read_csv by putting col_types=cols().\n\nMaxValues&lt;-map_dbl(NewList,max)\n\nggplot(mapping=aes(x=1:length(MaxValues),y=MaxValues))+\n  geom_point()+\n  theme_classic()\n\n\n\n\n\n\n\nVector_of_File_Locations&lt;-list.files(\"loop_data/\",full.names = T)\n\nList_of_dfs&lt;-map(Vector_of_File_Locations,\n                 read_csv, \n                 col_types = cols())\n\n\nhead(List_of_dfs[[1]])\n\n# A tibble: 6 × 3\n  response group1 group2\n     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n1     7.27 a      d     \n2     9.02 b      e     \n3    11.5  c      f     \n4     7.51 a      d     \n5    13.2  b      e     \n6    13.0  c      f     \n\n\nSo we now have a list of dataframes, maybe we would want to combine them but keep the information of which iteration they originally came from. To do this we can apply mutate to add a new column and then add them together with bind_rows(). Being part of the tidyverse we can start to use pipes to lay out our code more nicely. As we want to include information about how far along the list we are we will use imap before using map_df to bind the dfs together and return a df.\n\nDF&lt;-List_of_dfs %&gt;% \n  imap(~.x %&gt;% mutate(ID = as.factor(.y))) %&gt;% \n  map_df(bind_rows)\n  \nggplot(DF)+\n  geom_density(aes(x=response,colour=ID,fill=ID),\n               alpha=0.7)+\n  scale_colour_viridis_d()+\n  scale_fill_viridis_d()+\n  theme_classic()",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Loops in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/ForLoops.html#function-writing",
    "href": "IntermediateRTutorials/ForLoops.html#function-writing",
    "title": "Loops in R",
    "section": "",
    "text": "So we can apply already made functions to lists/vectors using map but maybe what we want to do is more complex than a couple functions but we will be doing this really often so we dont want to write a for loop every time we want to do it. This is where we can create out our function that we will save in our global environment and use when we want.\nThe layout of function writing is creating a name, this should not be the same as other functions from packages we will use, define what inputs the function will take (arguments) then code to show how the inputs will be processed.\nFunctionName &lt;- function(argument1,argument2,…){The process we want our function to do}\nAt its simplest it could be taking a vector and providing the mean. For Example:\n\nNewMean&lt;- function(vector){\n  \n  sum_of_vector&lt;-sum(vector)\n  N_vector&lt;-length(vector)\n  Average &lt;- sum_of_vector/N_vector\n  return(Average)\n  \n}\n\nVector_of_Numbers&lt;-c(2,3,6,4,2,2,4,455,6,777,8,8,9,5)\n\nNewMean(Vector_of_Numbers)\n\n[1] 92.21429\n\n\nSo now we have our bespoke function we can map it over a list of vectors to give a mean value for each vector in the list!\n\nmap_dbl(NewList,NewMean)\n\n  [1]  0.5214515  2.0179997  2.7349912  4.0123729  4.9755988  6.0041771\n  [7]  6.8691304  8.2631078  8.8711523 10.3674789 11.0093459 11.8649303\n [13] 13.0470201 13.8633256 14.9915393 15.9014576 16.9110091 18.1242437\n [19] 19.1040508 19.9395497 20.7747025 22.4479201 23.1566201 23.8133111\n [25] 24.9047370 25.7758256 26.8396820 28.2881186 29.1104265 30.4026036\n [31] 30.8958319 31.8929704 33.1412271 34.2243564 34.7783360 35.7962329\n [37] 36.4030056 37.8922454 39.0394889 40.1330569 41.1846061 42.4162350\n [43] 42.6296000 43.6898546 44.9472961 45.4498889 46.8415029 47.6696720\n [49] 49.2711117 49.7806280 50.7993460 52.2251594 52.8410748 53.6879532\n [55] 55.0303707 56.3160863 57.0412353 57.9503410 59.3288498 60.1076732\n [61] 60.7913924 62.6105341 62.8846831 63.7208229 64.9659304 65.8981619\n [67] 67.2200312 67.8072290 69.1879936 69.7272831 71.0474479 72.0917500\n [73] 73.0701974 73.8670266 74.6634460 76.1995099 76.8326152 77.9235796\n [79] 79.0114163 79.9434696 81.1518071 82.0465290 82.7513367 84.0554713\n [85] 84.6188140 85.8771705 86.6407995 88.4460343 88.6255059 90.0031396\n [91] 90.9752420 92.1510657 93.0900721 94.1502352 95.2476322 95.8953304\n [97] 96.6317660 97.8135420 98.7293428 99.8031986\n\n\nThis is quite simplistic but it could be quite a complex function, for example some way of plotting distributions. As column selection in ggplot2 is not using characters like other packages, a double curly brackets syntax is used to show that it is the name of a column inside the data and not another dataframe.\n\nDensity_Plot_New&lt;-function(data,response_colname,group_colname){\n  \n  ggplot(data)+\n    geom_density(aes(x={{response_colname}},\n                     fill={{group_colname}},\n                     colour={{group_colname}}),\n               alpha=0.7)+\n    scale_colour_viridis_d()+\n    scale_fill_viridis_d()+\n    coord_cartesian(xlim=c(-10,160))+\n    theme_classic()\n  \n}\n\nList_of_dfs %&gt;% \n  imap(~.x %&gt;% mutate(ID = as.factor(.y),\n                      response = case_when(group1==\"a\"~response*0.5,\n                                           group1==\"b\"~response,\n                                           group1==\"c\"~response*1.5))) %&gt;% \n  map(Density_Plot_New,response_colname=response,group_colname=group1)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Loops in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/MakingMaps.html",
    "href": "IntermediateRTutorials/MakingMaps.html",
    "title": "Making Static Maps in R",
    "section": "",
    "text": "Okay so I have strong and potentially controversial opinions on using GIS software. While very useful, user friendly tools, just like Excel, I strongly argue against their use for scientific analysis (I have caveats to this). The main pillars of science, in my opinion, are increasing societies knowledge in an open, repeatable, replicate-able and critique-able way. Therefore, using software that is paid for or not open access is a big no for me. Likewise, software that is just lots of clicking buttons, without a full transcript of the process being carried out, doesn’t allow full repeatablility. This may be repetition by other researchers to check your results, emulate your analysis method on another dataset or it may be repetition by future you or me! Another major element of using a coding language to carry out tasks is scaleability. If I make a map in a GIS software, to make the same map with a few changes will always take a similar amount of time, doing so in r (or another coding language) will decrease the time per map through looping or functional programming. I am also a lazy scientist, I can use R to carry out all elements of my research, so why learn another program? (I do use other languages from time to time actually but still)\n\n\n\n\n\nSpatial data is basically the same as any other data type but it has some information about its location, and generally it is stored in two major types: Raster or Shape. Raster data are gridded with repeating units or pixels, with each pixel having x and y coordinates and some data value(s). Shape data are generally types of irregular polygons or points.\nThankfully for us we can use ggplot2 and its functionality to plot a wide range of shapes and rasters, as well as combinations of them.\n\n\n\n\n\n\n\nSometimes this is all we want and can be many different types: satellite image, coastline, road map. Generally, it gives us the spatial context for other elements we want to display.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondly we want the elements we are going to display inside our geographical context, this could be bathymetry, sample sites, labels etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we want to add other info elements like scale bars, north arrows, data sources and generally improve the appearance.\n\n\n\n\n\n\n\n\n\n\nWith ggplot we can layer all these elements on top of each other in the order we want.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Static Maps in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/MakingMaps.html#step-1---background-map-1",
    "href": "IntermediateRTutorials/MakingMaps.html#step-1---background-map-1",
    "title": "Making Static Maps in R",
    "section": "Step 1 - Background Map",
    "text": "Step 1 - Background Map\nHere we need the data itself, then we want to plot it in a projection that makes sense for our use. Atlantic centric? Pacific centric? But depending on some projections we may need to do some extra work for it to look nice.\n\nData Download\nOften we will have our own shape or raster files locally, but there are also a wide range of easily accessible data for different forms of data.\nIn this tutorial we will be using readily available open access data, specifically the rnaturalearth shape files of countries to begin with.\nTo organise and edit we will use the sf package for shape files and the terra package for rasters. There are many others, such as sp, raster and maptools etc. However, they are being deprecated soon and recommend using sf or terra. There are also many supplements to these packages, when we need them we will install and use them.\n\n\nWorld Coastlines\nThe rnaturalearth packages have preloaded data that we can access for country polygons at different spatial scales, lets look at the middle scale. If we load the basic ne_countries() data set and tell the function we want it as an ‘sf’ object we can plot it using the base plot. However, as there are many columns other than the spatial information the base plot() function plots each separately.\n\nlibrary(tidyverse) \nlibrary(sf) \nlibrary(rnaturalearth) \nlibrary(rnaturalearthdata) \n#remotes::install_github(\"ropensci/rnaturalearthhires\")\nlibrary(rnaturalearthhires) \n\nworld_map &lt;- ne_countries(returnclass = \"sf\",scale = 50) \n\nplot(world_map)\n\nWarning: plotting the first 9 out of 63 attributes; use max.plot = 63 to plot\nall\n\n\n\n\n\n\n\n\n\nHere we have a world_map object in our global environment, and generally it seems just like a normal dataframe, with columns and rows of data. An sf object has extra information in a column called geometry. This is information on the type of shape (Point, Line or Polygon) for each row in the dataframe. Here we have a row with a polygon or group of polygons for each country, alongside lots of info on its administration and population etc. As a dataframe we can easily inspect the data we have and use the same techniques of data manipulation we have used before such as mutating, selecting, summarising, grouping etc.\nLets take a glance at the top of the first 6 columns:\n\nhead(world_map[c(1:6)])\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -70.06611 ymin: -18.01973 xmax: 74.89131 ymax: 60.40581\nGeodetic CRS:  +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n  scalerank      featurecla labelrank     sovereignt sov_a3 adm0_dif\n0         3 Admin-0 country         5    Netherlands    NL1        1\n1         1 Admin-0 country         3    Afghanistan    AFG        0\n2         1 Admin-0 country         3         Angola    AGO        0\n3         1 Admin-0 country         6 United Kingdom    GB1        1\n4         1 Admin-0 country         6        Albania    ALB        0\n5         3 Admin-0 country         6        Finland    FI1        1\n                        geometry\n0 MULTIPOLYGON (((-69.89912 1...\n1 MULTIPOLYGON (((74.89131 37...\n2 MULTIPOLYGON (((14.19082 -5...\n3 MULTIPOLYGON (((-63.00122 1...\n4 MULTIPOLYGON (((20.06396 42...\n5 MULTIPOLYGON (((20.61133 60...\n\n\n\nEnter ggplot2\nLets plot it using ggplot instead. Thankfully the sf package has its own geom (geom_sf()), this means we don’t have to give it too much information for it to do something pretty good.\n\nggplot(world_map)+\n  geom_sf()\n\n\n\n\n\n\n\n\nThere are some obvious issues with this map but generally it is a pretty good starting point.\n\n\n\nCoordinate Reference Systems and Projections\nHere we are looking at a map of a three dimensional sphere projected onto a flat surface. To do this we use different map projections, which can lead to interesting effects by distorting apparent size, such as Antarctica being long and thin at the base of this map, when really it is a circle shape around the south pole. We can easily change between CRS using sf functions. While previously CRS was denoted with quite a long string, now there are easy to use 4 digit EPSG codes. You can find loads here: https://spatialreference.org/ref/epsg/\nEPSG codes relate to preset or defined CRS strings. So we can use either EPSG codes or we can be more specific with a CRS we want to use e.g.: “+proj=laea +lat_0=0 +lon_0=-30 +x_0=43210000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs”. This string states the projection, the centre lat, centre lon, the centre x, centre y, the ellipsoid (type of sphere the globe is assumed to be), the conversion from WGS 84, the units, and other info. For the standard latlong projection that is often used we would have: “+proj=longlat +datum=WGS84 +no_defs”\nFor example here is the same map reprojected to Mercator:\n\nworld_map %&gt;%\nst_transform(crs=3857) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nBut we can see a BIG issue with this projection. It makes greenland look like the size of africa, and it also makes antarctica the size of the sun (not quite)! As it is a ggplot we can crop out the bottom of the graph instead.\n\nworld_map %&gt;%\nst_transform(crs=3857) %&gt;% \nggplot()+\n  geom_sf()+\n  coord_sf(ylim = c(-20000000,NA))+\n  theme_classic()\n\n\n\n\n\n\n\n\nThe mercator uses metres as its units so we have to use those units for setting limits. Lets see some other systems.\n\nworld_map %&gt;%\nst_transform(crs=3978) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\nworld_map %&gt;%\nst_transform(crs=32190) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\nworld_map %&gt;%\nst_transform(crs=26917) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nAs you can see, all of these are horrible. By reprojecting the sf file we are causing the polygons to become glitchy and distorted. It seems some issue is being caused by Antarctica. Perhaps if we remove it it will look better?\n\nworld_map %&gt;%\n  filter(!sovereignt==\"Antarctica\") %&gt;% \nst_transform(crs=3978) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay no it does not.\n\n\nDistortions\nThese projections have inherent edges to their projection (the far left or far right of the plotted map) which will be latitude or longitude values, they will also have the centre position. The areas of the map that are furthest from the centre, around the edges will become distorted when plotting on a 2D plane: think about looking at a 3D globe and trying to see the countries on the other side from you. When transforming and then plotting, a polygon of a country (the shape that country makes) will potentially get divided byt the edge of the projection area. Thus, we get these weird glitchy distortions. There are ways around having these distortions but it involves creating a really thin polygon along the edges of the original map, then deleting this thin polygon. This splits the polygons that will be split when changing the transformation. We can visualise this if we want.\nOriginal Map and transformed into desired CRS.\n\nworld_map %&gt;%\nst_transform(crs=4326) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\nworld_map %&gt;%\nst_transform(crs=st_crs(\"+proj=cea +lon_0=130 +x_0=0 +y_0=0 +lat_ts=45 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")) %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo in our desired projection we can see the the central longitude is 130. We therefore need to make a slim polygon opposite 130 on the globe! (180-130)\n\nworld&lt;-world_map %&gt;%\nst_transform(crs=4326)%&gt;%\n  st_make_valid()\n\n\n# define a long & slim polygon that overlaps the meridian line & set its CRS to match\n# that of world\n\n# Centered in lon 130 on this example\n\noffset &lt;- 180 - 130\n\n\npolygon &lt;- st_polygon(x = list(rbind(\n  c(-0.0001 - offset, 90),\n  c(0 - offset, 90),\n  c(0 - offset, -90),\n  c(-0.0001 - offset, -90),\n  c(-0.0001 - offset, 90)\n))) %&gt;%\n  st_sfc() %&gt;%\n  st_set_crs(4326)\n\nLets plot the slim polygon on the original map.\n\nworld_map %&gt;%\nst_transform(crs=4326) %&gt;% \nggplot()+\n  geom_sf()+\n  geom_sf(data=polygon,fill=\"red\",colour=\"red\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow we have this thin polygon we can remove this slim area from the original world map.\n\nworld_fixed &lt;- world %&gt;% \n  st_difference(polygon)\n\ntarget_crs&lt;-st_crs(\"+proj=cea +lon_0=130 +x_0=0 +y_0=0 +lat_ts=45 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n\nworld_fixed &lt;- world_fixed %&gt;% \n  st_transform(target_crs)\n\n\nggplot(data = world_fixed) +\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nHere there are still some odd looking shapes e.g. Papua New Guinea. Maybe we can find a better projection. Maybe we can use one that looks a little 3D even when in 2D?\n\nworld_map %&gt;% \n  st_transform(\"+proj=laea +lat_0=0 +lon_0=-30 +x_0=43210000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\") %&gt;% \nggplot()+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis projection is a favourite of mine, I used it above for the map of Europe. The choice of mapping projection will be dictated by the scale you want to show and on your own personal preference.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Static Maps in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/MakingMaps.html#step-2---spatially-explicit-details-1",
    "href": "IntermediateRTutorials/MakingMaps.html#step-2---spatially-explicit-details-1",
    "title": "Making Static Maps in R",
    "section": "Step 2 - Spatially Explicit Details",
    "text": "Step 2 - Spatially Explicit Details\nOkay so we have a global map we will use to be our Base Map, it isn’t perfect by any means but it is fine for now. Now lets plot some spatially explicit data. As a Marine Ecologist with a dark Oceanography past, I absolutely LOVE bathymetry plots so lets get some bathymetry and try make a nice Bathymetry plot of the Atlantic. We may have our own bathymetry data, or we could go to some website, download it and read into r, or we can download straight into r using the marmap package. This gets bathymetry data from the NOAA database. (Or GEBCO if you prefer)\nTo do this we use the getNOAA.bathy() and tell the function the longitude limits, then latitude limits, then the resolution. If our desired data cross the antimeridian (the longitude line 180 and -180) we can tell the function this too. This function creates a bathy object, but we can convert this to a data frame with as.xyz(). Remember when we set a big area and a fine resolution value the file gets bigger so will take longer to download and to plot. We shall also remove Depth values that are positive so we only get Depth. (This may not be ideal for inland areas that are below 0, such as the eastern coast of the UK).\n\n#install.packages(\"marmap\")\n\nlibrary(marmap)\n\n\n# Get bathymetric data\nbat &lt;- getNOAA.bathy(lon1=-90,lon2=40,lat1=70,lat2=-60, res = 4)\n\nbat_xyz &lt;- as.xyz(bat) %&gt;% \n  rename(Longitude=V1,Latitude=V2,Depth=V3) %&gt;% \n  filter(Depth&lt;1)\n\n\nggplot(bat_xyz)+\n  geom_tile(aes(x=Longitude,y=Latitude,fill=Depth))+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay so now we have some regular gridded bathymetry data but it isn’t the same projection we have our base map in, so if we try plot together it will be rather wrong. Infact on the global scale our bathymetry data which range from -90 to 40 in longitude and 70 to -60 in latitude are not even visible on a map using this projection.\n\nProjection3D&lt;-\"+proj=laea +lat_0=0 +lon_0=-30 +x_0=43210000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\"\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_sf()+\n  geom_tile(data=bat_xyz,aes(x=Longitude,y=Latitude,fill=Depth))+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo what we will do is convert the dataframe to a spatial object. We could use sf to do this. Then we can easily transform it to the projection we want! To do this we can use the st_as_sf() function. One issue is we will need to tell sf what projection our latitude and longitude data are. Thankfully getNOAAA.bathy() uses EPSG:4326.\n\nBathy_sf&lt;-st_as_sf(bat_xyz,coords = c(\"Longitude\",\"Latitude\"),crs=4326) %&gt;%  \n  st_transform(Projection3D)\n  \n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_sf()+\n  geom_sf(data=Bathy_sf,aes(fill=Depth))+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis doesn’t look like it has worked, what is happening is that each row of the sf is thought of as a point, ggplot is then plotting those points. It would be better for speed of plotting and how the plot looks if we use a raster format, as this is how the data are laid out. We can use the terra package to do this.\n\n#install.packages(\"terra\")\n#install.packages(\"tidyterra\")\n\nlibrary(terra)\nlibrary(tidyterra)\n\nBathy_terra&lt;-as_spatraster(bat_xyz,xycols = c(1:2),crs=4326) %&gt;%  \n  project(Projection3D)\n  \n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_sf()+\n  geom_spatraster(data=Bathy_terra,aes(fill=Depth))+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay, this is good, the projection is correct and the detail is enough for our current zoom level. The main issue now seems to be the NA values in the spat raster being plotted in a grey colour. So lets plot this with a nicer colour palette, and maybe we want our map lines to be on top of the bathymetry?\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_spatraster(data=Bathy_terra,aes(fill=Depth))+\n  geom_sf()+\n  scale_fill_viridis_c(na.value = NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nHmmm maybe it would be nicer with a full global bathymetry? Or we could zoom into the area we have bathymetry for? Or both?\n\nbat_whole &lt;- getNOAA.bathy(lon1=-180,lon2=180,lat1=90,lat2=-90, res = 10)\n\nBathy_Whole_World &lt;- as.xyz(bat_whole) %&gt;% \n  rename(Longitude=V1,Latitude=V2,Depth=V3) %&gt;% \n  filter(Depth&lt;1) %&gt;% \n  as_spatraster(xycols = c(1:2),crs=4326) %&gt;%  \n  project(Projection3D)\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_spatraster(data=Bathy_Whole_World,aes(fill=Depth))+\n  geom_sf()+\n  scale_fill_viridis_c(na.value = NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nLets also make some labels of points of interest, and lets zoom in on the mid atlantic ridge because it is very cool! Always have to refer to Marie Tharp for her magnificent, literally world changing work on mapping and effectively discovering this region of the ocean!\nAs our projection works in metres it would be a lot of trial and error to get the correct values, but we can create an sf object with the latitude and longitude we want then transform it and use that to create our limits! I will just look up some islands and archipelagoes and their Latitudes and Longitudes and add them as labels with the ggforce package function geom_mark_ellipse().\n\n#install.packages(\"sfheaders\")\n\nlibrary(sfheaders)\nlibrary(ggforce)\n\n\nCrop_MAR&lt;-data.frame(\n  lon = c(-80,50,-80,50),  \n  lat = c(-45,-45,45,45)\n) %&gt;% \n  st_as_sf(coords=c(\"lon\",\"lat\"), crs = 4326) %&gt;% \n  st_transform(Projection3D) %&gt;% \n  sf_to_df(fill=T)\n\nInterestingPoints&lt;-data.frame(\n  lon = c(-27.862,-14.3737,-15.7315,-16.751,-64.7896,\n          -23.777,-32.425,6.739,-5.705,-12.2821,-37.05,\n          -59.364,51.741,-59.5463),  \n  lat = c(38.723,-7.9481,28.620,32.860,32.352,15.9495,\n          -3.8543,0.4535,-15.9697,-37.115,-54.403,-51.772,\n          -46.4046,13.102),\n  Islands= c(\"Azores\",\"Ascension\",\"Canaries\",\"Madeira\",\n             \"Bermuda\",\"Cabo Verde\",\"Fernando de Noronha\",\n             \"São Tomé and Príncipe\",\"Saint Helena\",\"Tristan da Cunha\",\n             \"South Georgia\",\"Islas Malvinas\",\"Possession\",\"Barbados\")\n) %&gt;% \n  st_as_sf(coords=c(\"lon\",\"lat\"), crs = 4326) %&gt;% \n  st_transform(Projection3D) %&gt;% \n  sf_to_df(fill=T)\n\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_spatraster(data=Bathy_Whole_World,aes(fill=Depth))+\n  geom_sf()+\ngeom_mark_ellipse(data=InterestingPoints,\n               aes(x=x,\n                   y=y,\n                   label = Islands,\n                   group=Islands))+\n  scale_fill_viridis_c(na.value = NA)+\n  coord_sf(xlim=c(min(Crop_MAR$x),max(Crop_MAR$x)),\n           ylim=c(min(Crop_MAR$y),max(Crop_MAR$y)))+\n  theme_classic()",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Static Maps in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/MakingMaps.html#step-3---appearance-1",
    "href": "IntermediateRTutorials/MakingMaps.html#step-3---appearance-1",
    "title": "Making Static Maps in R",
    "section": "Step 3 - Appearance",
    "text": "Step 3 - Appearance\nAppearance is very down to personal preference but is all just using ggplot theme and scale elements. For a map this big i think a scale bar is unneccesary (but we will put a small one in anyway) and in my opinion a North arrow is never useful (except for navigational maps), as having Latitude on the y axis and Longitude on the x axis removes its utility.\nHere I add nicer legend labels. Again it is personal preference but Depth in my opinion should be positive going deeper. I also add a scale bar, although with this projection it will change how big it should be for different areas of the map. I change the colour scheme of the bathymetry to be a blue palette. Then I add Labels for x and y, and a caption of data source. I add some grid lines to show latitude and longitude lines with theme().\n\n#install.packages(\"ggsn\")\n\nlibrary(ggsn)\n\nworld_map %&gt;% \n  st_transform(Projection3D) %&gt;% \nggplot()+\n  geom_spatraster(data=Bathy_Whole_World,aes(fill=Depth), maxcell = 5005560)+\n  geom_sf(linewidth=0.1,alpha=0.9,\n          fill=\"palegreen4\",colour=\"grey30\")+\ngeom_mark_ellipse(data=InterestingPoints,\n               aes(x=x,\n                   y=y,\n                   label = Islands,\n                   group=Islands) ,\n               show.legend=F,\n               alpha=0.8,\n               label.buffer = unit(1, \"mm\"),\n               label.fill = \"grey80\")+\n  scale_fill_gradientn(colours=c(\"#5e24d6\",\"#22496d\",\"#042f66\",\"#054780\",\"#1074a6\",\n                                \"#218eb7\",\"#48b5d2\",\"#72d0e1\",\"#9ddee7\",\"#c6edec\"),\n                       breaks=c(0,-2500,-5000,-7500),\n                       labels=c(\"0\",\"2,500\",\"5,000\",\"7,500\"),\n                       na.value = NA)+\n  coord_sf(xlim=c(min(Crop_MAR$x),max(Crop_MAR$x)),\n           ylim=c(min(Crop_MAR$y),max(Crop_MAR$y)))+\n  scalebar(world_map %&gt;%  st_transform(Projection3D),\n           dist = 1500, dist_unit = \"km\", \n                 transform=FALSE, \n                 st.dist=0.008,\n                 height=0.01,\n                 location=\"bottomright\",\n                 anchor=c(x=min(Crop_MAR$x)*1.07,\n                          y=max(Crop_MAR$y)*0.25),\n                 border.size = 0.8)+\n  labs(x=\"Longitude\",y=\"Latitude\", \n       caption = \"Data Source: National Oceanic and Atmospheric Administration (NOAA)\",\n       title = \"Bathymetry of the Mid-Atlantic Ridge\",fill=\"Depth (m)\")+\n  theme(panel.background = element_blank(), # bg of the panel\n    panel.grid.major = element_line(linetype = \"dotted\",\n                                             colour=\"grey30\",\n                                             linewidth=0.25),\n    panel.ontop = TRUE,\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(colour = \"black\", fill=NA, linewidth=1))",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Static Maps in R"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html",
    "title": "Introduction to GLMMs",
    "section": "",
    "text": "General (or Generalised) Linear Mixed Effects Models (GLMMs) (sometimes called hierarchical models or multi-level models) are an extension on GLMs. They are a method of adding more complex model structure into account, with addition of random effects (sometimes called partial pooling or shrinkage). This means that when our data have some form of structure that isn’t our main question of interest we can tell the model about this structure, thus more correctly estimating our effects of interest (fixed effects). I will use the terms fixed and random effects to follow the nomenclature of the GLM tutorials, but there could be other names such as predictor variables, partial pooling variables etc. For a full FAQ of GLMMs by Ben Bolker go here: https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\n\n\nGLMMs follow an almost identical workflow as GLMs with:\n\n\nStatistical Model Formulation\n\n\nResponse Variable Distribution\n\n\nOrganising Fixed Effects (and Random Effects)\n\n\nAssessing Model Functioning\n\n\nModel Interpretation\n\n\nWe went through these steps excessively in GLM tutorials and for GLMMs the only differences will be the Statistical Model Formulation (1.), Organising Effects (3.) and Model Interpretation (5.). The other steps will stay almost identical and therefore I won’t focus too much on them here (Go back to GLM tutorials for full explanations).\n\n\n\nGenerally, we want to use random effects when we have some sort of grouping of variables (maybe spatial or temporal) that means each of our fixed effects are no longer independent; this might be some repeat measurement over time or perhaps different sub populations over a wider area. In these two examples we would expect values to be related within the same individual over time or within the same sub population. Therefore we need to tell our model that some correlation/variance can be explained by some grouping variable (the individual or sub population). It is worth noting that the name ‘random effect’ does not infer any mathematica randomness of the data within it, it is just the naming convention used. There are reasons behind its naming but they are long and not important for now.\nThere are three popular ways to add random effects into a model: allow varying intercepts, allow varying slopes or allow varying intercepts and slopes across groups. The data we will be using in this tutorial are from another tutorial here: https://ourcodingclub.github.io/tutorials/mixed-models/. So lets go into examples of intercept, slopes and intercept-slope random effects in relation to a dataset. The data we will use are to do with intelligence of dragons, their size in bodylength, which mountain range they come from and which site within that mountain range. Lets download the data into our base directory then load it into r. If this doesn’t work follow the URL and it will download the data for you and you can bring it into r in whatever way you prefer.\n\n\n#install.packages(\"HelpersMG\")\n\nHelpersMG::wget(\"https://github.com/ourcodingclub/CC-Linear-mixed-models/raw/master/dragons.RData\")\n\nload(\"dragons.RData\")\n\nhead(dragons)\n\n  testScore bodyLength mountainRange  X site\n1 16.147309   165.5485      Bavarian NA    a\n2 33.886183   167.5593      Bavarian NA    a\n3  6.038333   165.8830      Bavarian NA    a\n4 18.838821   167.6855      Bavarian NA    a\n5 33.862328   169.9597      Bavarian NA    a\n6 47.043246   168.6887      Bavarian NA    a\n\n\nWe are interested in the effect that bodylength has on test score of dragons. So following our glm tutorials from before we might apply a beta GLM (scores are percentages that include 0 and 100 but we will divide by 100 to be between 0 and 1, we will also apply a very small conversion so the 0s are never fully 0 and 1 never fully 1). Lets ignore model assessment for now and skip straight to interpretation. I will also create a siteUnique column for later on when we do a full model.\n\n# install.packages(\"mgcv\")\nlibrary(tidyverse)\nlibrary(mgcv)\n\ndragons_1&lt;-dragons %&gt;% \n  mutate(testScore=case_when(testScore/100==0~0.0001,\n                             testScore/100==1~0.9999,\n                             TRUE~testScore/100),\n         siteUnique=interaction(mountainRange,site))\n\nglm1 &lt;- gam(testScore ~ bodyLength, data = dragons_1, family = betar(link=\"logit\"))\n\n\nNewData_1&lt;-expand_grid(bodyLength=seq(min(dragons$bodyLength),max(dragons$bodyLength),length.out=50)\n                      )\n\nPred&lt;-predict(glm1,NewData_1,se.fit=T,type=\"response\")\n\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewData)+\n  geom_ribbon(aes(x=bodyLength,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=bodyLength,\n                 y=response))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore))+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay so there seems to be an apparent increase in test score with increasing body length. And even if we check the diagnostic plots this seems fine (not amazing but perhaps acceptable).\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nHowever, if we perhaps plot the different mountain ranges as different colours in our interpretation plot we might see some issues. There seems to be some specific mountain ranges with both lower body length and test scores (Bavarian and Southern) and some that are higher in both (Central). Our model really doesn’t seem to fit the different patterns in our different mountains, or even the sites within those mountains.\n\nggplot(NewData)+\n  geom_ribbon(aes(x=bodyLength,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=bodyLength,\n                 y=response))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore,\n                                colour=mountainRange,\n                                shape=site))+\n  scale_colour_viridis_d()+\nfacet_wrap(~mountainRange)+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\",colour=\"Mountain Range\",shape=\"Site\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo we can see that both mountain range and site within mountain range have some element of grouping. Yet we are not necessarily interested in which mountain range has the strongest effect of bodylength on test score. So we want to add our mountain range (and maybe our site) into our model, but not as fixed effects: here come the random effects.\nFor correctly formulating our statistical model we need to also define how we think our mountain range changes the fixed effects. This is where we chose varying intercept, varying slope or varying intercept and slope. If we expect the overall effect of body length on test score to be equivalent but that some mountain ranges may score higher or lower on tests overall we would chose a varying intercept model. If we think that each mountain has the same background level of testscore ability but that the effect of bodylength will vary (positive effect, negative effect, no effect) depending on the mountain range, we would select a varying slope model, and finally if we think that the effect of bodylength on testscore and the background level of testscore will change with mountain range we would chose a slope and intercept varying model.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#what-are-glmms",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#what-are-glmms",
    "title": "Introduction to GLMMs",
    "section": "",
    "text": "General (or Generalised) Linear Mixed Effects Models (GLMMs) (sometimes called hierarchical models or multi-level models) are an extension on GLMs. They are a method of adding more complex model structure into account, with addition of random effects (sometimes called partial pooling or shrinkage). This means that when our data have some form of structure that isn’t our main question of interest we can tell the model about this structure, thus more correctly estimating our effects of interest (fixed effects). I will use the terms fixed and random effects to follow the nomenclature of the GLM tutorials, but there could be other names such as predictor variables, partial pooling variables etc. For a full FAQ of GLMMs by Ben Bolker go here: https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#general-workflow-of-glmms",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#general-workflow-of-glmms",
    "title": "Introduction to GLMMs",
    "section": "",
    "text": "GLMMs follow an almost identical workflow as GLMs with:\n\n\nStatistical Model Formulation\n\n\nResponse Variable Distribution\n\n\nOrganising Fixed Effects (and Random Effects)\n\n\nAssessing Model Functioning\n\n\nModel Interpretation\n\n\nWe went through these steps excessively in GLM tutorials and for GLMMs the only differences will be the Statistical Model Formulation (1.), Organising Effects (3.) and Model Interpretation (5.). The other steps will stay almost identical and therefore I won’t focus too much on them here (Go back to GLM tutorials for full explanations).",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#when-to-use-a-random-effect",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#when-to-use-a-random-effect",
    "title": "Introduction to GLMMs",
    "section": "",
    "text": "Generally, we want to use random effects when we have some sort of grouping of variables (maybe spatial or temporal) that means each of our fixed effects are no longer independent; this might be some repeat measurement over time or perhaps different sub populations over a wider area. In these two examples we would expect values to be related within the same individual over time or within the same sub population. Therefore we need to tell our model that some correlation/variance can be explained by some grouping variable (the individual or sub population). It is worth noting that the name ‘random effect’ does not infer any mathematica randomness of the data within it, it is just the naming convention used. There are reasons behind its naming but they are long and not important for now.\nThere are three popular ways to add random effects into a model: allow varying intercepts, allow varying slopes or allow varying intercepts and slopes across groups. The data we will be using in this tutorial are from another tutorial here: https://ourcodingclub.github.io/tutorials/mixed-models/. So lets go into examples of intercept, slopes and intercept-slope random effects in relation to a dataset. The data we will use are to do with intelligence of dragons, their size in bodylength, which mountain range they come from and which site within that mountain range. Lets download the data into our base directory then load it into r. If this doesn’t work follow the URL and it will download the data for you and you can bring it into r in whatever way you prefer.\n\n\n#install.packages(\"HelpersMG\")\n\nHelpersMG::wget(\"https://github.com/ourcodingclub/CC-Linear-mixed-models/raw/master/dragons.RData\")\n\nload(\"dragons.RData\")\n\nhead(dragons)\n\n  testScore bodyLength mountainRange  X site\n1 16.147309   165.5485      Bavarian NA    a\n2 33.886183   167.5593      Bavarian NA    a\n3  6.038333   165.8830      Bavarian NA    a\n4 18.838821   167.6855      Bavarian NA    a\n5 33.862328   169.9597      Bavarian NA    a\n6 47.043246   168.6887      Bavarian NA    a\n\n\nWe are interested in the effect that bodylength has on test score of dragons. So following our glm tutorials from before we might apply a beta GLM (scores are percentages that include 0 and 100 but we will divide by 100 to be between 0 and 1, we will also apply a very small conversion so the 0s are never fully 0 and 1 never fully 1). Lets ignore model assessment for now and skip straight to interpretation. I will also create a siteUnique column for later on when we do a full model.\n\n# install.packages(\"mgcv\")\nlibrary(tidyverse)\nlibrary(mgcv)\n\ndragons_1&lt;-dragons %&gt;% \n  mutate(testScore=case_when(testScore/100==0~0.0001,\n                             testScore/100==1~0.9999,\n                             TRUE~testScore/100),\n         siteUnique=interaction(mountainRange,site))\n\nglm1 &lt;- gam(testScore ~ bodyLength, data = dragons_1, family = betar(link=\"logit\"))\n\n\nNewData_1&lt;-expand_grid(bodyLength=seq(min(dragons$bodyLength),max(dragons$bodyLength),length.out=50)\n                      )\n\nPred&lt;-predict(glm1,NewData_1,se.fit=T,type=\"response\")\n\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewData)+\n  geom_ribbon(aes(x=bodyLength,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=bodyLength,\n                 y=response))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore))+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay so there seems to be an apparent increase in test score with increasing body length. And even if we check the diagnostic plots this seems fine (not amazing but perhaps acceptable).\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nHowever, if we perhaps plot the different mountain ranges as different colours in our interpretation plot we might see some issues. There seems to be some specific mountain ranges with both lower body length and test scores (Bavarian and Southern) and some that are higher in both (Central). Our model really doesn’t seem to fit the different patterns in our different mountains, or even the sites within those mountains.\n\nggplot(NewData)+\n  geom_ribbon(aes(x=bodyLength,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=bodyLength,\n                 y=response))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore,\n                                colour=mountainRange,\n                                shape=site))+\n  scale_colour_viridis_d()+\nfacet_wrap(~mountainRange)+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\",colour=\"Mountain Range\",shape=\"Site\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo we can see that both mountain range and site within mountain range have some element of grouping. Yet we are not necessarily interested in which mountain range has the strongest effect of bodylength on test score. So we want to add our mountain range (and maybe our site) into our model, but not as fixed effects: here come the random effects.\nFor correctly formulating our statistical model we need to also define how we think our mountain range changes the fixed effects. This is where we chose varying intercept, varying slope or varying intercept and slope. If we expect the overall effect of body length on test score to be equivalent but that some mountain ranges may score higher or lower on tests overall we would chose a varying intercept model. If we think that each mountain has the same background level of testscore ability but that the effect of bodylength will vary (positive effect, negative effect, no effect) depending on the mountain range, we would select a varying slope model, and finally if we think that the effect of bodylength on testscore and the background level of testscore will change with mountain range we would chose a slope and intercept varying model.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#first-step-of-analysis---statistical-model-formulation",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#first-step-of-analysis---statistical-model-formulation",
    "title": "Introduction to GLMMs",
    "section": "First Step of Analysis - Statistical Model Formulation",
    "text": "First Step of Analysis - Statistical Model Formulation\nOkay, lets say we think the effect of bodylength is consistent accross mountain ranges but the level of test ability is not.\nThis would create a stats model of:\ntestScore ~ bodyLength + (1|mountainRange)\nHere we use the notation of (1|grouping) for our random effect. Everything after the vertical line is related to the intercept and everything before is related to the slope.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#second-step-of-analysis---response-variable-distribution",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#second-step-of-analysis---response-variable-distribution",
    "title": "Introduction to GLMMs",
    "section": "Second Step of Analysis - Response Variable Distribution",
    "text": "Second Step of Analysis - Response Variable Distribution\nAs we said earlier this is the same as GLMs, our response variable is a continuous value between an upper and low bound and therefore is a beta distribution.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#third-step-of-analysis---organising-fixed-effects-and-random-effects",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#third-step-of-analysis---organising-fixed-effects-and-random-effects",
    "title": "Introduction to GLMMs",
    "section": "Third Step of Analysis - Organising Fixed Effects (and Random Effects)",
    "text": "Third Step of Analysis - Organising Fixed Effects (and Random Effects)\nWe could perhaps do some scaling of body size but we will keep things simple with the raw data. We do need to make sure our random variable is classed as a factor but that is already the case so no need to worry.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fourth-step-of-analysis---assessing-model-functioning",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fourth-step-of-analysis---assessing-model-functioning",
    "title": "Introduction to GLMMs",
    "section": "Fourth Step of Analysis - Assessing Model Functioning",
    "text": "Fourth Step of Analysis - Assessing Model Functioning\nSo lets run our new formula as a GLMM and run some simple diagnostic plots as with GLMs. We will use the glmmTMB package to use the glmmTMB() function as it allows the use of beta regression. To do this we install TMB then glmmTMB. The diagnostics are not perfect but again we will accept them for now. Those extreme values are due to the 0s and 1s we converted, for a better model we might want to do some sort of hurdle/zero_one_inflated beta model, for now it should be fine.\n\n#install.packages(\"TMB\", type=\"source\")\n#install.packages(\"glmmTMB\", type=\"source\")\n\nlibrary(glmmTMB)\n\nglmm1&lt;-glmmTMB(testScore ~ bodyLength+ (1|mountainRange), data = dragons_1, family = beta_family)\n\nglmm1ModelOutputs&lt;-data.frame(Fitted=fitted(glmm1),\n                  Residuals=resid(glmm1))\n\np5&lt;-ggplot(glmm1ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np6&lt;-ggplot(glmm1ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fifth-step-of-analysis---model-interpretation",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fifth-step-of-analysis---model-interpretation",
    "title": "Introduction to GLMMs",
    "section": "Fifth Step of Analysis - Model Interpretation",
    "text": "Fifth Step of Analysis - Model Interpretation\nAgain, as with GLMs, we make an ‘empty’ data frame with all the fixed effects values from the original data set, but we also have to include the random effects (mountain range) too. So we are not predicting outside of the range of the data from within each mountain range we will group by mountain range and use data_grid() from the modelr package to create our ‘newdata’. This shows us that with an intercept varying model there is no effect of bodylength on test score.\n\nNewData_2&lt;-dragons_1 %&gt;% \n  group_by(mountainRange) %&gt;% \n  modelr::data_grid(bodyLength)\n\nPred&lt;-predict(glmm1,NewData_2,se.fit=T,type=\"response\")\n\n\nNewDataglmm&lt;-NewData_2 %&gt;% \n  ungroup() %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewDataglmm)+\n  geom_ribbon(aes(x=bodyLength,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=mountainRange,\n                  group=mountainRange),\n              alpha=0.7)+\n  geom_line(aes(x=bodyLength,\n                 y=response,\n                  colour=mountainRange))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore,\n                  colour=mountainRange))+\n  facet_wrap(~mountainRange)+\n    scale_colour_viridis_d()+\n    scale_fill_viridis_d()+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\",\n       fill=\"Mountain Range\",colour=\"Mountain Range\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nBut what if we thought the effect would change but that background testscores are the same across mountain ranges?",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#first-step-of-analysis---statistical-model-formulation-1",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#first-step-of-analysis---statistical-model-formulation-1",
    "title": "Introduction to GLMMs",
    "section": "First Step of Analysis - Statistical Model Formulation",
    "text": "First Step of Analysis - Statistical Model Formulation\nOkay, we think the effect of bodylength isn’t consistent accross mountain ranges but the level of test ability is.\nThis would create a stats model of:\ntestScore ~ bodyLength + (bodyLength|1)\nHere we use the notation of (effectValue|1) for our random effect. Everything before the vertical line is related to the slope and everything after is related to the intercept.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#second-step-of-analysis---response-variable-distribution-1",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#second-step-of-analysis---response-variable-distribution-1",
    "title": "Introduction to GLMMs",
    "section": "Second Step of Analysis - Response Variable Distribution",
    "text": "Second Step of Analysis - Response Variable Distribution\nSee above.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#third-step-of-analysis---organising-fixed-effects-and-random-effects-1",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#third-step-of-analysis---organising-fixed-effects-and-random-effects-1",
    "title": "Introduction to GLMMs",
    "section": "Third Step of Analysis - Organising Fixed Effects (and Random Effects)",
    "text": "Third Step of Analysis - Organising Fixed Effects (and Random Effects)\nSee above.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fourth-step-of-analysis---assessing-model-functioning-1",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fourth-step-of-analysis---assessing-model-functioning-1",
    "title": "Introduction to GLMMs",
    "section": "Fourth Step of Analysis - Assessing Model Functioning",
    "text": "Fourth Step of Analysis - Assessing Model Functioning\nSo lets run our new formula as a GLMM and run some simple diagnostic plots as with GLMs. We will use the glmmTMB package to use the glmmTMB() function as it allows the use of beta regression. To do this we install TMB then glmmTMB. The diagnostics are a lot better now but still not perfect.\n\nglmm2&lt;-glmmTMB(testScore ~ bodyLength+ (bodyLength|1), data = dragons_1, family = beta_family)\n\nglmm2ModelOutputs&lt;-data.frame(Fitted=fitted(glmm2),\n                  Residuals=resid(glmm2))\n\np5&lt;-ggplot(glmm2ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np6&lt;-ggplot(glmm2ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np5+p6",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fifth-step-of-analysis---model-interpretation-1",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fifth-step-of-analysis---model-interpretation-1",
    "title": "Introduction to GLMMs",
    "section": "Fifth Step of Analysis - Model Interpretation",
    "text": "Fifth Step of Analysis - Model Interpretation\nWe use the same ‘newdata’ but use the slope varying model to predict. This shows us that with a slope varying model there is an effect of bodylength on test score.\n\nPred_2&lt;-predict(glmm2,NewData_2,se.fit=T,type=\"response\")\n\n\nNewDataglmm2&lt;-NewData_2 %&gt;% \n  ungroup() %&gt;% \n  mutate(response=Pred_2$fit,\n         se.fit=Pred_2$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewDataglmm2)+\n  geom_ribbon(aes(x=bodyLength,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=mountainRange,\n                  group=mountainRange),\n              alpha=0.7)+\n  geom_line(aes(x=bodyLength,\n                 y=response,\n                  colour=mountainRange))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore,\n                  colour=mountainRange))+\n  facet_wrap(~mountainRange)+\n    scale_colour_viridis_d()+\n    scale_fill_viridis_d()+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\",\n       fill=\"Mountain Range\",colour=\"Mountain Range\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nBut what if we thought the effect of bodylength and background testscores change with mountain ranges?",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#first-step-of-analysis---statistical-model-formulation-2",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#first-step-of-analysis---statistical-model-formulation-2",
    "title": "Introduction to GLMMs",
    "section": "First Step of Analysis - Statistical Model Formulation",
    "text": "First Step of Analysis - Statistical Model Formulation\nOkay, lets say we think the effect of bodylength isn’t consistent across mountain ranges nor is level of test ability.\nThis would create a stats model of:\ntestScore ~ bodyLength + (bodyLength+1|mountainRange)",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#second-step-of-analysis---response-variable-distribution-2",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#second-step-of-analysis---response-variable-distribution-2",
    "title": "Introduction to GLMMs",
    "section": "Second Step of Analysis - Response Variable Distribution",
    "text": "Second Step of Analysis - Response Variable Distribution\nSee above.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#third-step-of-analysis---organising-fixed-effects-and-random-effects-2",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#third-step-of-analysis---organising-fixed-effects-and-random-effects-2",
    "title": "Introduction to GLMMs",
    "section": "Third Step of Analysis - Organising Fixed Effects (and Random Effects)",
    "text": "Third Step of Analysis - Organising Fixed Effects (and Random Effects)\nSee above.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fourth-step-of-analysis---assessing-model-functioning-2",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fourth-step-of-analysis---assessing-model-functioning-2",
    "title": "Introduction to GLMMs",
    "section": "Fourth Step of Analysis - Assessing Model Functioning",
    "text": "Fourth Step of Analysis - Assessing Model Functioning\nSo lets run our new formula as a GLMM and run some simple diagnostic plots as with GLMs. We will use the glmmTMB package to use the glmmTMB() function as it allows the use of beta regression. To do this we install TMB then glmmTMB. The diagnostics are even better than previously but still an odd gap in the middle (maybe our site level difference/heirachry is a problem).\n\nglmm3&lt;-glmmTMB(testScore ~ bodyLength+ (bodyLength|mountainRange), data = dragons_1, family = beta_family)\n\nglmm3ModelOutputs&lt;-data.frame(Fitted=fitted(glmm3),\n                  Residuals=resid(glmm3))\n\np7&lt;-ggplot(glmm3ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np8&lt;-ggplot(glmm3ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np7+p8",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fifth-step-of-analysis---model-interpretation-2",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fifth-step-of-analysis---model-interpretation-2",
    "title": "Introduction to GLMMs",
    "section": "Fifth Step of Analysis - Model Interpretation",
    "text": "Fifth Step of Analysis - Model Interpretation\nAgain, as with GLMs, we make an ‘empty’ data frame with all the fixed effects values from the original data set, but we also have to include the random effects (mountain range) too. So we are not predicting outside of the range of the data from within each mountain range we will group by mountain range and use data_grid() from the modelr package to create our ‘newdata’. This shows us that with a slope and intercept varying model there are different effects of bodylength on testscore depending on different mountain ranges.\n\nNewData_2&lt;-dragons_1 %&gt;% \n  group_by(mountainRange) %&gt;% \n  modelr::data_grid(bodyLength)\n\nPred_3&lt;-predict(glmm3,NewData_2,se.fit=T,type=\"response\")\n\n\nNewDataglmm3&lt;-NewData_2 %&gt;% \n  ungroup() %&gt;% \n  mutate(response=Pred_3$fit,\n         se.fit=Pred_3$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewDataglmm3)+\n  geom_ribbon(aes(x=bodyLength,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=mountainRange,\n                  group=mountainRange),\n              alpha=0.7)+\n  geom_line(aes(x=bodyLength,\n                 y=response,\n                  colour=mountainRange))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore,\n                  colour=mountainRange))+\n    scale_colour_viridis_d()+\n    scale_fill_viridis_d()+\n  facet_wrap(~mountainRange)+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\",\n       colour=\"Mountain Range\",fill=\"Mountain Range\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can also inspect the overall effect regardless of the mountain range. We can use ggpredict from the ggeffects package to do this.\n\n#install.packages(\"ggeffects\")\n\nlibrary(ggeffects)\n\nggpredict(glmm3, terms = \"bodyLength\") %&gt;% \n  as_tibble()%&gt;% \n  ggplot()+\n  geom_ribbon(aes(x=x,\n                    ymax=conf.low,\n                    ymin=conf.high),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=x,\n                 y=predicted))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore,\n                  colour=mountainRange))+\n    scale_colour_viridis_d()+\n    scale_fill_viridis_d()+\n  facet_wrap(~mountainRange)+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\",\n       colour=\"Mountain Range\",fill=\"Mountain Range\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis model also shows no effect of bodylength on test score when using a slope and intercept varying model. However, this still hasn’t fully captured the hierarchy of the data, we have a site column that has not been considered.\nLets do a final full model.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#first-step-of-analysis---statistical-model-formulation-3",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#first-step-of-analysis---statistical-model-formulation-3",
    "title": "Introduction to GLMMs",
    "section": "First Step of Analysis - Statistical Model Formulation",
    "text": "First Step of Analysis - Statistical Model Formulation\nOkay, lets say we think the effect of bodylength isn’t consistent across mountain ranges nor is level of test ability and unique Sites have differing intercepts too.\nThis would create a stats model of:\ntestScore ~ bodyLength + (bodyLength|mountainRange)+ (1|siteUnique)\nHere we add in another intercept term. We use the unique version of site as site was originally nested in the mountainRange column (there was a, b and c in all mountain ranges but they were not the same).",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#second-step-of-analysis---response-variable-distribution-3",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#second-step-of-analysis---response-variable-distribution-3",
    "title": "Introduction to GLMMs",
    "section": "Second Step of Analysis - Response Variable Distribution",
    "text": "Second Step of Analysis - Response Variable Distribution\nSame as above.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#third-step-of-analysis---organising-fixed-effects-and-random-effects-3",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#third-step-of-analysis---organising-fixed-effects-and-random-effects-3",
    "title": "Introduction to GLMMs",
    "section": "Third Step of Analysis - Organising Fixed Effects (and Random Effects)",
    "text": "Third Step of Analysis - Organising Fixed Effects (and Random Effects)\nSame as above.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fourth-step-of-analysis---assessing-model-functioning-3",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fourth-step-of-analysis---assessing-model-functioning-3",
    "title": "Introduction to GLMMs",
    "section": "Fourth Step of Analysis - Assessing Model Functioning",
    "text": "Fourth Step of Analysis - Assessing Model Functioning\nSo lets run our new formula as a GLMM and run some simple diagnostic plots as with GLMs. We will use the glmmTMB package to use the glmmTMB() function as it allows the use of beta regression. To do this we install TMB then glmmTMB. The diagnostics are even better than previously.\n\nglmm4&lt;-glmmTMB(testScore ~ bodyLength+ (bodyLength|mountainRange)+(1|siteUnique), data = dragons_1, family = beta_family)\n\nglmm4ModelOutputs&lt;-data.frame(Fitted=fitted(glmm4),\n                  Residuals=resid(glmm4))\n\np9&lt;-ggplot(glmm4ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np10&lt;-ggplot(glmm4ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np9+p10",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fifth-step-of-analysis---model-interpretation-3",
    "href": "StatisticsTutorialsGLMM/IntroductionGLMMs.html#fifth-step-of-analysis---model-interpretation-3",
    "title": "Introduction to GLMMs",
    "section": "Fifth Step of Analysis - Model Interpretation",
    "text": "Fifth Step of Analysis - Model Interpretation\nAgain, as with GLMs, we make an ‘empty’ data frame with all the fixed effects values from the original data set, but we also have to include the random effects (mountain range) too. So we are not predicting outside of the range of the data from within each mountain range we will group by mountain range and use data_grid() from the modelr package to create our ‘newdata’. This shows us that with a slope and intercept varying model there are different effects of bodylength on testscore depending on different mountain ranges and unique sites within those mountains.\n\nNewData_3&lt;-dragons_1 %&gt;% \n  group_by(mountainRange,siteUnique) %&gt;% \n  modelr::data_grid(bodyLength)\n\nPred_4&lt;-predict(glmm4,NewData_3,se.fit=T,type=\"response\")\n\n\nNewDataglmm4&lt;-NewData_3 %&gt;% \n  ungroup() %&gt;% \n  mutate(response=Pred_4$fit) \n\nggplot(NewDataglmm4)+\n  geom_line(aes(x=bodyLength,\n                 y=response,\n                  colour=mountainRange,\n                group=siteUnique))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore,\n                  colour=mountainRange,shape=site))+\n    scale_colour_viridis_d()+\n    scale_fill_viridis_d()+\n  facet_wrap(~mountainRange)+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\",\n       colour=\"Mountain Range\",fill=\"Mountain Range\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can also inspect the overall effect regardless of the mountain range and site. We can use ggpredict from the ggeffects package to do this.\n\nggpredict(glmm4, terms = \"bodyLength\") %&gt;% \n  as_tibble()%&gt;% \n  ggplot()+\n  geom_line(aes(x=x,\n                 y=predicted))+\n  geom_point(data=dragons_1,aes(x=bodyLength,y=testScore,\n                  colour=mountainRange))+\n    scale_colour_viridis_d()+\n    scale_fill_viridis_d()+\n  facet_wrap(~mountainRange)+\n  labs(x=\"Body Length\",y=\"Response Variable (Test Score)\",\n       colour=\"Mountain Range\",fill=\"Mountain Range\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis shows no effect of body length on test score given the hierarchy of our data.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to GLMMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ProblematicGLMs.html#data-loading---lets-go-back-to-penguins",
    "href": "StatisticsTutorials/ProblematicGLMs.html#data-loading---lets-go-back-to-penguins",
    "title": "Common GLM Problems",
    "section": "Data Loading - Lets go Back to Penguins",
    "text": "Data Loading - Lets go Back to Penguins\nWe will use the Penguins data set again, but this time we will not take into consideration the causal implications of Sex or Species for the morphometric bill_depth_mm. This would not make any sense with a dataset where different species and different sexes of these species are present.\n\n#install.packages(\"palmerpenguins\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\nlibrary(performance)\n\ndata(penguins)\n\npenguins_noNAs&lt;-penguins%&gt;% \n  drop_na()\n\n\nStep One - Scienctific Model to Stats Model\n\nHere we will see if the effect of flipper length on the depth of the bill.\nThis is a fairly simple model with one fixed effect and can be written as:\nBill Depth ~ Flipper Length\n\n\nStep Two - Resonse Variable Distribution\nAs before when we modelled flipper length, bill depth is technically Gamma distributed but a linear model will work well.\n\n\nStep Three - Organising Fixed Effects\nOur data are fairly well distributed across the values, although there are two clear peaks. HINT HINT - This might cause issues if we haven’t taken into consideration why there are two peaks (one of which is twice the height of the other).\n\nggplot(penguins_noNAs,aes(x=flipper_length_mm))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Flipper Length (mm)\",y=\"Density\")\n\n\n\n\n\n\n\n\nAs we will pretend these look fine, we shall fit out model with a Gaussian distribution.\n\nlm1&lt;-lm(bill_depth_mm~flipper_length_mm,data=penguins_noNAs)\n\n\n\nStep Four - Assessing Model Functioning\nWe can use the check_model() function to assess the residuals from the performance package.\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm1),\n                  Residuals=resid(lm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAhhh No, this looks pretty bad. Well the qqnorm plot looks good actually, but the residuals vs fitted values has clear patterns that cannot be ignored.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Common GLM Problems"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ProblematicGLMs.html#what-is-wrong-and-how-do-we-fix-it",
    "href": "StatisticsTutorials/ProblematicGLMs.html#what-is-wrong-and-how-do-we-fix-it",
    "title": "Common GLM Problems",
    "section": "What is wrong and How do we fix it?",
    "text": "What is wrong and How do we fix it?\nSo for us to see such strong patterns in our residuals vs fitted plot there are a number of different issues that could be causing this:\n\nWe have used the wrong distribution,\nWe haven’t modelled the structure of the data properly,\nWe have ignored issues in the sampling that created inherent bias in the data,\nOur data have excessive numbers of 0s (either caused by our sampling methods or precision issues).\n\n\n1) Wrong Distribution\nIf we have used the wrong distribution we should remodel with the correct distribution. This can happen especially for poisson models where there is high levels of variance. The poisson distribution expects variance to be proportional to the expected value. When there is significant over dispersion of values we can use a different distribution called the negative binomial distribution.\n\n\n2) Wrong Model Structure\nIf we have modelled the data structure incorrectly, i.e. ignored the difference in the relationship of bill depth and flipper depending on different species, then we need to include that structure in the model. Sometimes this will be some sort of hierarchy, repeat measures or multi-level structure, to model this correctly we will need to use mixed effect or multi-level models (General Linear Mixed Effect Models: GLMMs, we will go through these in future tutorials).\n\n\n3) Inherent Bias in Sampling\nThis is a more complex issue. This could, for example, be that we only sampled male gentoo penguins and female chinstrap penguins, thus our ability to split by sex and species to model the data structure properly will be impossible. In many situations, if we know the bias we could perhaps model within one of the groups, just male gentoo penguins, but this would mean our model was only applicable to this group. Other times we may have a sampling cut off/threshold, which may or may not be acceptable. For example, when measuring penguins there may be ethical issues/sampling protocols of measuring individuals smaller than a certain size, as it could harm their parent-chick relationship, or cause too much distress to the individual. This will effectively mean you miss data below a certain size. Often this can be acceptable as long as the interpretation of the model takes this into consideration.\n\n\n4) Zero or One Inflated Data\nZero (or one) inflation of data can have multiple root causes, such as sampling bias as mentioned above, also subsetting of multivariate datasets e.g. we have catch data for a experimental trawls. From the trawls we get abundance per trawl of all the different species caught, but we are interested in just one species and its abundance. Sometimes this means trawls will have high numbers of zeros for this species. Sometimes zeros will not cause issues but sometimes high numbers of zeros will cause clear patterns in both the residuals vs fitted and qqnorm plots. When this happens we can employ zero inflated or zero adjusted versions of the model we want. This is effectively creating two models in sequence. Firstly, we model the presence or absence with a Bernoulli model, then when there is presence we model all non-zero values with the desired distribution (We will also explore these models in a future tutorial).",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Common GLM Problems"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ProblematicGLMs.html#solution",
    "href": "StatisticsTutorials/ProblematicGLMs.html#solution",
    "title": "Common GLM Problems",
    "section": "Solution",
    "text": "Solution\nWhile we could go down the GLMM route here, we can also model the species into our model to sort the issues we found.\n\nlm2&lt;-lm(bill_depth_mm~flipper_length_mm*species,data=penguins_noNAs)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm2),\n                  Residuals=resid(lm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nThis is a lot better, but again we can see two clear groups of points with different spreads, what other element of our data has two factors that we may have missed?\n\nlm3&lt;-lm(bill_depth_mm~flipper_length_mm*species+sex,data=penguins_noNAs)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm3),\n                  Residuals=resid(lm3))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nThis is better, but not perfect as we still see some patterns, but generally our points are more evenly distributed above and below the 0 line.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Common GLM Problems"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ProblematicGLMs.html#galapagos",
    "href": "StatisticsTutorials/ProblematicGLMs.html#galapagos",
    "title": "Common GLM Problems",
    "section": "Galapagos",
    "text": "Galapagos\nHere we will bring the Galapagos dataset back. When we used this for a poisson model we talked about it having high levels of over dispersion. We ignored it before to give the example, but lets now actually take action.\n\n\nlibrary(faraway)\ndata(gala)\n\n\nglm1&lt;-glm(Species~Elevation+Nearest,data=gala, family= \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nSo how do we the fit a negative binomial model? We need to load another package as the negative binomial distribution in glm() needs to already know the overdispersion, whereas we won’t know it and will want to estimate it within the model! To do this we can use the glm.nb() function from the MASS package. Note of caution: the MASS package has a function called select() this will conflict with the select function from dplyr we use for data manipulation - Be aware!\n\n#install.packages(\"MASS\")\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nglm2&lt;-glm.nb(Species~Elevation+Nearest,data=gala)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\nggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    scale_x_continuous(limits=c(0,100))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs we can see this is a better residuals check. As mentioned before when we used this data set there are very few data points so we are very unlikely to get a perfect model fit (At least not with a simple glm). We can still see some very large fitted values that might be worth investigating but when we zoom in we can see the other residuals seem pretty good.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Common GLM Problems"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BinomialGLMs.html",
    "href": "StatisticsTutorials/BinomialGLMs.html",
    "title": "Binomial GLMs",
    "section": "",
    "text": "Okay let’s grab the data from the yarrr dataset for diamonds to assess what drives the value of diamonds based on their weight, clarity and colour.\n\nlibrary(performance)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n#install.packages(\"yarrr\")\nlibrary(yarrr)\n\nLoading required package: jpeg\nLoading required package: BayesFactor\nLoading required package: coda\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n************\nWelcome to BayesFactor 0.9.12-4.7. If you have questions, please contact Richard Morey (richarddmorey@gmail.com).\n\nType BFManual() to open the manual.\n************\nLoading required package: circlize\n========================================\ncirclize version 0.4.16\nCRAN page: https://cran.r-project.org/package=circlize\nGithub page: https://github.com/jokergoo/circlize\nDocumentation: https://jokergoo.github.io/circlize_book/book/\n\nIf you use it in published research, please cite:\nGu, Z. circlize implements and enhances circular visualization\n  in R. Bioinformatics 2014.\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(circlize))\n========================================\n\nyarrr v0.1.5. Citation info at citation('yarrr'). Package guide at yarrr.guide()\nEmail me at Nathaniel.D.Phillips.is@gmail.com\n\nAttaching package: 'yarrr'\n\nThe following object is masked from 'package:ggplot2':\n\n    diamonds\n\ndata(diamonds)\nsummary(diamonds)\n\n     weight          clarity           color          value      \n Min.   : 5.360   Min.   :0.4400   Min.   :2.00   Min.   :174.0  \n 1st Qu.: 8.598   1st Qu.:0.8900   1st Qu.:4.00   1st Qu.:184.0  \n Median : 9.805   Median :1.0000   Median :5.00   Median :189.6  \n Mean   : 9.901   Mean   :0.9996   Mean   :4.96   Mean   :189.4  \n 3rd Qu.:11.155   3rd Qu.:1.1200   3rd Qu.:6.00   3rd Qu.:194.9  \n Max.   :14.270   Max.   :1.4400   Max.   :8.00   Max.   :206.4  \n\n\n\n\n\nSo we could model the actual diamond value but for this example we will split the value into high and low values (higher than 190 or lower than 190).\n\ndf_bin&lt;-diamonds %&gt;% \n  mutate(binary=if_else(value&gt;190,1,0))\n\nggplot(df_bin,aes(x=as.factor(binary)))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Binary Value Above 190 or not\",y=\"Count\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe imagine that the colour, weight and clarity will all influence the value of a diamond, thus we shall additively include each of these variables in the model.\nBinary Value (above 190) ~ Clarity + Weight + Colour\n\n\n\nWe created the response variable to be either 1 or 0. We therefore know that it is a binomial distribution but more specifically it is a Bernoulli distribution. For modelling in r the binomial distribution family is what is used.\n\n\n\nOkay so lets look at our fixed effects, we will use density for continuous values (clarity and weight), while bars can show the counts for the integer Colour. Here colour is an ordinal integer, therefore we will model it as such. If we had Colour as a category (such as red, green, blue etc) we would want to convert it to be a factor for modelling as it would be more nominal.\n\np1&lt;-ggplot(df_bin,aes(x=clarity))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Clarity\",y=\"Density\")+\n  theme_classic()\n\np2&lt;-ggplot(df_bin,aes(x=color))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Colour\",y=\"Count\")+\n  theme_classic()\n\np3&lt;-ggplot(df_bin,aes(x=weight))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Weight\",y=\"Denisty\")+\n  theme_classic()\n\n\np1+p2+p3\n\n\n\n\n\n\n\n\nLet’s fit the glm using these our statistical model from above.\n\nglm1&lt;-glm(binary~clarity+color+weight,data=df_bin, family= \"binomial\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_histogram(aes(x=Residuals))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Count\")\n\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nOkay, here we have a different looking plot than before as the response variable is not continuous or integers. Therefore, homogeneity of residuals is less important, but normality of residuals is. Here we see that the qqnorm plot looks fairly good but high values are moving away from normality. Generally the model seems to fit well so we will interpret it.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = binary ~ clarity + color + weight, family = \"binomial\", \n    data = df_bin)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -18.8009     3.4634  -5.428 5.69e-08 ***\nclarity       9.2910     1.9629   4.733 2.21e-06 ***\ncolor        -0.3836     0.2481  -1.547    0.122    \nweight        1.1251     0.1968   5.716 1.09e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 207.52  on 149  degrees of freedom\nResidual deviance: 106.67  on 146  degrees of freedom\nAIC: 114.67\n\nNumber of Fisher Scoring iterations: 6\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[ Value \\:Above \\:190 \\:(or \\:Not) = Binomial(N, Pr)\\]\n\\[Pr=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Weight + \\beta_{2} Colour + \\beta_{3} Clarity+ Intercept\n\\end{aligned}\n\\]\nHere our link function is slightly different again. This time it is a logit. This is effectively a way of converting a value to be a probability.\nWe can check this is the link function r is using here.\n\nglm1$family$link\n\n[1] \"logit\"\n\n\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just Clarity, Color and Weight the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Probability that a Diamond has a value over 190 based on its weight, color and clarity.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval, we make sure these Upper and Lower confidence intervals don’t stray outside of 0 and 1).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\nTo make plotting easier I will set a Low, middle and high value for weight and color as clarity has the highest influence on diamond value.\n\nNewData_1&lt;-expand.grid(weight=c(min(df_bin$weight),mean(df_bin$weight),max(df_bin$weight)),\n                       color=c(2,5,8),\n                       clarity=seq(min(df_bin$clarity),max(df_bin$clarity),length.out=50)\n                       ) %&gt;% \n  mutate(Weight_Group=factor(case_when(weight==min(df_bin$weight)~\"Low\",\n                                weight==mean(df_bin$weight)~\"Mid\",\n                                weight==max(df_bin$weight)~\"High\"\n                                ),levels=c(\"Low\",\"Mid\",\"High\")),\n         Color_Group=factor(case_when(color==2~\"Low Colour\",\n                                color==5~\"Mid Colour\",\n                                color==8~\"High Colour\"\n                                ),levels=c(\"Low Colour\",\"Mid Colour\",\"High Colour\")))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Upr=case_when(Upr&gt;1~1,\n                       TRUE~Upr),\n         Lwr=case_when(Lwr&lt;0~0,\n                       TRUE~Lwr))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=clarity,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Weight_Group),\n              alpha=0.6)+\n  geom_line(aes(x=clarity,\n                 y=response,\n                  colour=Weight_Group))+\n  facet_wrap(~Color_Group,ncol = 1)+\n   scale_colour_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"))+\n  labs(x=\"Clarity\",y=\"Response Variable (Probability of Diamond value over 190)\",\n       fill=\"Weight Group\",colour=\"Weight Group\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nFrom this graph we can see that Higher Weight class leads to a higher probability of a diamond being over 190 in value, likewise higher clarity leads to higher probability of high value. Colour seems to have less effect, although this is hard to see from this graph.\nNow lets plot this model output over the raw values to see how well the model has worked. We will create new columns to show the Colour and Weight Group from the raw data. I will set arbitary cut offs, which could be interquartiles or thirds of the data. Or if there were specific values of interest these could be plotted on their own.\n\nRaw_df_bin&lt;-df_bin %&gt;% \n  mutate(Weight_Group=factor(case_when(weight&lt;=7.5~\"Low\",\n                                weight&gt;7.5 & weight&lt;12.5~\"Mid\",\n                                weight&gt;=12.5~\"High\"\n                                ),levels=c(\"Low\",\"Mid\",\"High\")),\n         Color_Group=factor(case_when(color&lt;=3~\"Low Colour\",\n                                color&gt;3 & color&lt;7~\"Mid Colour\",\n                                color&gt;=7~\"High Colour\"\n                                ),levels=c(\"Low Colour\",\"Mid Colour\",\"High Colour\")))\n\n\nggplot(NewData)+\n  geom_ribbon(aes(x=clarity,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Weight_Group),\n              alpha=0.6)+\n  geom_line(aes(x=clarity,\n                 y=response,\n                  colour=Weight_Group))+\n  geom_point(data=Raw_df_bin,aes(x=clarity,y=binary,colour=Weight_Group))+\n  facet_grid(Weight_Group~Color_Group)+\n   scale_colour_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"))+\n  labs(x=\"Clarity\",y=\"Response Variable (Probability of Diamond value over 190)\",\n       fill=\"Weight Group\",colour=\"Weight Group\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good, with more uncertainty where there are less values to influence the prediction. This is a very simplified model that is not taking into consideration many different factors, such as origin of diamond, the current diamond market etc. It is also a relatively small data set.\n\n\n\n\nLets create a more complex Binomial model. This data set is the frequency of affairs within the last year with their gender, age, yearsmarried, children, religiousness, education, occupation and self rating of marriage.\n\n#install.packages(\"AER\")\nlibrary(\"AER\")\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\ndata(Affairs, package=\"AER\")\n\nsummary(Affairs)\n\n    affairs          gender         age         yearsmarried    children \n Min.   : 0.000   female:315   Min.   :17.50   Min.   : 0.125   no :171  \n 1st Qu.: 0.000   male  :286   1st Qu.:27.00   1st Qu.: 4.000   yes:430  \n Median : 0.000                Median :32.00   Median : 7.000            \n Mean   : 1.456                Mean   :32.49   Mean   : 8.178            \n 3rd Qu.: 0.000                3rd Qu.:37.00   3rd Qu.:15.000            \n Max.   :12.000                Max.   :57.00   Max.   :15.000            \n religiousness     education       occupation        rating     \n Min.   :1.000   Min.   : 9.00   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:14.00   1st Qu.:3.000   1st Qu.:3.000  \n Median :3.000   Median :16.00   Median :5.000   Median :4.000  \n Mean   :3.116   Mean   :16.17   Mean   :4.195   Mean   :3.932  \n 3rd Qu.:4.000   3rd Qu.:18.00   3rd Qu.:6.000   3rd Qu.:5.000  \n Max.   :5.000   Max.   :20.00   Max.   :7.000   Max.   :5.000  \n\nAffairs_binom&lt;-Affairs %&gt;% \n  mutate(affairs_bin=if_else(affairs&gt;0,1,0))\n\nOur response variable will be a summary of the affairs column for occurrence or not of an affair in the last year where affairs was how often someone engaged in extramarital sexual intercourse during the past year? 0 = none, 1 = once, 2 = twice, 3 = 3 times, 7 = 4–10 times, 12 = monthly, 12 = weekly, 12 = daily.\n\n\nWe will assess the frequency of affairs and whether someones gender, religiousness affects this value, we will also include yearsmarried and whether the pattern of yearsmarried changes depending having children or not.\nThis is a bit more complex model with some interacting fixed effects and additional additive fixed effects and can be written as:\nFrequency of Affairs ~ Children*Yearsmarried + Gender + Religiousness\n\n\n\nAs before there are only integer values for affairs or none in the last year. This type of data could be presented in a different way if we wanted but we will use this scale for now.\n\n\n\nLets check all our fixed effects. For numeric values we can assess their distribution, categorical we can see the number of samples and if it is relatively even. From the way our data has been stored, they are all ordered categories but r has read some as numeric values. This is why we code them as factors first. Gender and children are already factors. We could use the yearsmarried as a categry but we will use a numeric here. I will also do a bit of house keeping such as capitalising the factors (personal preference!).\n\nAffairs_binom&lt;-Affairs_binom %&gt;% \n  mutate(Yearsmarried_fct=factor(case_when(yearsmarried==0.125~\"&lt;1 Year\",\n                                          yearsmarried==0.417~\"&lt;1 Year\",\n                                          yearsmarried==0.75~\"&lt;1 Year\",\n                                          yearsmarried==1.5~\"1-2 Years\",\n                                          yearsmarried==4~\"3-5 Years\",\n                                          yearsmarried==7~\"6-8 Years\",\n                                          yearsmarried==10~\"9-11 Years\",\n                                          yearsmarried==15~\"&gt; 12 Years\"\n                                          ),\n                                 levels=c(\"&lt;1 Year\",\n                                          \"1-2 Years\",\"3-5 Years\",\"6-8 Years\",\n                                          \"9-11 Years\",\"&gt; 12 Years\")),\n         Religiousness_fct=factor(case_when(religiousness==1~\"Anti\",\n                                          religiousness==2~\"Not\",\n                                          religiousness==3~\"Slightly\",\n                                          religiousness==4~\"Somewhat\",\n                                          religiousness==5~\"Very\"\n                                          ),\n                                 levels=c(\"Anti\",\"Not\",\"Slightly\",\"Somewhat\",\"Very\")),\n         Gender=case_when(gender==\"male\"~\"Male\",\n                          gender==\"female\"~\"Female\"),\n         Children=case_when(children==\"yes\"~\"Yes\",\n                            children==\"no\"~\"No\")) %&gt;% \n  dplyr::rename(Yearsmarried=yearsmarried)\n  \n\np1&lt;-ggplot(Affairs_binom,aes(x=Children))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(Affairs_binom,aes(x=Yearsmarried))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np3&lt;-ggplot(Affairs_binom,aes(x=Gender))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n  \np4&lt;-ggplot(Affairs_binom,aes(x=Religiousness_fct))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\nThese look okay, not always totally even but we shall see how the model performs.\n\nglm2&lt;-glm(affairs_bin~Children*Yearsmarried+Gender+Religiousness_fct,data=Affairs_binom, family= \"binomial\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_histogram(aes(x=Residuals))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Count\")\n\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAs earlier, we don’t see the plot for homogeneity of residuals but we do see binned residuals, which show two clear peaks of residuals. There seems to be two clear elements/patterns within our data. Generally, we shouldn’t accept this model diagnostic. One trick to find out where these issues come from can be to plot the residuals against the fixed factors in the model.\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2),\n                  Children= Affairs_binom$Children,\n                  Yearsmarried= Affairs_binom$Yearsmarried,\n                  Gender= Affairs_binom$Gender,\n                  Religiousness_fct= Affairs_binom$Religiousness_fct)\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Children))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Children\")\n\np4&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Yearsmarried))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Years Married\")\n\np5&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Gender))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Gender\")\n\np6&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Religiousness_fct))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Religiousness\")\n\n(p3+p4)/(p5+p6)\n\n\n\n\n\n\n\n\nThere seems to be no clear patterns we can see, so we must have some other issue somewhere, we will continue with caution now but that would not be advisable normally.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = affairs_bin ~ Children * Yearsmarried + Gender + \n    Religiousness_fct, family = \"binomial\", data = Affairs_binom)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -1.61951    0.42272  -3.831 0.000128 ***\nChildrenYes                1.10918    0.40614   2.731 0.006314 ** \nYearsmarried               0.16780    0.05222   3.213 0.001312 ** \nGenderMale                 0.19597    0.19837   0.988 0.323212    \nReligiousness_fctNot      -0.73164    0.35634  -2.053 0.040052 *  \nReligiousness_fctSlightly -0.42674    0.36140  -1.181 0.237685    \nReligiousness_fctSomewhat -1.45482    0.36579  -3.977 6.97e-05 ***\nReligiousness_fctVery     -1.37705    0.44453  -3.098 0.001950 ** \nChildrenYes:Yearsmarried  -0.12986    0.05682  -2.286 0.022282 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 675.38  on 600  degrees of freedom\nResidual deviance: 628.38  on 592  degrees of freedom\nAIC: 646.38\n\nNumber of Fisher Scoring iterations: 4\n\n\nHere we have, for the first time sing our penguins example, multiple levels in a categorical fixed effect. This highlights a behaviour that \\(\\beta\\) values or Estimate values from GLMs show pairwise effect. So each row in this summary where it says yearsmarried_ft then one of the factor levels, it is comparing that factor level to the base level (&lt;1 Year). Likewise, for religiousness_fct, where Anti is our base value and all factor levels are compared to this base level.\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this (this is a long one as there are so many factors):\n\\[ Affair \\:Occurence \\:in \\:Last  \\:Year = Binomial(N, Pr)\\]\n\\[Pr=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Children : Years \\:Married \\\\\n+ \\beta_{2} Anti \\:vs \\:Not \\:Regligious \\\\\n+ \\beta_{3} Anti \\:vs \\:Somewhat \\:Regligious \\\\\n+ \\beta_{4} Anti \\:vs \\:Slightly \\:Regligious \\\\\n+ \\beta_{5} Anti \\:vs \\:Very \\:Regligious \\\\\n+ \\beta_{6} Female \\:vs \\:Male \\:Gender \\\\\n+ \\beta_{7} Years \\:Married \\\\\n+ \\beta_{8} Children \\\\\n+ Intercept\n\\end{aligned}\n\\]\nThis is a lot of \\(\\beta\\) values but thankfully we don’t have to deal with them directly. As before our link function is the logit value.\n\nglm2$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with Children, Years Married, Gender and Religiousness the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the probability of affairs based on those Children, Years Married, Gender and Religiousness factors.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval again we make sure they stay between 0 and 1).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(Children=as.factor(c(\"Yes\",\"No\")),\n                       Yearsmarried=unique(Affairs_binom$Yearsmarried),\n                       Gender=as.factor(c(\"Female\",\"Male\")),\n                       Religiousness_fct=as.factor(c(\"Anti\",\"Not\",\"Slightly\",\"Somewhat\",\"Very\")))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Upr=case_when(Upr&gt;1~1,\n                       TRUE~Upr),\n         Lwr=case_when(Lwr&lt;0~0,\n                       TRUE~Lwr))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Yearsmarried,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Children),\n              alpha=0.6)+\n  geom_line(aes(x=Yearsmarried,\n                 y=response,\n                  colour=Children),\n              alpha=0.6)+\n  facet_grid(Religiousness_fct~Gender)+\n   scale_colour_manual(values=c(\"darkorange\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"darkcyan\"))+\n  labs(x=\"Years Married\",y=\"Probability of an Affair\\nin the Last Year\",\n       fill=\"Children\",\n       colour=\"Children\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe wont plot this model output with the raw values to see how well the model has worked as it will look pretty horrible. This is one of the biggest issues with Binomial models, the raw data are all 0s and 1s so when plotted it is very hard to see any patterns.\nWe can see from these plots that the probability of having had an affair in the last year increases with how many years you have been married especially if you don’t have children. This pattern is less clear or obvious if someone is more religious and there doesn’t appear to be any difference in these trends between Males or Females.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Binomial GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BinomialGLMs.html#data-loading-simple---diamonds",
    "href": "StatisticsTutorials/BinomialGLMs.html#data-loading-simple---diamonds",
    "title": "Binomial GLMs",
    "section": "",
    "text": "Okay let’s grab the data from the yarrr dataset for diamonds to assess what drives the value of diamonds based on their weight, clarity and colour.\n\nlibrary(performance)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n#install.packages(\"yarrr\")\nlibrary(yarrr)\n\nLoading required package: jpeg\nLoading required package: BayesFactor\nLoading required package: coda\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n************\nWelcome to BayesFactor 0.9.12-4.7. If you have questions, please contact Richard Morey (richarddmorey@gmail.com).\n\nType BFManual() to open the manual.\n************\nLoading required package: circlize\n========================================\ncirclize version 0.4.16\nCRAN page: https://cran.r-project.org/package=circlize\nGithub page: https://github.com/jokergoo/circlize\nDocumentation: https://jokergoo.github.io/circlize_book/book/\n\nIf you use it in published research, please cite:\nGu, Z. circlize implements and enhances circular visualization\n  in R. Bioinformatics 2014.\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(circlize))\n========================================\n\nyarrr v0.1.5. Citation info at citation('yarrr'). Package guide at yarrr.guide()\nEmail me at Nathaniel.D.Phillips.is@gmail.com\n\nAttaching package: 'yarrr'\n\nThe following object is masked from 'package:ggplot2':\n\n    diamonds\n\ndata(diamonds)\nsummary(diamonds)\n\n     weight          clarity           color          value      \n Min.   : 5.360   Min.   :0.4400   Min.   :2.00   Min.   :174.0  \n 1st Qu.: 8.598   1st Qu.:0.8900   1st Qu.:4.00   1st Qu.:184.0  \n Median : 9.805   Median :1.0000   Median :5.00   Median :189.6  \n Mean   : 9.901   Mean   :0.9996   Mean   :4.96   Mean   :189.4  \n 3rd Qu.:11.155   3rd Qu.:1.1200   3rd Qu.:6.00   3rd Qu.:194.9  \n Max.   :14.270   Max.   :1.4400   Max.   :8.00   Max.   :206.4  \n\n\n\n\n\nSo we could model the actual diamond value but for this example we will split the value into high and low values (higher than 190 or lower than 190).\n\ndf_bin&lt;-diamonds %&gt;% \n  mutate(binary=if_else(value&gt;190,1,0))\n\nggplot(df_bin,aes(x=as.factor(binary)))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Binary Value Above 190 or not\",y=\"Count\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe imagine that the colour, weight and clarity will all influence the value of a diamond, thus we shall additively include each of these variables in the model.\nBinary Value (above 190) ~ Clarity + Weight + Colour\n\n\n\nWe created the response variable to be either 1 or 0. We therefore know that it is a binomial distribution but more specifically it is a Bernoulli distribution. For modelling in r the binomial distribution family is what is used.\n\n\n\nOkay so lets look at our fixed effects, we will use density for continuous values (clarity and weight), while bars can show the counts for the integer Colour. Here colour is an ordinal integer, therefore we will model it as such. If we had Colour as a category (such as red, green, blue etc) we would want to convert it to be a factor for modelling as it would be more nominal.\n\np1&lt;-ggplot(df_bin,aes(x=clarity))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Clarity\",y=\"Density\")+\n  theme_classic()\n\np2&lt;-ggplot(df_bin,aes(x=color))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Colour\",y=\"Count\")+\n  theme_classic()\n\np3&lt;-ggplot(df_bin,aes(x=weight))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  labs(x=\"Diamond Weight\",y=\"Denisty\")+\n  theme_classic()\n\n\np1+p2+p3\n\n\n\n\n\n\n\n\nLet’s fit the glm using these our statistical model from above.\n\nglm1&lt;-glm(binary~clarity+color+weight,data=df_bin, family= \"binomial\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_histogram(aes(x=Residuals))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Count\")\n\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nOkay, here we have a different looking plot than before as the response variable is not continuous or integers. Therefore, homogeneity of residuals is less important, but normality of residuals is. Here we see that the qqnorm plot looks fairly good but high values are moving away from normality. Generally the model seems to fit well so we will interpret it.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = binary ~ clarity + color + weight, family = \"binomial\", \n    data = df_bin)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -18.8009     3.4634  -5.428 5.69e-08 ***\nclarity       9.2910     1.9629   4.733 2.21e-06 ***\ncolor        -0.3836     0.2481  -1.547    0.122    \nweight        1.1251     0.1968   5.716 1.09e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 207.52  on 149  degrees of freedom\nResidual deviance: 106.67  on 146  degrees of freedom\nAIC: 114.67\n\nNumber of Fisher Scoring iterations: 6\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[ Value \\:Above \\:190 \\:(or \\:Not) = Binomial(N, Pr)\\]\n\\[Pr=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Weight + \\beta_{2} Colour + \\beta_{3} Clarity+ Intercept\n\\end{aligned}\n\\]\nHere our link function is slightly different again. This time it is a logit. This is effectively a way of converting a value to be a probability.\nWe can check this is the link function r is using here.\n\nglm1$family$link\n\n[1] \"logit\"\n\n\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just Clarity, Color and Weight the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Probability that a Diamond has a value over 190 based on its weight, color and clarity.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval, we make sure these Upper and Lower confidence intervals don’t stray outside of 0 and 1).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\nTo make plotting easier I will set a Low, middle and high value for weight and color as clarity has the highest influence on diamond value.\n\nNewData_1&lt;-expand.grid(weight=c(min(df_bin$weight),mean(df_bin$weight),max(df_bin$weight)),\n                       color=c(2,5,8),\n                       clarity=seq(min(df_bin$clarity),max(df_bin$clarity),length.out=50)\n                       ) %&gt;% \n  mutate(Weight_Group=factor(case_when(weight==min(df_bin$weight)~\"Low\",\n                                weight==mean(df_bin$weight)~\"Mid\",\n                                weight==max(df_bin$weight)~\"High\"\n                                ),levels=c(\"Low\",\"Mid\",\"High\")),\n         Color_Group=factor(case_when(color==2~\"Low Colour\",\n                                color==5~\"Mid Colour\",\n                                color==8~\"High Colour\"\n                                ),levels=c(\"Low Colour\",\"Mid Colour\",\"High Colour\")))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Upr=case_when(Upr&gt;1~1,\n                       TRUE~Upr),\n         Lwr=case_when(Lwr&lt;0~0,\n                       TRUE~Lwr))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=clarity,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Weight_Group),\n              alpha=0.6)+\n  geom_line(aes(x=clarity,\n                 y=response,\n                  colour=Weight_Group))+\n  facet_wrap(~Color_Group,ncol = 1)+\n   scale_colour_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"))+\n  labs(x=\"Clarity\",y=\"Response Variable (Probability of Diamond value over 190)\",\n       fill=\"Weight Group\",colour=\"Weight Group\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nFrom this graph we can see that Higher Weight class leads to a higher probability of a diamond being over 190 in value, likewise higher clarity leads to higher probability of high value. Colour seems to have less effect, although this is hard to see from this graph.\nNow lets plot this model output over the raw values to see how well the model has worked. We will create new columns to show the Colour and Weight Group from the raw data. I will set arbitary cut offs, which could be interquartiles or thirds of the data. Or if there were specific values of interest these could be plotted on their own.\n\nRaw_df_bin&lt;-df_bin %&gt;% \n  mutate(Weight_Group=factor(case_when(weight&lt;=7.5~\"Low\",\n                                weight&gt;7.5 & weight&lt;12.5~\"Mid\",\n                                weight&gt;=12.5~\"High\"\n                                ),levels=c(\"Low\",\"Mid\",\"High\")),\n         Color_Group=factor(case_when(color&lt;=3~\"Low Colour\",\n                                color&gt;3 & color&lt;7~\"Mid Colour\",\n                                color&gt;=7~\"High Colour\"\n                                ),levels=c(\"Low Colour\",\"Mid Colour\",\"High Colour\")))\n\n\nggplot(NewData)+\n  geom_ribbon(aes(x=clarity,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Weight_Group),\n              alpha=0.6)+\n  geom_line(aes(x=clarity,\n                 y=response,\n                  colour=Weight_Group))+\n  geom_point(data=Raw_df_bin,aes(x=clarity,y=binary,colour=Weight_Group))+\n  facet_grid(Weight_Group~Color_Group)+\n   scale_colour_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"grey70\",\"darkcyan\"))+\n  labs(x=\"Clarity\",y=\"Response Variable (Probability of Diamond value over 190)\",\n       fill=\"Weight Group\",colour=\"Weight Group\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good, with more uncertainty where there are less values to influence the prediction. This is a very simplified model that is not taking into consideration many different factors, such as origin of diamond, the current diamond market etc. It is also a relatively small data set.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Binomial GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BinomialGLMs.html#data-loading-complex---affairs",
    "href": "StatisticsTutorials/BinomialGLMs.html#data-loading-complex---affairs",
    "title": "Binomial GLMs",
    "section": "",
    "text": "Lets create a more complex Binomial model. This data set is the frequency of affairs within the last year with their gender, age, yearsmarried, children, religiousness, education, occupation and self rating of marriage.\n\n#install.packages(\"AER\")\nlibrary(\"AER\")\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\ndata(Affairs, package=\"AER\")\n\nsummary(Affairs)\n\n    affairs          gender         age         yearsmarried    children \n Min.   : 0.000   female:315   Min.   :17.50   Min.   : 0.125   no :171  \n 1st Qu.: 0.000   male  :286   1st Qu.:27.00   1st Qu.: 4.000   yes:430  \n Median : 0.000                Median :32.00   Median : 7.000            \n Mean   : 1.456                Mean   :32.49   Mean   : 8.178            \n 3rd Qu.: 0.000                3rd Qu.:37.00   3rd Qu.:15.000            \n Max.   :12.000                Max.   :57.00   Max.   :15.000            \n religiousness     education       occupation        rating     \n Min.   :1.000   Min.   : 9.00   Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:14.00   1st Qu.:3.000   1st Qu.:3.000  \n Median :3.000   Median :16.00   Median :5.000   Median :4.000  \n Mean   :3.116   Mean   :16.17   Mean   :4.195   Mean   :3.932  \n 3rd Qu.:4.000   3rd Qu.:18.00   3rd Qu.:6.000   3rd Qu.:5.000  \n Max.   :5.000   Max.   :20.00   Max.   :7.000   Max.   :5.000  \n\nAffairs_binom&lt;-Affairs %&gt;% \n  mutate(affairs_bin=if_else(affairs&gt;0,1,0))\n\nOur response variable will be a summary of the affairs column for occurrence or not of an affair in the last year where affairs was how often someone engaged in extramarital sexual intercourse during the past year? 0 = none, 1 = once, 2 = twice, 3 = 3 times, 7 = 4–10 times, 12 = monthly, 12 = weekly, 12 = daily.\n\n\nWe will assess the frequency of affairs and whether someones gender, religiousness affects this value, we will also include yearsmarried and whether the pattern of yearsmarried changes depending having children or not.\nThis is a bit more complex model with some interacting fixed effects and additional additive fixed effects and can be written as:\nFrequency of Affairs ~ Children*Yearsmarried + Gender + Religiousness\n\n\n\nAs before there are only integer values for affairs or none in the last year. This type of data could be presented in a different way if we wanted but we will use this scale for now.\n\n\n\nLets check all our fixed effects. For numeric values we can assess their distribution, categorical we can see the number of samples and if it is relatively even. From the way our data has been stored, they are all ordered categories but r has read some as numeric values. This is why we code them as factors first. Gender and children are already factors. We could use the yearsmarried as a categry but we will use a numeric here. I will also do a bit of house keeping such as capitalising the factors (personal preference!).\n\nAffairs_binom&lt;-Affairs_binom %&gt;% \n  mutate(Yearsmarried_fct=factor(case_when(yearsmarried==0.125~\"&lt;1 Year\",\n                                          yearsmarried==0.417~\"&lt;1 Year\",\n                                          yearsmarried==0.75~\"&lt;1 Year\",\n                                          yearsmarried==1.5~\"1-2 Years\",\n                                          yearsmarried==4~\"3-5 Years\",\n                                          yearsmarried==7~\"6-8 Years\",\n                                          yearsmarried==10~\"9-11 Years\",\n                                          yearsmarried==15~\"&gt; 12 Years\"\n                                          ),\n                                 levels=c(\"&lt;1 Year\",\n                                          \"1-2 Years\",\"3-5 Years\",\"6-8 Years\",\n                                          \"9-11 Years\",\"&gt; 12 Years\")),\n         Religiousness_fct=factor(case_when(religiousness==1~\"Anti\",\n                                          religiousness==2~\"Not\",\n                                          religiousness==3~\"Slightly\",\n                                          religiousness==4~\"Somewhat\",\n                                          religiousness==5~\"Very\"\n                                          ),\n                                 levels=c(\"Anti\",\"Not\",\"Slightly\",\"Somewhat\",\"Very\")),\n         Gender=case_when(gender==\"male\"~\"Male\",\n                          gender==\"female\"~\"Female\"),\n         Children=case_when(children==\"yes\"~\"Yes\",\n                            children==\"no\"~\"No\")) %&gt;% \n  dplyr::rename(Yearsmarried=yearsmarried)\n  \n\np1&lt;-ggplot(Affairs_binom,aes(x=Children))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(Affairs_binom,aes(x=Yearsmarried))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np3&lt;-ggplot(Affairs_binom,aes(x=Gender))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n  \np4&lt;-ggplot(Affairs_binom,aes(x=Religiousness_fct))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\nThese look okay, not always totally even but we shall see how the model performs.\n\nglm2&lt;-glm(affairs_bin~Children*Yearsmarried+Gender+Religiousness_fct,data=Affairs_binom, family= \"binomial\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_histogram(aes(x=Residuals))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Count\")\n\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAs earlier, we don’t see the plot for homogeneity of residuals but we do see binned residuals, which show two clear peaks of residuals. There seems to be two clear elements/patterns within our data. Generally, we shouldn’t accept this model diagnostic. One trick to find out where these issues come from can be to plot the residuals against the fixed factors in the model.\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2),\n                  Children= Affairs_binom$Children,\n                  Yearsmarried= Affairs_binom$Yearsmarried,\n                  Gender= Affairs_binom$Gender,\n                  Religiousness_fct= Affairs_binom$Religiousness_fct)\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Children))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Children\")\n\np4&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Yearsmarried))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Years Married\")\n\np5&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Gender))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Gender\")\n\np6&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Residuals,y=Religiousness_fct))+\n    theme_classic()+\n    labs(x=\"Residuals\",y=\"Religiousness\")\n\n(p3+p4)/(p5+p6)\n\n\n\n\n\n\n\n\nThere seems to be no clear patterns we can see, so we must have some other issue somewhere, we will continue with caution now but that would not be advisable normally.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = affairs_bin ~ Children * Yearsmarried + Gender + \n    Religiousness_fct, family = \"binomial\", data = Affairs_binom)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -1.61951    0.42272  -3.831 0.000128 ***\nChildrenYes                1.10918    0.40614   2.731 0.006314 ** \nYearsmarried               0.16780    0.05222   3.213 0.001312 ** \nGenderMale                 0.19597    0.19837   0.988 0.323212    \nReligiousness_fctNot      -0.73164    0.35634  -2.053 0.040052 *  \nReligiousness_fctSlightly -0.42674    0.36140  -1.181 0.237685    \nReligiousness_fctSomewhat -1.45482    0.36579  -3.977 6.97e-05 ***\nReligiousness_fctVery     -1.37705    0.44453  -3.098 0.001950 ** \nChildrenYes:Yearsmarried  -0.12986    0.05682  -2.286 0.022282 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 675.38  on 600  degrees of freedom\nResidual deviance: 628.38  on 592  degrees of freedom\nAIC: 646.38\n\nNumber of Fisher Scoring iterations: 4\n\n\nHere we have, for the first time sing our penguins example, multiple levels in a categorical fixed effect. This highlights a behaviour that \\(\\beta\\) values or Estimate values from GLMs show pairwise effect. So each row in this summary where it says yearsmarried_ft then one of the factor levels, it is comparing that factor level to the base level (&lt;1 Year). Likewise, for religiousness_fct, where Anti is our base value and all factor levels are compared to this base level.\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this (this is a long one as there are so many factors):\n\\[ Affair \\:Occurence \\:in \\:Last  \\:Year = Binomial(N, Pr)\\]\n\\[Pr=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Children : Years \\:Married \\\\\n+ \\beta_{2} Anti \\:vs \\:Not \\:Regligious \\\\\n+ \\beta_{3} Anti \\:vs \\:Somewhat \\:Regligious \\\\\n+ \\beta_{4} Anti \\:vs \\:Slightly \\:Regligious \\\\\n+ \\beta_{5} Anti \\:vs \\:Very \\:Regligious \\\\\n+ \\beta_{6} Female \\:vs \\:Male \\:Gender \\\\\n+ \\beta_{7} Years \\:Married \\\\\n+ \\beta_{8} Children \\\\\n+ Intercept\n\\end{aligned}\n\\]\nThis is a lot of \\(\\beta\\) values but thankfully we don’t have to deal with them directly. As before our link function is the logit value.\n\nglm2$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with Children, Years Married, Gender and Religiousness the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the probability of affairs based on those Children, Years Married, Gender and Religiousness factors.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval again we make sure they stay between 0 and 1).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(Children=as.factor(c(\"Yes\",\"No\")),\n                       Yearsmarried=unique(Affairs_binom$Yearsmarried),\n                       Gender=as.factor(c(\"Female\",\"Male\")),\n                       Religiousness_fct=as.factor(c(\"Anti\",\"Not\",\"Slightly\",\"Somewhat\",\"Very\")))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Upr=case_when(Upr&gt;1~1,\n                       TRUE~Upr),\n         Lwr=case_when(Lwr&lt;0~0,\n                       TRUE~Lwr))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Yearsmarried,\n                    ymax=Upr,\n                    ymin=Lwr,\n                  fill=Children),\n              alpha=0.6)+\n  geom_line(aes(x=Yearsmarried,\n                 y=response,\n                  colour=Children),\n              alpha=0.6)+\n  facet_grid(Religiousness_fct~Gender)+\n   scale_colour_manual(values=c(\"darkorange\",\"darkcyan\"),)+\n   scale_fill_manual(values=c(\"darkorange\",\"darkcyan\"))+\n  labs(x=\"Years Married\",y=\"Probability of an Affair\\nin the Last Year\",\n       fill=\"Children\",\n       colour=\"Children\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe wont plot this model output with the raw values to see how well the model has worked as it will look pretty horrible. This is one of the biggest issues with Binomial models, the raw data are all 0s and 1s so when plotted it is very hard to see any patterns.\nWe can see from these plots that the probability of having had an affair in the last year increases with how many years you have been married especially if you don’t have children. This pattern is less clear or obvious if someone is more religious and there doesn’t appear to be any difference in these trends between Males or Females.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Binomial GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html",
    "href": "StatisticsTutorials/IntroductionGLMs.html",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Statistical analysis in r is very easy (on the whole) but needs a bit more background to fully understand. But for most of our situations we will want to use a group of models called General Linear Models (GLMs) or if more complex General Linear Mixed Effects Models (GLMMs). These modelling frameworks plus the additions/alterations from their defaults have lots of different names but in ecology and specifically r the names here are the most commonly used.\n\n\nThese modelling frameworks can be used to model almost all situations that will arise in our work. We are almost always using models to describe relationships. We can then use those models to predict future events but this is less common and more nuanced so we might use other tools for that.\nIn more traditional “Fischer” style statistics there are a wealth of different tests for each different scenario such as t-test, ANOVA, Kruskal-Wallis, Friedman, Mann-Whitney-U etc. These tests were created, designed and developed from work by a researcher called Fischer, with many ecologists/researchers generally holding onto this style as it has been used and taught for decades. However, they were designed and created for very specific cases. This means that they are extremely powerful and useful in those situations but are almost always used in slightly different situations or scenarios and so lose their power.\n\n\n\nOld Stats Test - Image from https://onishlab.colostate.edu/\n\n\nGLMs and GLMMs can be used in all the same cases as these older style tests as well as many many more. GLMs and by extension GLMMs have the advantage of being more consistency with terminology, have higher flexibility and you do not need to know 1000 different tests to use them. There are important options to chose within GLMs and GLMMs but as I mention they are more consistent and easier to check or adhere to their assumptions.\n\n\n\nDiscussion, examples and code here about GLMs and GLMMs will be carried out and more aimed at Frequentist methods of analysis. However, as Richard McElreath points out amazingly in his Statistical Rethinking Course (GO CHECK IT OUT IT IS AMAZING AND FREE!! https://www.youtube.com/@rmcelreath or https://github.com/rmcelreath), the general principles are the same between Frequentist and Bayesian methods, the important part is interpretation and building scientific and statistical models to allow Causal Inference! (i.e. does the model actually provide us with evidence of a cause and if so what is that cause.)\n\n\n\nBayes vs Frequentists Killed by Doge - Flagrantly stolen from https://github.com/rmcelreath\n\n\n\n\n\nEffectively General Linear Models are ways of creating generalising lines or linear equations that best describe the data we put into them given some equation of interest.\nWe can summarise most with three equations. We have the linear equation \\(y = \\beta x + c\\) and how that relates to the data we sample (\\(s\\)) \\(s = Distribution(y,\\sigma)\\) .\nThis will often be written generally as:\n\\[s = Distribution(y',\\sigma)\\]\n\\[y'=f(y)\\]\n\\[y = \\beta x + c\\]\nThe first of these equations is saying that the data we recorded (\\(s\\)) comes from some distribution (We will define this later) with a mean of \\(y'\\) and a standard deviation of \\(\\sigma\\). \\(y\\) is the result of an effect (\\(\\beta\\)) multiplied by some fixed effect (\\(x\\)) and an intercept value (\\(c\\)). Between these two equations we have a link function (\\(f()\\)). This converts \\(y\\) into a value that the distribution function will accept \\(y'\\), again we will define what these elements are later. The elements that we want our GLM to tell us is the effect of x (\\(\\beta\\)) and the intercept (\\(c\\)). We don’t have to do maths at all for GLMs but it can help some people understand what the model is telling us. (If you don’t like equations/maths ignore this bit - But we will discuss link functions later on).\n\n\n\nGenerally we will have a response variable (Dependent Variable) with one or more fixed effects (independent variables).\nHere I will discuss exclusively univariate modelling (where only one variable is the response variable) but multivariate analysis could follow the same framework, it is just more complicated to implement.\nOur research question will dictate how we create our scientific model, which dictates what statistical model to use and then how we interpret that statistical model.\nMany researchers like to draw out what is called a DAG, which diagrammatically shows assumed influences of factors, and how the directionallity of that influence. At first these are quite difficult to formulate until you have created a few. Therefore, the easiest way I can think to do this is saying some sort of hypothesis or statement in English then we can turn that statement into the Statistical model.\nFor example: I hypothesise that the flipper length of a penguin is described primarily by the species of that penguin but also influenced by its sex. Effectively Flipper length changes with Species and Sex.\n\n\n\nPenguins Image by Allison Horst\n\n\nHere our response variable is Flipper length (Dependant variable) and our fixed effects (Independent variables) are Species and Sex.\nIn statistical notation we could write this in a couple of ways depending on our assumptions:\n1). flipper_length_mm ~ sex * species\n2). flipper_length_mm ~ sex + species\nFirstly, we see on the left hand side of the ~ (tilde) we have our response variable, with our fixed effects on the other side.\nSecondly, we see that we either multiply or add these fixed effects. The difference between the two is subtle but important.\nEquation 1 is saying that flipper length is effected by species and by sexes but that the effect of sex is different depending on the species. (females always bigger in one species but males always bigger in another but one species is always much larger than the other).\nEquation 2 is saying that flipper length is effected by species and by sexes but that the effect of sex is the same or similar regardless of the species. (males always bigger across all species, but sizes different across species).\nWe may think either equation is correct given our knowledge of penguins.\n\n\n\nThe next most important element is what type of data our response variable is. This is the step that is often carried out wrong and leads to many inaccuracies of analysis.\nWhen modelling we are using our sample to make inference about a population. Therefore, our model should be based on our assumptions of the population that we took the sample from.\nThe type of data your response variable comes from may not be exactly what your response variable data looks like! However, if we have sufficiently sampled the population and not introduced any selection or sampling bias we should be confident in saying our sample (response variable) comes from the distribution of the population. We should be aware of this when analysing data from specific geographies or demographics etc!!!\nSo we should be able to tell the type of response variable data before we even collect the samples themselves! WE SHOULD NOT LOOK AT A GRAPH OF OUR DATA AND CHOOSE THE DISTRIBUTION IT COMES FROM JUST BY THE SHAPE!!!! (Speaking from experience and errors made!)\n\n\nThere are a group of widely used distributions that will cover almost all situations.\nThey are:\n\nGaussian\nBeta\nGamma\nPoisson\nBinomial/Bernoulli\n\nThere are many, many, many others such as Student-t or Direchlet or Tweedie. All of which are very useful and utilised throughout science but we won’t go through them here.\n\n\n\nDistributions - https://learnbyinsight.com/2020/09/27/probability-distribution-an-aid-to-know-the-data/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo when deciding what type of model to run we can first think whether our data are numerica or categoric. We will only discuss numerical data (temperature or weight or numbers of fish) but sometimes we might have a categorical response variable, such as questionnaire answers. Within these types of model there might be order to the categories (Not likely, Somewhat likely, Very likely) or there is no logical order (green, blue, purple). There are mixed effects models for these that are widely used, such as Multinomial Logisitic Regression for non-ordered and Ordinal Logistic Regression for ordered categorical response variables, but again we won’t discuss them here.\n\n\n\nOnce we know our data are numeric, we need to choose what type: continuous or integers. This is quite an easy one, can you measure it in decimals or is it complete units. When studying nesting birds we don’t count half eggs; a nest will have 1, 2, 3 etc. eggs. But if we are measuring the width of a tree it might be 4 cm or 2.365441 m.\n\n\n\n\n\n\n\n\n\nNest - https://www.thespruce.com\n\n\n\n\n\n\n\nTree - https://fennerschool-associated.anu.edu.au\n\n\n\n\n\n\n\n\nOnce we know our data are continuous numeric, we can think can the values be negative? A response variable that is continuous and can be negative is mostly modelled under the most common distribution: Gaussian. Examples of this could be temperature measured in \\(^\\circ\\)C or acceleration. A response variable that cannot be negative or even 0, would most correctly be: Gamma. However, Gaussian models are very often used for these models as a Guassian distribution is simpler computationally and mathematically, and only really causes issues when the values approach 0. This is actually most continuous values we measure, such as thickness of a stem, fish length, body mass or area of habitat.\n\n\n\n\n\n\n\n\n\n\n\n\nIf your response variable is continuous, and it may be negative or not, but is bound between an upper and lower known bound then it would most correctly be: Beta. For most practical terms Beta distribution is between 0 and 1. However, most data between bounds could be scaled to 0 and 1 and keep all of their characteristics. For example, percentage or proportional data are effectively between 0 and 1 and can be modelling using a Beta distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nWe know our data are integers but do we know whether there is an upper limit? If our data could potentially be very big as counts then we will use the Poisson distribution. This is for response variables like number of eggs in a nest or abundance of fish or the number of different species found along a transect. If we know there is an upper limit to the integers we could get then we will use a Binomial model. This could be for the number of eggs inside a six egg carton. The most common version of this is when we have a binary response variable (either 1 or 0). Presence or absence of a rare species for example. This is binary Binomial but is sometimes referred to specifically as Bernoulli.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now know what distribution we will most likely use to analyse our response data, so the next step is organising our fixed effects so the model treats them how we would like them to be treated. Effectively fixed effects are either numeric or categorical variables. With GLMs there is no different name (nor changes in code) generally used for what combination of numeric or categorical variables are used. So we could have multiple numeric effects, multiple categorical factors or a mixture of both.\n\n\nIn r numeric effects are quite straight forward, “what is the effect on y with a one unit change of value of x?”. However, if our numeric column has really big values, big range and a skewed distribution we may have issues with our model not working or running as we expect. For fixed effects this is easy to overcome, generally we would use some sort of transformation that maintains the structure of the data but minimises the issues the model runs into.\n\nSkewed Data\n\nFor example for skewed data, if our fixed effect was based on population size with the nearest 2 km, we may have some seriously big values in cities and seriously small values in the countryside. For these data we may use a log transformation, or a square root.\n\n\n\n\n\n\n\n\n\n\nOverly Large Data\n\nIf our fixed effect was a really big value but actually we are interested in small changes we might consider it differently. For example we might look at Year, in our data that would be above 2000 generally but actually if we have 5 years of data we are looking at it would be easier for the model to convert our data to year 1, year 2, year 2 rather than 2020, 2021, 2022. Here we maintain the difference of 1 year between each value so change the scale.\n\n\n\n\n\n\n\n\n\n\nAwkwardly Placed but ‘Nicely’ Distributed Data\n\nSometimes we may have a distribution of values that is ‘nicely’ distributed, with a clear mean, most values centered on that mean with less values further away from that mean. But if that mean is quite large, or really really small, or really really negative we may have issues with models not working correctly. This can also occur when we have more than one fixed effect and they are on very different scales. For example, body measurements as fixed effects where one body measurement is on the scale of metres and another is millimetres. For these we can centered and scale, sometimes call normalise, our effect by subtracting the mean from every value and dividing by the standard deviation.\n\n\n\nCategorical factors can be less simple. R takes the first level of the categorical effect as the baseline or reference state, then compares all others to that one reference level. If you only have two levels then that is fine, the model will tell you the effect of changing from state 1 to state 2. This is directional. However, with more than two levels the results in r will only tell you the pairwise comparisons from state 1 to state 2, state 1 to state 3, state 1 to state 4 …. state 1 to state n. This is harder to interpret in the results table but if you plot the model it becomes easy. It does mean that we can make sure if there is a control or base level in our factor we can use that as our reference level. This is done by ordering our column so that our desired base level is first in the column. If the column is a character style data r will use the first level alphabetically as our reference data. Sometimes we won’t care though, we just have to be aware that it is happening.\n\n\n\n\n\n\nGLMs and GLMMs have some broad assumptions for them to be appropriately used. Regardless of the distribution type an Independence of sample is assumed as well as homoscedasticity and normality of residuals is assumed.\nIf we have created an appropriate statistical model that takes into consideration the heirarchy/structure of the data our samples should be independent.\nSo what are residuals and what is homoscedasticity?\n\n\n\nTo assess the assumptions of our models we can look at the residual distance between the model line and the points, these are called the residuals of the model.\nIn its most basic version the residuals are the distance between the model and the raw data, then the plot of those residuals against the fitted values (or our response variable) can show how the sizes of residuals change across the fitted values. Homoscedasticity means homogeneity of variance or that the variance is consistent across the whole range of data. Therefore, ideally we want the residuals to represent a cloud of points with no clear patterns across the range of the response variable (fitted values), this is very subjective but a useful qualitative assessment of how good our model is. If our model was missing a clear pattern in the raw data then we would see a clear pattern in the residuals.\nIf we see patterns in our Response Variable vs Residuals plots (not just a random cloud of points) this may mean we missed an important fixed effect in our model (Underlying structure of the system has been ignored), the data do not come from the assigned distribution, there are a high number of zeros in our data or the data do come from the correct distribution but we have been biased in our sampling method (There could be many other reasons but these are the main ones).\n\n\n\n\n\n\n\n\n\n\n\n\nFor the vast majority of GLM and GLMMs, there is an assumption that our residuals are normally distributed. Basically this means that our residuals have a mean distance from the model and most of the values are around this mean. Then the likelihood or probability of having much larger or much smaller values than the mean becomes smaller and smaller. We can therefore plot our residuals as a histogram to see if it follows a generally normal or bell shaped distribution. We can also do what is called a Quantile-Quantile plot, which creates a theoretical distribution of values given the data range, then plots it against the empirical or true distribution. If the residual distribution are perfectly the same as the theoretical distribution it follows a 1 to 1 line (y=x). Again these are more qualitative checks where some variation and divergence from normality is fine.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce we are happy with our model, we can assess what the model tells us about our data. Our interpretation of the model will depend on the statistical model we used and the assumptions we made about our raw data. The best way to interpret a model, once we are happy with it, is to plot the model. To do this we can simulate data that covers the range of our fixed effects, then use the model to predict our response variable for all the values of these fixed effects we simulated. As a final check of how well our model has Generalised the patterns of the raw data we can also plot the raw data alongside our modelled data.\n\n\n\nExample of Model Prediction from Davies et al., 2022. Here points and error bars show raw data and lines with shading show model estimates and 95% confidence intervals.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#short-and-probably-biased-history",
    "href": "StatisticsTutorials/IntroductionGLMs.html#short-and-probably-biased-history",
    "title": "Introduction GLMs",
    "section": "",
    "text": "These modelling frameworks can be used to model almost all situations that will arise in our work. We are almost always using models to describe relationships. We can then use those models to predict future events but this is less common and more nuanced so we might use other tools for that.\nIn more traditional “Fischer” style statistics there are a wealth of different tests for each different scenario such as t-test, ANOVA, Kruskal-Wallis, Friedman, Mann-Whitney-U etc. These tests were created, designed and developed from work by a researcher called Fischer, with many ecologists/researchers generally holding onto this style as it has been used and taught for decades. However, they were designed and created for very specific cases. This means that they are extremely powerful and useful in those situations but are almost always used in slightly different situations or scenarios and so lose their power.\n\n\n\nOld Stats Test - Image from https://onishlab.colostate.edu/\n\n\nGLMs and GLMMs can be used in all the same cases as these older style tests as well as many many more. GLMs and by extension GLMMs have the advantage of being more consistency with terminology, have higher flexibility and you do not need to know 1000 different tests to use them. There are important options to chose within GLMs and GLMMs but as I mention they are more consistent and easier to check or adhere to their assumptions.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#side-note---bayesfrequentistwho-cares",
    "href": "StatisticsTutorials/IntroductionGLMs.html#side-note---bayesfrequentistwho-cares",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Discussion, examples and code here about GLMs and GLMMs will be carried out and more aimed at Frequentist methods of analysis. However, as Richard McElreath points out amazingly in his Statistical Rethinking Course (GO CHECK IT OUT IT IS AMAZING AND FREE!! https://www.youtube.com/@rmcelreath or https://github.com/rmcelreath), the general principles are the same between Frequentist and Bayesian methods, the important part is interpretation and building scientific and statistical models to allow Causal Inference! (i.e. does the model actually provide us with evidence of a cause and if so what is that cause.)\n\n\n\nBayes vs Frequentists Killed by Doge - Flagrantly stolen from https://github.com/rmcelreath",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#what-are-glms",
    "href": "StatisticsTutorials/IntroductionGLMs.html#what-are-glms",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Effectively General Linear Models are ways of creating generalising lines or linear equations that best describe the data we put into them given some equation of interest.\nWe can summarise most with three equations. We have the linear equation \\(y = \\beta x + c\\) and how that relates to the data we sample (\\(s\\)) \\(s = Distribution(y,\\sigma)\\) .\nThis will often be written generally as:\n\\[s = Distribution(y',\\sigma)\\]\n\\[y'=f(y)\\]\n\\[y = \\beta x + c\\]\nThe first of these equations is saying that the data we recorded (\\(s\\)) comes from some distribution (We will define this later) with a mean of \\(y'\\) and a standard deviation of \\(\\sigma\\). \\(y\\) is the result of an effect (\\(\\beta\\)) multiplied by some fixed effect (\\(x\\)) and an intercept value (\\(c\\)). Between these two equations we have a link function (\\(f()\\)). This converts \\(y\\) into a value that the distribution function will accept \\(y'\\), again we will define what these elements are later. The elements that we want our GLM to tell us is the effect of x (\\(\\beta\\)) and the intercept (\\(c\\)). We don’t have to do maths at all for GLMs but it can help some people understand what the model is telling us. (If you don’t like equations/maths ignore this bit - But we will discuss link functions later on).",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#first-step-of-analysis---statistical-model-formulation",
    "href": "StatisticsTutorials/IntroductionGLMs.html#first-step-of-analysis---statistical-model-formulation",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Generally we will have a response variable (Dependent Variable) with one or more fixed effects (independent variables).\nHere I will discuss exclusively univariate modelling (where only one variable is the response variable) but multivariate analysis could follow the same framework, it is just more complicated to implement.\nOur research question will dictate how we create our scientific model, which dictates what statistical model to use and then how we interpret that statistical model.\nMany researchers like to draw out what is called a DAG, which diagrammatically shows assumed influences of factors, and how the directionallity of that influence. At first these are quite difficult to formulate until you have created a few. Therefore, the easiest way I can think to do this is saying some sort of hypothesis or statement in English then we can turn that statement into the Statistical model.\nFor example: I hypothesise that the flipper length of a penguin is described primarily by the species of that penguin but also influenced by its sex. Effectively Flipper length changes with Species and Sex.\n\n\n\nPenguins Image by Allison Horst\n\n\nHere our response variable is Flipper length (Dependant variable) and our fixed effects (Independent variables) are Species and Sex.\nIn statistical notation we could write this in a couple of ways depending on our assumptions:\n1). flipper_length_mm ~ sex * species\n2). flipper_length_mm ~ sex + species\nFirstly, we see on the left hand side of the ~ (tilde) we have our response variable, with our fixed effects on the other side.\nSecondly, we see that we either multiply or add these fixed effects. The difference between the two is subtle but important.\nEquation 1 is saying that flipper length is effected by species and by sexes but that the effect of sex is different depending on the species. (females always bigger in one species but males always bigger in another but one species is always much larger than the other).\nEquation 2 is saying that flipper length is effected by species and by sexes but that the effect of sex is the same or similar regardless of the species. (males always bigger across all species, but sizes different across species).\nWe may think either equation is correct given our knowledge of penguins.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#second-step-of-analysis---response-variable-distribution",
    "href": "StatisticsTutorials/IntroductionGLMs.html#second-step-of-analysis---response-variable-distribution",
    "title": "Introduction GLMs",
    "section": "",
    "text": "The next most important element is what type of data our response variable is. This is the step that is often carried out wrong and leads to many inaccuracies of analysis.\nWhen modelling we are using our sample to make inference about a population. Therefore, our model should be based on our assumptions of the population that we took the sample from.\nThe type of data your response variable comes from may not be exactly what your response variable data looks like! However, if we have sufficiently sampled the population and not introduced any selection or sampling bias we should be confident in saying our sample (response variable) comes from the distribution of the population. We should be aware of this when analysing data from specific geographies or demographics etc!!!\nSo we should be able to tell the type of response variable data before we even collect the samples themselves! WE SHOULD NOT LOOK AT A GRAPH OF OUR DATA AND CHOOSE THE DISTRIBUTION IT COMES FROM JUST BY THE SHAPE!!!! (Speaking from experience and errors made!)\n\n\nThere are a group of widely used distributions that will cover almost all situations.\nThey are:\n\nGaussian\nBeta\nGamma\nPoisson\nBinomial/Bernoulli\n\nThere are many, many, many others such as Student-t or Direchlet or Tweedie. All of which are very useful and utilised throughout science but we won’t go through them here.\n\n\n\nDistributions - https://learnbyinsight.com/2020/09/27/probability-distribution-an-aid-to-know-the-data/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo when deciding what type of model to run we can first think whether our data are numerica or categoric. We will only discuss numerical data (temperature or weight or numbers of fish) but sometimes we might have a categorical response variable, such as questionnaire answers. Within these types of model there might be order to the categories (Not likely, Somewhat likely, Very likely) or there is no logical order (green, blue, purple). There are mixed effects models for these that are widely used, such as Multinomial Logisitic Regression for non-ordered and Ordinal Logistic Regression for ordered categorical response variables, but again we won’t discuss them here.\n\n\n\nOnce we know our data are numeric, we need to choose what type: continuous or integers. This is quite an easy one, can you measure it in decimals or is it complete units. When studying nesting birds we don’t count half eggs; a nest will have 1, 2, 3 etc. eggs. But if we are measuring the width of a tree it might be 4 cm or 2.365441 m.\n\n\n\n\n\n\n\n\n\nNest - https://www.thespruce.com\n\n\n\n\n\n\n\nTree - https://fennerschool-associated.anu.edu.au\n\n\n\n\n\n\n\n\nOnce we know our data are continuous numeric, we can think can the values be negative? A response variable that is continuous and can be negative is mostly modelled under the most common distribution: Gaussian. Examples of this could be temperature measured in \\(^\\circ\\)C or acceleration. A response variable that cannot be negative or even 0, would most correctly be: Gamma. However, Gaussian models are very often used for these models as a Guassian distribution is simpler computationally and mathematically, and only really causes issues when the values approach 0. This is actually most continuous values we measure, such as thickness of a stem, fish length, body mass or area of habitat.\n\n\n\n\n\n\n\n\n\n\n\n\nIf your response variable is continuous, and it may be negative or not, but is bound between an upper and lower known bound then it would most correctly be: Beta. For most practical terms Beta distribution is between 0 and 1. However, most data between bounds could be scaled to 0 and 1 and keep all of their characteristics. For example, percentage or proportional data are effectively between 0 and 1 and can be modelling using a Beta distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nWe know our data are integers but do we know whether there is an upper limit? If our data could potentially be very big as counts then we will use the Poisson distribution. This is for response variables like number of eggs in a nest or abundance of fish or the number of different species found along a transect. If we know there is an upper limit to the integers we could get then we will use a Binomial model. This could be for the number of eggs inside a six egg carton. The most common version of this is when we have a binary response variable (either 1 or 0). Presence or absence of a rare species for example. This is binary Binomial but is sometimes referred to specifically as Bernoulli.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#third-step-of-analysis---organising-fixed-effects",
    "href": "StatisticsTutorials/IntroductionGLMs.html#third-step-of-analysis---organising-fixed-effects",
    "title": "Introduction GLMs",
    "section": "",
    "text": "We now know what distribution we will most likely use to analyse our response data, so the next step is organising our fixed effects so the model treats them how we would like them to be treated. Effectively fixed effects are either numeric or categorical variables. With GLMs there is no different name (nor changes in code) generally used for what combination of numeric or categorical variables are used. So we could have multiple numeric effects, multiple categorical factors or a mixture of both.\n\n\nIn r numeric effects are quite straight forward, “what is the effect on y with a one unit change of value of x?”. However, if our numeric column has really big values, big range and a skewed distribution we may have issues with our model not working or running as we expect. For fixed effects this is easy to overcome, generally we would use some sort of transformation that maintains the structure of the data but minimises the issues the model runs into.\n\nSkewed Data\n\nFor example for skewed data, if our fixed effect was based on population size with the nearest 2 km, we may have some seriously big values in cities and seriously small values in the countryside. For these data we may use a log transformation, or a square root.\n\n\n\n\n\n\n\n\n\n\nOverly Large Data\n\nIf our fixed effect was a really big value but actually we are interested in small changes we might consider it differently. For example we might look at Year, in our data that would be above 2000 generally but actually if we have 5 years of data we are looking at it would be easier for the model to convert our data to year 1, year 2, year 2 rather than 2020, 2021, 2022. Here we maintain the difference of 1 year between each value so change the scale.\n\n\n\n\n\n\n\n\n\n\nAwkwardly Placed but ‘Nicely’ Distributed Data\n\nSometimes we may have a distribution of values that is ‘nicely’ distributed, with a clear mean, most values centered on that mean with less values further away from that mean. But if that mean is quite large, or really really small, or really really negative we may have issues with models not working correctly. This can also occur when we have more than one fixed effect and they are on very different scales. For example, body measurements as fixed effects where one body measurement is on the scale of metres and another is millimetres. For these we can centered and scale, sometimes call normalise, our effect by subtracting the mean from every value and dividing by the standard deviation.\n\n\n\nCategorical factors can be less simple. R takes the first level of the categorical effect as the baseline or reference state, then compares all others to that one reference level. If you only have two levels then that is fine, the model will tell you the effect of changing from state 1 to state 2. This is directional. However, with more than two levels the results in r will only tell you the pairwise comparisons from state 1 to state 2, state 1 to state 3, state 1 to state 4 …. state 1 to state n. This is harder to interpret in the results table but if you plot the model it becomes easy. It does mean that we can make sure if there is a control or base level in our factor we can use that as our reference level. This is done by ordering our column so that our desired base level is first in the column. If the column is a character style data r will use the first level alphabetically as our reference data. Sometimes we won’t care though, we just have to be aware that it is happening.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#fourth-step-of-analysis---assessing-model-functioning",
    "href": "StatisticsTutorials/IntroductionGLMs.html#fourth-step-of-analysis---assessing-model-functioning",
    "title": "Introduction GLMs",
    "section": "",
    "text": "GLMs and GLMMs have some broad assumptions for them to be appropriately used. Regardless of the distribution type an Independence of sample is assumed as well as homoscedasticity and normality of residuals is assumed.\nIf we have created an appropriate statistical model that takes into consideration the heirarchy/structure of the data our samples should be independent.\nSo what are residuals and what is homoscedasticity?\n\n\n\nTo assess the assumptions of our models we can look at the residual distance between the model line and the points, these are called the residuals of the model.\nIn its most basic version the residuals are the distance between the model and the raw data, then the plot of those residuals against the fitted values (or our response variable) can show how the sizes of residuals change across the fitted values. Homoscedasticity means homogeneity of variance or that the variance is consistent across the whole range of data. Therefore, ideally we want the residuals to represent a cloud of points with no clear patterns across the range of the response variable (fitted values), this is very subjective but a useful qualitative assessment of how good our model is. If our model was missing a clear pattern in the raw data then we would see a clear pattern in the residuals.\nIf we see patterns in our Response Variable vs Residuals plots (not just a random cloud of points) this may mean we missed an important fixed effect in our model (Underlying structure of the system has been ignored), the data do not come from the assigned distribution, there are a high number of zeros in our data or the data do come from the correct distribution but we have been biased in our sampling method (There could be many other reasons but these are the main ones).\n\n\n\n\n\n\n\n\n\n\n\n\nFor the vast majority of GLM and GLMMs, there is an assumption that our residuals are normally distributed. Basically this means that our residuals have a mean distance from the model and most of the values are around this mean. Then the likelihood or probability of having much larger or much smaller values than the mean becomes smaller and smaller. We can therefore plot our residuals as a histogram to see if it follows a generally normal or bell shaped distribution. We can also do what is called a Quantile-Quantile plot, which creates a theoretical distribution of values given the data range, then plots it against the empirical or true distribution. If the residual distribution are perfectly the same as the theoretical distribution it follows a 1 to 1 line (y=x). Again these are more qualitative checks where some variation and divergence from normality is fine.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/IntroductionGLMs.html#fifth-step-of-analysis---model-interpretation",
    "href": "StatisticsTutorials/IntroductionGLMs.html#fifth-step-of-analysis---model-interpretation",
    "title": "Introduction GLMs",
    "section": "",
    "text": "Once we are happy with our model, we can assess what the model tells us about our data. Our interpretation of the model will depend on the statistical model we used and the assumptions we made about our raw data. The best way to interpret a model, once we are happy with it, is to plot the model. To do this we can simulate data that covers the range of our fixed effects, then use the model to predict our response variable for all the values of these fixed effects we simulated. As a final check of how well our model has Generalised the patterns of the raw data we can also plot the raw data alongside our modelled data.\n\n\n\nExample of Model Prediction from Davies et al., 2022. Here points and error bars show raw data and lines with shading show model estimates and 95% confidence intervals.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BetaGLMs.html",
    "href": "StatisticsTutorials/BetaGLMs.html",
    "title": "Beta GLMs",
    "section": "",
    "text": "Here we will use the Proportion of Gasoline yielded from Crude Oil after distillation and fractionation give the gravity of the crude oil, pressure of the crude oil, the temperature (in F) at which 10 percent of the crude oil had vaporised and temperature (in F) when all crude oil had vaporised. First of all, because it makes me uncomfortable, we will convert the temperatures to \\(\\circ\\)C.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(performance)\nlibrary(patchwork)\n\n# install.packages(\"betareg\")\n\nlibrary(betareg)\n\ndata(\"GasolineYield\", package = \"betareg\")\n \nglimpse(GasolineYield)\n\nRows: 32\nColumns: 6\n$ yield    &lt;dbl&gt; 0.122, 0.223, 0.347, 0.457, 0.080, 0.131, 0.266, 0.074, 0.182…\n$ gravity  &lt;dbl&gt; 50.8, 50.8, 50.8, 50.8, 40.8, 40.8, 40.8, 40.0, 40.0, 40.0, 3…\n$ pressure &lt;dbl&gt; 8.6, 8.6, 8.6, 8.6, 3.5, 3.5, 3.5, 6.1, 6.1, 6.1, 6.1, 6.1, 6…\n$ temp10   &lt;dbl&gt; 190, 190, 190, 190, 210, 210, 210, 217, 217, 217, 220, 220, 2…\n$ temp     &lt;dbl&gt; 205, 275, 345, 407, 218, 273, 347, 212, 272, 340, 235, 300, 3…\n$ batch    &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7…\n\ndf&lt;-GasolineYield %&gt;% \n  mutate(temp=(temp-32)*5/9,\n         temp10=(temp10-32)*5/9)\n\n\n\n\nHere we will see if the the temperature that the crude oil totally evaporates and the pressure of the crude oil effects the proportional yield of gasoline from that crude oil.\nThis is a fairly simple model with two fixed effect and can be written as:\nYield of Gasoline ~ Pressure + Temperature\n\n\n\nAs a proportion, Yield of Gasoline can be between 0 and 1, but all real numbers between these upper and lower limits. Therefore it will be a Beta distribution.\n\n\n\nOur data are fairly well distributed across the values\n\np1&lt;-ggplot(df,aes(x=temp))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Temperature (°C)\",y=\"Density\")\n\np2&lt;-ggplot(df,aes(x=pressure))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Pressure\",y=\"Density\")\n\np1+p2\n\n\n\n\n\n\n\n\nAs these are looking fine, we shall fit out model with a Beta distribution. The Beta family is not initiated in GLM so we could use the betareg function from the betareg package. However, for predicting standard error and syntax reasons this is very different from all the GLMs we have already carried out. So we will use the gam() function. This is from the mgcv package and used to model non linear General Additive Models using something called splines. We can use it without using splines and it will behave identically to the glm() function. Infact, for all the GLM tutorials we could have swapped the glm() function for gam(). We will cover GAMs later on but GAMs are GLMs that have smooth terms attached to fixed effects.\n\n# install.packages(\"mgcv\")\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\nglm1 &lt;- gam(yield ~ pressure+temp, data = df, family = betar(link=\"logit\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nThese plots look fairly good, very little patterns in the Fitted vs Residuals and the points generally follow the 1:1 qqnorm plot, with a few points and low and high values not following the line.\n\nsummary(glm1)\n\n\nFamily: Beta regression(119.257) \nLink function: logit \n\nFormula:\nyield ~ pressure + temp\n\nParametric coefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.180079   0.261752  -19.79   &lt;2e-16 ***\npressure     0.174289   0.016830   10.36   &lt;2e-16 ***\ntemp         0.017455   0.001261   13.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.894   Deviance explained = 91.1%\n-REML = -54.103  Scale est. = 1         n = 32\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Yield\\; of\\;  Gasoline = Beta(y',\\phi)\\]\n\\[y'=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Pressure \\\\\n\\beta_{2} Temperature \\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Beta distribution requires two shape parameters (\\(y'\\) and \\(\\phi\\)), where \\(y'\\) must be above zero and below 1, we must convert out linear equation results (\\(y\\)) so that it is bound between 0 and 1. This means we use the link function, which for Beta models is by default logit. We can use a different link function if we want such as probit or clogit. But generally the default is good.\n\nglm1$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nAs always we then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with pressure and temperature the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average yield of gasoline based on those pressures and temperatures.\nWe can tell the predict function to get standard errors. Currently, there are no simple methods for estimating the standard error for a betareg object. This is why we used the gam() function. We will only predict three temperature values to make things easier to plot and we can colour by those temperatures.\n\nNewData_1&lt;-expand_grid(pressure=seq(min(df$pressure),max(df$pressure),length.out=50),\n                      temp=c(100,150,200)\n                      )\n\nPred&lt;-predict(glm1,NewData_1,se.fit=T,type=\"response\")\n\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewData)+\n  geom_ribbon(aes(x=pressure,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    fill=as.factor(temp)),\n              alpha=0.7)+\n  geom_line(aes(x=pressure,\n                 y=response,\n                  colour=as.factor(temp)))+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  labs(x=\"Pressure\",y=\"Response Variable (Yeild of Gasoline)\",\n       fill=\"Temperature (°C)\",colour=\"Temperature (°C)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked. To colour by temperature in the original data we will set groups.\n\ndf_1&lt;-df %&gt;% \n  mutate(temp=case_when(temp&lt;=125~100,\n                              temp&gt;125 & temp&lt;=175~150,\n                              temp&gt;175~200))\n\nggplot(NewData)+\n  geom_point(data=df_1,aes(x=pressure,y=yield,colour=as.factor(temp)))+\n  geom_ribbon(aes(x=pressure,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    fill=as.factor(temp)),\n              alpha=0.7)+\n  geom_line(aes(x=pressure,\n                 y=response,\n                  colour=as.factor(temp)))+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  labs(x=\"Pressure\",y=\"Response Variable (Yeild of Gasoline)\",\n       fill=\"Temperature (°C)\",colour=\"Temperature (°C)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nThis looks quite good. However, again it is based off of very little data, only 32 data points, which could be misleading. However it illustrates our point well and shows that from this data, given our assumptions and understanding of the effects, Yield increases with increasing pressure and also increases with increasing temperature.\n\n\n\n\nThis dataset is a number of reading accuracy scores from children with and without dyslexia as well as their non-verbal IQ scores.\n\n\ndata(\"ReadingSkills\", package = \"betareg\")\n\n\n\nWe want to assess, from this data, if reading accuracy is increase by an individuals non-verbal IQ and if this effect is influenced by them having been diagnosed as dyslexic or not.\nWe can write the stats model as:\nAccuracy ~ IQ*Dyslexia\n\n\n\nHere the accuracy score is a value from 0 to 1, as all accuracy scores have to logically be. Therefore, we will use the Beta distribution.\n\n\n\nLets check all our fixed effects.\n\np1&lt;-ggplot(ReadingSkills,aes(x=dyslexia))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Dyslexia Diagnosis\",y=\"Count\")\n\n\np2&lt;-ggplot(ReadingSkills,aes(x=iq))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Non-Verbal IQ\",y=\"Density\")\n\n\n(p1+p2)\n\n\n\n\n\n\n\n\nThese both look well spread or evenly spread generally. Although less Dyslexic diagnoses.\n\nglm2&lt;-gam(accuracy ~ iq*dyslexia,data=ReadingSkills, family = betar(link=\"logit\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nHere the qqnorm plot is okay, with a bit of under prediction but nothing too wrong. Now these residuals vs fitted values look horrible. However, for homoskedastity in residuals we want a mirror image above and below the 0 line. We are actually getting a pretty good mirror image, our data is just split between two clear groups. This may be an issue that we haven’t come across or we may chose to ignore it. Here we will proceed with caution.\n\nsummary(glm2)\n\n\nFamily: Beta regression(10.455) \nLink function: logit \n\nFormula:\naccuracy ~ iq * dyslexia\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.3205     0.1308  10.098  &lt; 2e-16 ***\niq            0.1582     0.1363   1.160    0.246    \ndyslexia     -0.9626     0.1308  -7.361 1.83e-13 ***\niq:dyslexia  -0.2156     0.1363  -1.582    0.114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.673   Deviance explained =   76%\n-REML = -46.461  Scale est. = 1         n = 44\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Reading \\;Accuracy = Beta(y',\\phi)\\]\n\\[y'=y^{-1}\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} IQ:Dyslexia\\\\\n+ \\beta_{2} Dyslexia\\\\\n+ \\beta_{3} IQ\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Beta distribution requires two shape parameters (\\(y'\\) and \\(\\phi\\)), where \\(y'\\) must be above zero and below 1, we must convert out linear equation results (\\(y\\)) so that it is bound between 0 and 1. This means we use the link function, which for Beta models is by default logit. We can use a different link function if we want such as probit or clogit. But generally the default is good.\n\nglm2$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age and diet the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average weight based on those ages and diets.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(iq=seq(min(ReadingSkills$iq),max(ReadingSkills$iq),length.out=100),\n                       dyslexia=as.factor(c(\"no\",\"yes\")))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=iq,ymax=Upr,ymin=Lwr,fill=dyslexia),\n              alpha=0.6)+\n  geom_line(aes(x=iq,y=response,colour=dyslexia))+\n  labs(x=\"Non-Verbal IQ\",y=\"Predicted Reading Accuracy\",\n       fill=\"Dyslexia\\nDiagnosis\",colour=\"Dyslexia\\nDiagnosis\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=ReadingSkills,aes(x=iq,y=accuracy,colour=dyslexia))+\n  geom_ribbon(aes(x=iq,ymax=Upr,ymin=Lwr,fill=dyslexia),\n              alpha=0.6)+\n  geom_line(aes(x=iq,y=response,colour=dyslexia))+\n  labs(x=\"Non-Verbal IQ\",y=\"Predicted Reading Accuracy\",\n       fill=\"Dyslexia\\nDiagnosis\",colour=\"Dyslexia\\nDiagnosis\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nHere we have some clear differences in reading accuracy seen between dyslexic diagnosis no and yes, but from this model we wouldn’t say this effect changes with non-verbal iq. However, as a topic to analyse Dyslexia is a far more complex subject than these 44 observations.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Beta GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BetaGLMs.html#data-loading---gasoline",
    "href": "StatisticsTutorials/BetaGLMs.html#data-loading---gasoline",
    "title": "Beta GLMs",
    "section": "",
    "text": "Here we will use the Proportion of Gasoline yielded from Crude Oil after distillation and fractionation give the gravity of the crude oil, pressure of the crude oil, the temperature (in F) at which 10 percent of the crude oil had vaporised and temperature (in F) when all crude oil had vaporised. First of all, because it makes me uncomfortable, we will convert the temperatures to \\(\\circ\\)C.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(performance)\nlibrary(patchwork)\n\n# install.packages(\"betareg\")\n\nlibrary(betareg)\n\ndata(\"GasolineYield\", package = \"betareg\")\n \nglimpse(GasolineYield)\n\nRows: 32\nColumns: 6\n$ yield    &lt;dbl&gt; 0.122, 0.223, 0.347, 0.457, 0.080, 0.131, 0.266, 0.074, 0.182…\n$ gravity  &lt;dbl&gt; 50.8, 50.8, 50.8, 50.8, 40.8, 40.8, 40.8, 40.0, 40.0, 40.0, 3…\n$ pressure &lt;dbl&gt; 8.6, 8.6, 8.6, 8.6, 3.5, 3.5, 3.5, 6.1, 6.1, 6.1, 6.1, 6.1, 6…\n$ temp10   &lt;dbl&gt; 190, 190, 190, 190, 210, 210, 210, 217, 217, 217, 220, 220, 2…\n$ temp     &lt;dbl&gt; 205, 275, 345, 407, 218, 273, 347, 212, 272, 340, 235, 300, 3…\n$ batch    &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7…\n\ndf&lt;-GasolineYield %&gt;% \n  mutate(temp=(temp-32)*5/9,\n         temp10=(temp10-32)*5/9)\n\n\n\n\nHere we will see if the the temperature that the crude oil totally evaporates and the pressure of the crude oil effects the proportional yield of gasoline from that crude oil.\nThis is a fairly simple model with two fixed effect and can be written as:\nYield of Gasoline ~ Pressure + Temperature\n\n\n\nAs a proportion, Yield of Gasoline can be between 0 and 1, but all real numbers between these upper and lower limits. Therefore it will be a Beta distribution.\n\n\n\nOur data are fairly well distributed across the values\n\np1&lt;-ggplot(df,aes(x=temp))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Temperature (°C)\",y=\"Density\")\n\np2&lt;-ggplot(df,aes(x=pressure))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Pressure\",y=\"Density\")\n\np1+p2\n\n\n\n\n\n\n\n\nAs these are looking fine, we shall fit out model with a Beta distribution. The Beta family is not initiated in GLM so we could use the betareg function from the betareg package. However, for predicting standard error and syntax reasons this is very different from all the GLMs we have already carried out. So we will use the gam() function. This is from the mgcv package and used to model non linear General Additive Models using something called splines. We can use it without using splines and it will behave identically to the glm() function. Infact, for all the GLM tutorials we could have swapped the glm() function for gam(). We will cover GAMs later on but GAMs are GLMs that have smooth terms attached to fixed effects.\n\n# install.packages(\"mgcv\")\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\nglm1 &lt;- gam(yield ~ pressure+temp, data = df, family = betar(link=\"logit\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nThese plots look fairly good, very little patterns in the Fitted vs Residuals and the points generally follow the 1:1 qqnorm plot, with a few points and low and high values not following the line.\n\nsummary(glm1)\n\n\nFamily: Beta regression(119.257) \nLink function: logit \n\nFormula:\nyield ~ pressure + temp\n\nParametric coefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.180079   0.261752  -19.79   &lt;2e-16 ***\npressure     0.174289   0.016830   10.36   &lt;2e-16 ***\ntemp         0.017455   0.001261   13.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.894   Deviance explained = 91.1%\n-REML = -54.103  Scale est. = 1         n = 32\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Yield\\; of\\;  Gasoline = Beta(y',\\phi)\\]\n\\[y'=logit(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Pressure \\\\\n\\beta_{2} Temperature \\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Beta distribution requires two shape parameters (\\(y'\\) and \\(\\phi\\)), where \\(y'\\) must be above zero and below 1, we must convert out linear equation results (\\(y\\)) so that it is bound between 0 and 1. This means we use the link function, which for Beta models is by default logit. We can use a different link function if we want such as probit or clogit. But generally the default is good.\n\nglm1$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nAs always we then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with pressure and temperature the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average yield of gasoline based on those pressures and temperatures.\nWe can tell the predict function to get standard errors. Currently, there are no simple methods for estimating the standard error for a betareg object. This is why we used the gam() function. We will only predict three temperature values to make things easier to plot and we can colour by those temperatures.\n\nNewData_1&lt;-expand_grid(pressure=seq(min(df$pressure),max(df$pressure),length.out=50),\n                      temp=c(100,150,200)\n                      )\n\nPred&lt;-predict(glm1,NewData_1,se.fit=T,type=\"response\")\n\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96)) \n\nggplot(NewData)+\n  geom_ribbon(aes(x=pressure,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    fill=as.factor(temp)),\n              alpha=0.7)+\n  geom_line(aes(x=pressure,\n                 y=response,\n                  colour=as.factor(temp)))+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  labs(x=\"Pressure\",y=\"Response Variable (Yeild of Gasoline)\",\n       fill=\"Temperature (°C)\",colour=\"Temperature (°C)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked. To colour by temperature in the original data we will set groups.\n\ndf_1&lt;-df %&gt;% \n  mutate(temp=case_when(temp&lt;=125~100,\n                              temp&gt;125 & temp&lt;=175~150,\n                              temp&gt;175~200))\n\nggplot(NewData)+\n  geom_point(data=df_1,aes(x=pressure,y=yield,colour=as.factor(temp)))+\n  geom_ribbon(aes(x=pressure,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    fill=as.factor(temp)),\n              alpha=0.7)+\n  geom_line(aes(x=pressure,\n                 y=response,\n                  colour=as.factor(temp)))+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"forestgreen\"))+\n  labs(x=\"Pressure\",y=\"Response Variable (Yeild of Gasoline)\",\n       fill=\"Temperature (°C)\",colour=\"Temperature (°C)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nThis looks quite good. However, again it is based off of very little data, only 32 data points, which could be misleading. However it illustrates our point well and shows that from this data, given our assumptions and understanding of the effects, Yield increases with increasing pressure and also increases with increasing temperature.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Beta GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BetaGLMs.html#data-loading---dyslexic-reading",
    "href": "StatisticsTutorials/BetaGLMs.html#data-loading---dyslexic-reading",
    "title": "Beta GLMs",
    "section": "",
    "text": "This dataset is a number of reading accuracy scores from children with and without dyslexia as well as their non-verbal IQ scores.\n\n\ndata(\"ReadingSkills\", package = \"betareg\")\n\n\n\nWe want to assess, from this data, if reading accuracy is increase by an individuals non-verbal IQ and if this effect is influenced by them having been diagnosed as dyslexic or not.\nWe can write the stats model as:\nAccuracy ~ IQ*Dyslexia\n\n\n\nHere the accuracy score is a value from 0 to 1, as all accuracy scores have to logically be. Therefore, we will use the Beta distribution.\n\n\n\nLets check all our fixed effects.\n\np1&lt;-ggplot(ReadingSkills,aes(x=dyslexia))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Dyslexia Diagnosis\",y=\"Count\")\n\n\np2&lt;-ggplot(ReadingSkills,aes(x=iq))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()+\n  labs(x=\"Non-Verbal IQ\",y=\"Density\")\n\n\n(p1+p2)\n\n\n\n\n\n\n\n\nThese both look well spread or evenly spread generally. Although less Dyslexic diagnoses.\n\nglm2&lt;-gam(accuracy ~ iq*dyslexia,data=ReadingSkills, family = betar(link=\"logit\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nHere the qqnorm plot is okay, with a bit of under prediction but nothing too wrong. Now these residuals vs fitted values look horrible. However, for homoskedastity in residuals we want a mirror image above and below the 0 line. We are actually getting a pretty good mirror image, our data is just split between two clear groups. This may be an issue that we haven’t come across or we may chose to ignore it. Here we will proceed with caution.\n\nsummary(glm2)\n\n\nFamily: Beta regression(10.455) \nLink function: logit \n\nFormula:\naccuracy ~ iq * dyslexia\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.3205     0.1308  10.098  &lt; 2e-16 ***\niq            0.1582     0.1363   1.160    0.246    \ndyslexia     -0.9626     0.1308  -7.361 1.83e-13 ***\niq:dyslexia  -0.2156     0.1363  -1.582    0.114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.673   Deviance explained =   76%\n-REML = -46.461  Scale est. = 1         n = 44\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Reading \\;Accuracy = Beta(y',\\phi)\\]\n\\[y'=y^{-1}\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} IQ:Dyslexia\\\\\n+ \\beta_{2} Dyslexia\\\\\n+ \\beta_{3} IQ\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Beta distribution requires two shape parameters (\\(y'\\) and \\(\\phi\\)), where \\(y'\\) must be above zero and below 1, we must convert out linear equation results (\\(y\\)) so that it is bound between 0 and 1. This means we use the link function, which for Beta models is by default logit. We can use a different link function if we want such as probit or clogit. But generally the default is good.\n\nglm2$family$link\n\n[1] \"logit\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age and diet the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average weight based on those ages and diets.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(iq=seq(min(ReadingSkills$iq),max(ReadingSkills$iq),length.out=100),\n                       dyslexia=as.factor(c(\"no\",\"yes\")))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=iq,ymax=Upr,ymin=Lwr,fill=dyslexia),\n              alpha=0.6)+\n  geom_line(aes(x=iq,y=response,colour=dyslexia))+\n  labs(x=\"Non-Verbal IQ\",y=\"Predicted Reading Accuracy\",\n       fill=\"Dyslexia\\nDiagnosis\",colour=\"Dyslexia\\nDiagnosis\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=ReadingSkills,aes(x=iq,y=accuracy,colour=dyslexia))+\n  geom_ribbon(aes(x=iq,ymax=Upr,ymin=Lwr,fill=dyslexia),\n              alpha=0.6)+\n  geom_line(aes(x=iq,y=response,colour=dyslexia))+\n  labs(x=\"Non-Verbal IQ\",y=\"Predicted Reading Accuracy\",\n       fill=\"Dyslexia\\nDiagnosis\",colour=\"Dyslexia\\nDiagnosis\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\"))+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Beta GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/BetaGLMs.html#some-caveats-1",
    "href": "StatisticsTutorials/BetaGLMs.html#some-caveats-1",
    "title": "Beta GLMs",
    "section": "",
    "text": "Here we have some clear differences in reading accuracy seen between dyslexic diagnosis no and yes, but from this model we wouldn’t say this effect changes with non-verbal iq. However, as a topic to analyse Dyslexia is a far more complex subject than these 44 observations.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Beta GLMs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "",
    "text": "Bede Ffinian Rowe Davies Post-Doctoral Researcher bedeffinian@gmail.com Laboratoire ISOMer, UFR Sciences et Techniques Nantes University, France.\n\n\n\n\nMy name is Bede Ffinian Rowe Davies a Post-Doctoral Researcher in Marine Ecology and Remote Sensing at the University of Nantes. I am a researcher interested in the large scale changes in marine ecosystems in relation to anthropogenic activities and drivers, using novel techniques alongside traditional methods.\n\n\nThe current project I am taking part in aims to create scaleable and generaliseable classification models to use Spectral Reflectance measured from Satellite imagery to predict spatial extent of intertidal habitats across the whole of Northern Europe. This can assist biodiversity management over great spatial scales while also providing real-time data (assuming regular available satellite imagery).\n\n\n\n\n\n\n\n\nAfter my PhD I was a Post-Doctoral researcher at the University of Plymouth assessing changes in Blue Carbon stores around Jersey in the UK Channel Islands. The project aimed to assess the stored carbon within sedimentary habitats in Jersey. This information was then being used alongside spatial habitat data to create a preliminary estimate of total carbon within sedimentary habitats across Jersey’s territorial waters. Cores and Grabs were taken across Jersey’s waters, then prepared and analysed for their total and organic carbon content with CHN analysis.\n\n\n\nI completed my PhD based at the University of Plymouth. The title of my PhD was: The Effectiveness of Partially Protected Marine Areas for Ecosystem Based Fisheries Management. The work centred around the Marine Protected Area (MPA) in Lyme Bay (UK), which excluded all demersal towed fishing activity across 206km\\(^2\\) of seabed in 2008. University of Plymouth staff and students have monitored the benthic environment across the bay (inside and outside the MPA) annually, using a range of underwater sampling equipment.\nThe methods include:\n\nPassive Acoustic Monitoring (PAM)\nTowed Underwater Video Systems (TUVs)\nBaited Remote Underwater Video Systems (BRUVs)\n\n\nThe objective of my PhD was to assess how the protection affected the benthic ecosystems inside vs outside the MPA.\n\nMy duties included:\n\nthe continued application of annual surveys alongside other members of the team,\norganisation of the 12 year data sets for BRUVs and TUVs, and 5 years of PAM recordings,\nanalysis of BRUVs and PAM data sets,\nstatistical analysis of the time series data,\nassessment of the applicability of the methods,\nthe creation of journal style articles for publication.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\n\n\n\n\n\n\n\n\n\n\n\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation.",
    "crumbs": [
      "Home",
      "About Me",
      "Dr. Bede Ffinian Rowe Davies"
    ]
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "",
    "text": "I have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\n\n\n\n\n\n\n\n\n\n\n\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation.",
    "crumbs": [
      "Home",
      "About Me",
      "Dr. Bede Ffinian Rowe Davies"
    ]
  },
  {
    "objectID": "index.html#projects-1",
    "href": "index.html#projects-1",
    "title": "Dr. Bede Ffinian Rowe Davies",
    "section": "Projects",
    "text": "Projects\nI have worked and assisted in many projects across the UK including:\n\nLong term MPA Assessment\nEcological Impact Assessment of a 50 Year Storm\nFisheries Stock Assessment\nSeagrass Mapping\nWave Energy Device Impact Assessment\nOffshore Aquaculture Impact Assessment\nSedimentary Blue Carbon Assessment\nAcoustic Assessments of Marine Soundscapes.\n\n\n\n\n\n\n\n\n\n\n\n\nFurther afield, I have also worked in location and with data from:\n\nthe Galapagos Islands assessing both Plastic Pollution and Biological Invasive species\nthe Ecuadorian fisheries department (Instituto Nacional de Pesca) asssessing bycatch of protected species\nthe Himalayan mountains and the Ganges River alongside National Geographic researchers assessing plastics.\nthe Greenlandic Arctic Ocean as a Benthic Taxonomist aboard a Fisheries Research Vessel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThrough this work I have collaborated with researchers from across the globe and across many disciplines combining in-the-field research (Underwater videography (BRUVs), boatwork, snorkelling, SCUBA, intertidal estuary surveys) and in the lab (Sediment core and grab preparation). Often, I have then been responsible for data organisation, analysis and presentation.",
    "crumbs": [
      "Home",
      "About Me",
      "Dr. Bede Ffinian Rowe Davies"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/ComparingMLs.html",
    "href": "StatisticsMLTutorials/ComparingMLs.html",
    "title": "Comparing and Ensembling Machine Learning Models",
    "section": "",
    "text": "Lets take some tidy tuesday data (either with the package or with the code) https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-01-25/readme.md. Here we will look at some data on board games and try and predict a board games rating based on some of its information.\n\n#install.packages(\"tidytuesdayR\")\n#library(tidytuesdayR)\n#\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(stacks)\nlibrary(ggpointdensity)\nlibrary(viridis)\n\n#tt_data &lt;- tt_load(\"2022-01-25\")\n\nratings &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv\")\n\nRows: 21831 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, url, thumbnail\ndbl (7): num, id, year, rank, average, bayes_average, users_rated\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndetails &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv\")\n\nRows: 21631 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): primary, description, boardgamecategory, boardgamemechanic, boardg...\ndbl (13): num, id, yearpublished, minplayers, maxplayers, playingtime, minpl...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nratings_joined &lt;- ratings %&gt;%\n  left_join(details, by = \"id\") %&gt;% \n  filter(!yearpublished%in%NA) %&gt;% \n  select(id,average,yearpublished,minplayers,maxplayers,minage,playingtime,minplaytime,maxplaytime)",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Comparing and Ensembling Machine Learning Models"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/ComparingMLs.html#data-splitting",
    "href": "StatisticsMLTutorials/ComparingMLs.html#data-splitting",
    "title": "Comparing and Ensembling Machine Learning Models",
    "section": "Data Splitting",
    "text": "Data Splitting\n\ninit&lt;-initial_split(ratings_joined,strata = average)\n\ntrain&lt;-training(init)\n\nfolds&lt;-vfold_cv(train,strata = average)\n\ntest&lt;-testing(init)",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Comparing and Ensembling Machine Learning Models"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/ComparingMLs.html#create-recipe",
    "href": "StatisticsMLTutorials/ComparingMLs.html#create-recipe",
    "title": "Comparing and Ensembling Machine Learning Models",
    "section": "Create Recipe",
    "text": "Create Recipe\nFirst we want to create a recipe that takes all columns (apart from id) to predict the average ranking. We also square root transform max players as some have huge max players.\n\nrecipe&lt;-recipe(average~.,data=train) %&gt;% \n  update_role(id,new_role=\"id\") %&gt;% \n  step_sqrt(maxplayers, skip = TRUE)",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Comparing and Ensembling Machine Learning Models"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/ComparingMLs.html#create-model-specification",
    "href": "StatisticsMLTutorials/ComparingMLs.html#create-model-specification",
    "title": "Comparing and Ensembling Machine Learning Models",
    "section": "Create Model Specification",
    "text": "Create Model Specification\nThis is the first step where we choose the models we will compare. Lets do a glm, random forest, XGBoost, SVM and KNN.\n\nglm_spec&lt;-linear_reg(\n  penalty = tune(), \n  mixture = tune()) %&gt;% \n  set_mode(\"regression\")%&gt;%\n  set_engine(\"glmnet\")\n\nrf_spec&lt;- rand_forest(\n  mtry = tune(),\n  trees = tune(),\n  min_n = tune()\n) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"ranger\")\n\nxgboost_spec&lt;-boost_tree(\n    trees = tune(),\n    mtry = tune(),\n    min_n = tune(),\n    learn_rate = tune()\n  ) %&gt;%\n  set_mode(\"regression\")%&gt;%\n  set_engine(\"xgboost\") \n\n# install.packages(\"kknn\")\n# install.packages(\"kernlab\")\n\nsvm_spec&lt;-  svm_rbf(\n  cost = tune(), \n  rbf_sigma = tune()\n  ) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"kernlab\")\n\nknn_spec&lt;-nearest_neighbor(neighbors = tune())%&gt;%\n  set_mode(\"regression\") %&gt;% \n  set_engine(\"kknn\")",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Comparing and Ensembling Machine Learning Models"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/ComparingMLs.html#model-hyper-parameter-tuning",
    "href": "StatisticsMLTutorials/ComparingMLs.html#model-hyper-parameter-tuning",
    "title": "Comparing and Ensembling Machine Learning Models",
    "section": "Model Hyper Parameter Tuning",
    "text": "Model Hyper Parameter Tuning\nSo we could train each of these models as separate workflows then assess hyper parameter tuning as follows.\n\nlibrary(tidymodels)\n\nglm_wf&lt;-workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(glm_spec)\n\n\ntune_res_glm &lt;- tune_grid(\n  glm_wf,\n  resamples = folds,\n  grid = 5\n)\n\ntune_res_glm %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  select(mean, penalty, mixture) %&gt;%\n  pivot_longer(penalty:mixture,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %&gt;%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"RMSE\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nBut we will be comparing lots of specs of models so lets make a workflowset and then do the same but with lots of trained models. This will take a long time, so i set the grid to be just 5, really it should be set to around 10, or you can use other methods other than grid search. We will also add (using option_add()) a control to the grid being used and the metric to be assessed (this will be necessary when we want to ensemble the models).\n\nmetric &lt;- metric_set(rmse)\n\nset.seed(234)\n\ndoParallel::registerDoParallel()\n\nall_wf&lt;-workflow_set(\n  list(recipe),\n  list(glm_spec,\n       rf_spec,\n       xgboost_spec,\n       svm_spec,\n       knn_spec)) %&gt;%\n  option_add(\n    control = control_stack_grid(),\n    metrics = metric\n  )\n\nall_res&lt;- all_wf %&gt;% \n  workflow_map(\n    resamples=folds,\n    grid=10\n  )\n\ni Creating pre-processing data to finalize unknown parameter: mtry\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nOnce that finishes running we can compare between all the tuned models.\n\nautoplot(\n   all_res,\n   rank_metric = \"rmse\",  # &lt;- how to order models\n   metric = \"rmse\",       # &lt;- which metric to visualize\n   select_best = TRUE     # &lt;- one point per workflow\n) +\n   geom_text(aes(y = mean - 0.075, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(0.5, 1)) +\n   theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Comparing and Ensembling Machine Learning Models"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/ComparingMLs.html#select-best-model",
    "href": "StatisticsMLTutorials/ComparingMLs.html#select-best-model",
    "title": "Comparing and Ensembling Machine Learning Models",
    "section": "Select Best Model",
    "text": "Select Best Model\nLets select a good model, then see how well it performs on predicting the test data. All models performed fairly well but lets select the recipe_boost_tree model. The best_results object provides us the ‘best’ hyper parameter values for this model framework (the best hyper parameters for the boosted tree models looked at). This is based on a very small amount of tests here, in reality we should use many more combinations of hyper parameters to test the best ones.\n\nbest_results &lt;- all_res %&gt;% \n   extract_workflow_set_result(\"recipe_boost_tree\") %&gt;% \n   select_best(metric = \"rmse\")\n\nbest_results\n\n# A tibble: 1 × 5\n   mtry trees min_n learn_rate .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                \n1     2  1333    39    0.00850 Preprocessor1_Model02",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Comparing and Ensembling Machine Learning Models"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/ComparingMLs.html#assess-ability-of-best-model",
    "href": "StatisticsMLTutorials/ComparingMLs.html#assess-ability-of-best-model",
    "title": "Comparing and Ensembling Machine Learning Models",
    "section": "Assess Ability of ‘Best’ Model",
    "text": "Assess Ability of ‘Best’ Model\nNow lets combine these hyperparrameter values with the workflow, finalise the model and then fit to the initial split of our data. Then we can collect the metrics when this model was applied to the test dataset.\n\nboosting_test_results &lt;- \n   all_res %&gt;% \n   extract_workflow(\"recipe_boost_tree\") %&gt;% \n   finalize_workflow(best_results) %&gt;% \n   last_fit(split = init)\n  \ncollect_metrics(boosting_test_results)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.720 Preprocessor1_Model1\n2 rsq     standard       0.408 Preprocessor1_Model1\n\n\nWe can also plot the predicted verses the true results.\n\nboosting_test_results %&gt;% \n   collect_predictions() %&gt;% \n   ggplot(aes(x = average, y = .pred)) + \n   geom_abline(colour = \"gray50\", linetype = 2) + \n   geom_pointdensity() + \n   coord_obs_pred()+\n  scale_color_viridis() + \n   labs(x = \"Observed\", y = \"Predicted\",colour=\"Density\")+\n   theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Comparing and Ensembling Machine Learning Models"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataVisualisation.html",
    "href": "BasicRTutorials/DataVisualisation.html",
    "title": "Data Visualisation in R",
    "section": "",
    "text": "Lets make some data as we did in the introduction\n\nYear &lt;- seq(from=1950,to=2023,by=1)\nTreatment &lt;- c(\"Control\",\"Treatment 1\",\"Treatment 2\")\nRep&lt;- seq(from=1,to=10,by=1)\n\ndf&lt;-expand.grid(Year=Year,Treatment=Treatment,Rep=Rep)\n\nResponse&lt;-rnorm(n=nrow(df),mean = 15,sd=8) \n\ndf$Response&lt;-Response\n\n\n\nWe can check the data we have using a base plot function\nThe base plot() function will create a grid that is each column of the data set plotted against all the others, this is fine for continuous data (Such as bill_depth_mm)\nbut it is hard to understand or read fully.\n\nplot(df)\n\n\n\n\n\n\n\n\nWe can define x and y axes from columns of the data set\n\nplot(x=df$Year,y=df$Response)\n\n\n\n\n\n\n\n\nIt automatically does a scatter plot, maybe we want to colour different treatments differently? and perhaps some axis labels that are more clear?\n\nplot(x=df$Year,\n     y=df$Response,\n     col=df$Treatment, \n     xlab=\"Year\",\n     ylab=\"Response\")\n\n\n\n\n\n\n\n\nSome of the arguments in this function are well named but you need to know the names of the arguments to properly use a function best,\nyou can easily find documentation of what arguments are within a function by typing two ? before the function in the console like so:\n\n?plot()\n\n# plot is a common function name \n# so we have to go to the base::plot() section of the help\n\nWith the points coloured we should add a legend to the plot, this is accomplished with another function run after the plot function.\n\nplot(x=df$Year,\n     y=df$Response,\n     col=unique(df$Treatment), \n     xlab=\"Year\",\n     ylab=\"Response\",\n     pch=19)\nlegend(x = \"topright\",          # Position\n       legend = unique(df$Treatment),  # Legend texts\n       col = c(1,2, 3),           # point colors\n       pch=19)                    # point type\n\n\n\n\n\n\n\n\nHere we can see that the response is totally random, regardless of treatment and year.\nFor interesting plotting we can change this, we can add patterns and jitter (random noise) to our data based on its treatment.\nWe will use some of our data manipulation code to do this.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf_1&lt;-df %&gt;% \n  mutate(Response_updated=case_when(\n    Treatment==\"Control\"~jitter(Response)*((Year-1930)/5)-50,\n    Treatment==\"Treatment 1\"~jitter(Response)*((Year-1930)*-2)+1000,\n    Treatment==\"Treatment 2\"~jitter(Response)*((Year-1930)*1.5)-4))\n\nWe can now calculate some summary statistics such as mean or standard deviation using group_by and summarise\n\ndf_1_summary&lt;-df_1 %&gt;% \n  group_by(Treatment) %&gt;% \n  summarise(MeanResponse=mean(Response_updated),\n            SDResponse=sd(Response_updated))\n\ndf_1_summary\n\n# A tibble: 3 × 3\n  Treatment   MeanResponse SDResponse\n  &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n1 Control             119.       113.\n2 Treatment 1        -621.      1109.\n3 Treatment 2        1295.       850.\n\ndf_1_summary_year&lt;-df_1 %&gt;% \n  group_by(Treatment,Year) %&gt;% \n  summarise(MeanResponse=mean(Response_updated),\n            SDResponse=sd(Response_updated))\n\n`summarise()` has grouped output by 'Treatment'. You can override using the\n`.groups` argument.\n\ndf_1_summary_year\n\n# A tibble: 222 × 4\n# Groups:   Treatment [3]\n   Treatment  Year MeanResponse SDResponse\n   &lt;fct&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 Control    1950         1.98       29.7\n 2 Control    1951        23.0        36.9\n 3 Control    1952        14.5        33.6\n 4 Control    1953        16.5        24.9\n 5 Control    1954        13.0        59.8\n 6 Control    1955        46.0        38.4\n 7 Control    1956        25.2        35.3\n 8 Control    1957        43.5        39.7\n 9 Control    1958        10.5        58.4\n10 Control    1959        41.8        41.9\n# ℹ 212 more rows\n\n\n\n\n\n\n\n\nFor more complex (and very simple) plots ggplot2 is by far the best package!\nThe gg in ggplot2 and a lot of the associated packages stands for grammar of graphics, which is a very old framework for good visualisation.\nWe don’t need to go far into it, but it effectively relies on three elements:\n\nData\nMapping\nGeometry\n\nIn every plot we will want to plot Data, it may be 1 value, 1 dimensional, 2 dimensional…. all the way up to multi facetted and dimensional.\nWe then want to assign some element of the data to elements of the plot, this is called mapping.\nThis mapping will change based on the geometry (shape/style).\nFor example earlier with base plot we Mapped a column (Data) to be the x axis and another to the y. (base plot assumed our geometry was a scatter plot)\n\n\n\nUsing these principles we can build some very impressive and complicated plots.\nWe first make a blank ggplot saying what data we will include using ggplot().\nFor elements of the plot (aesthetics) that change with data from the df we use the aes() function to show what column we want to map to what element (mapping=aes()).\nThen we want to add layers (geometries) to the plot for what we want to plot and how we want it to look.\n\nggplot(data=df_1,\n       mapping=aes(x=Year,\n                   y=Response_updated,\n                   colour=Treatment))+\n  geom_point()\n\n\n\n\n\n\n\n\nThis looks good but could be nicer! There are many geom types to dictate the type of plot.\nDifferent geom types have different required aesthetic elements, we could check by using ??geom_point for example\nGenerally we use x, y, colour, fill and alpha (transparency). (Sometimes z or label or many others)\nUnlike base plotting ggplot creates a legend automatically, this becomes more complex as the plot becomes more complex but is generally a nice default.\n\nggplot(df_1,aes(x=Response_updated,colour=Treatment))+\n  geom_density()\n\n\n\n\n\n\n\nggplot(df_1,aes(x=Year,y=Response_updated,fill=Treatment))+\n  geom_col(alpha=0.5)\n\n\n\n\n\n\n\nggplot(df_1,aes(x=Treatment,y=Response_updated,fill=Treatment))+\n  geom_boxplot(alpha=0.5)\n\n\n\n\n\n\n\n\nWe can also add new data into the same plot, so we could use the summary data too, for this we add the data to the geom (geometry) we want it used for.\nHere we have both the fill and the colour with the same levels (Treatment), ggplot will automatically combine these into one legend.\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))\n\n\n\n\n\n\n\n\n\n\n\nWe can now change the style and appearance of the whole plot with the function theme()\nThere are also some preset theme functions like theme_classic() or theme_bw()\nWe can also change the appearance with our mapping elements to define the colours or fill we want using the scale_colour_manual() and scale_fill_manual() functions. This is also true for all mapping, the x or y axis can be edited with scale_x/y_*() functions).\nThere are many different scale functions within ggplot so we some times use a * to say that it could be any that we use.\nMaybe we want to set odd x breaks or flip the y axis upside down?\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_x_continuous(breaks = c(1955,1982,2010,2023))+ \n  scale_y_reverse()+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe could even separate plots by another column (facetting)\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  facet_wrap(~Rep, nrow=2)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can add up different layers but also edit all elements of the plot using either the theme() or scale_*_() functions\nThese are all the elements you can change within theme:\ntheme( line, rect, text, title, aspect.ratio, axis.title, axis.title.x, axis.title.x.top, axis.title.x.bottom, axis.title.y, axis.title.y.left, axis.title.y.right, axis.text, axis.text.x, axis.text.x.top, axis.text.x.bottom, axis.text.y, axis.text.y.left, axis.text.y.right, axis.ticks, axis.ticks.x, axis.ticks.x.top, axis.ticks.x.bottom, axis.ticks.y, axis.ticks.y.left, axis.ticks.y.right, axis.ticks.length, axis.ticks.length.x, axis.ticks.length.x.top, axis.ticks.length.x.bottom, axis.ticks.length.y, axis.ticks.length.y.left, axis.ticks.length.y.right, axis.line, axis.line.x, axis.line.x.top, axis.line.x.bottom, axis.line.y, axis.line.y.left, axis.line.y.right, legend.background, legend.margin, legend.spacing, legend.spacing.x, legend.spacing.y, legend.key, legend.key.size, legend.key.height, legend.key.width, legend.text, legend.text.align, legend.title, legend.title.align, legend.position, legend.direction, legend.justification, legend.box, legend.box.just, legend.box.margin, legend.box.background, legend.box.spacing, panel.background, panel.border, panel.spacing, panel.spacing.x, panel.spacing.y, panel.grid, panel.grid.major, panel.grid.minor, panel.grid.major.x, panel.grid.major.y, panel.grid.minor.x, panel.grid.minor.y, panel.ontop, plot.background, plot.title, plot.title.position, plot.subtitle, plot.caption, plot.caption.position, plot.tag, plot.tag.position, plot.margin, strip.background, strip.background.x, strip.background.y, strip.clip, strip.placement, strip.text, strip.text.x, strip.text.x.bottom, strip.text.x.top, strip.text.y, strip.text.y.left, strip.text.y.right, strip.switch.pad.grid, strip.switch.pad.wrap )\nEach one can be edited by its own element, so for a background it is a rectangle so we might say (inside theme()) but for a grid line it is a line.\nplot.background=element_rect(fill=“grey80”) this will mean the rectangular element of the plot background will be filled with the colour grey80\npanel.grid.major.y= element_line(colour = “green”,linetype = “dotdash”,linewidth =2) this will mean the axis lines element of the panel background will be a green dotdashed line of linewidth 2 with the colour grey80\nWe can add titles and subtitles or change the legend title inside the labs function (if we only change the fill title it will split the colour and fill legends),\nWe can even add geoms that are labels or texts or other shapes/lines not base on our data\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\")\n        \n        )\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\")\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1),\n        panel.grid.major.y= element_line(colour = \"green\",linetype = \"dotdash\",linewidth =2)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")+\n  geom_segment(aes(x=1960,xend=2020,y=2500,yend=-2500))\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1),\n        panel.grid.major.y= element_line(colour = \"green\",linetype = \"dotdash\",linewidth =2),\n        panel.grid.minor.x= element_line(colour = \"blue\",linetype = \"solid\",linewidth =3),\n        panel.grid.minor.y= element_line(colour = \"pink\",linetype = \"solid\",linewidth =2.5)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")+\n  geom_segment(aes(x=1960,xend=2020,y=2500,yend=-2500))\n\n\n\n\n\n\n\n\nThe above plot is horrendous but shows different important elements we might want to add or take away from plots.\nOnce we have got a plot to how we want it we can then save it as a file on our computer using the ggsave() function\nto do this we can save our plot as an object, then provide the location we want to save the file and its name and extension in quotations, and its size,\nwe will save to the current directory.\n\nPlot_To_Save&lt;-ggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\nggsave(\"This_Is_Our_First_Saved_Plot.png\", Plot_To_Save, width=10, height=10) # dont forget to put the file type at the end!!! we will use .png",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Data Visualisation in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataVisualisation.html#ggplot2",
    "href": "BasicRTutorials/DataVisualisation.html#ggplot2",
    "title": "Data Visualisation in R",
    "section": "",
    "text": "For more complex (and very simple) plots ggplot2 is by far the best package!\nThe gg in ggplot2 and a lot of the associated packages stands for grammar of graphics, which is a very old framework for good visualisation.\nWe don’t need to go far into it, but it effectively relies on three elements:\n\nData\nMapping\nGeometry\n\nIn every plot we will want to plot Data, it may be 1 value, 1 dimensional, 2 dimensional…. all the way up to multi facetted and dimensional.\nWe then want to assign some element of the data to elements of the plot, this is called mapping.\nThis mapping will change based on the geometry (shape/style).\nFor example earlier with base plot we Mapped a column (Data) to be the x axis and another to the y. (base plot assumed our geometry was a scatter plot)\n\n\n\nUsing these principles we can build some very impressive and complicated plots.\nWe first make a blank ggplot saying what data we will include using ggplot().\nFor elements of the plot (aesthetics) that change with data from the df we use the aes() function to show what column we want to map to what element (mapping=aes()).\nThen we want to add layers (geometries) to the plot for what we want to plot and how we want it to look.\n\nggplot(data=df_1,\n       mapping=aes(x=Year,\n                   y=Response_updated,\n                   colour=Treatment))+\n  geom_point()\n\n\n\n\n\n\n\n\nThis looks good but could be nicer! There are many geom types to dictate the type of plot.\nDifferent geom types have different required aesthetic elements, we could check by using ??geom_point for example\nGenerally we use x, y, colour, fill and alpha (transparency). (Sometimes z or label or many others)\nUnlike base plotting ggplot creates a legend automatically, this becomes more complex as the plot becomes more complex but is generally a nice default.\n\nggplot(df_1,aes(x=Response_updated,colour=Treatment))+\n  geom_density()\n\n\n\n\n\n\n\nggplot(df_1,aes(x=Year,y=Response_updated,fill=Treatment))+\n  geom_col(alpha=0.5)\n\n\n\n\n\n\n\nggplot(df_1,aes(x=Treatment,y=Response_updated,fill=Treatment))+\n  geom_boxplot(alpha=0.5)\n\n\n\n\n\n\n\n\nWe can also add new data into the same plot, so we could use the summary data too, for this we add the data to the geom (geometry) we want it used for.\nHere we have both the fill and the colour with the same levels (Treatment), ggplot will automatically combine these into one legend.\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))\n\n\n\n\n\n\n\n\n\n\n\nWe can now change the style and appearance of the whole plot with the function theme()\nThere are also some preset theme functions like theme_classic() or theme_bw()\nWe can also change the appearance with our mapping elements to define the colours or fill we want using the scale_colour_manual() and scale_fill_manual() functions. This is also true for all mapping, the x or y axis can be edited with scale_x/y_*() functions).\nThere are many different scale functions within ggplot so we some times use a * to say that it could be any that we use.\nMaybe we want to set odd x breaks or flip the y axis upside down?\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_x_continuous(breaks = c(1955,1982,2010,2023))+ \n  scale_y_reverse()+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe could even separate plots by another column (facetting)\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  facet_wrap(~Rep, nrow=2)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can add up different layers but also edit all elements of the plot using either the theme() or scale_*_() functions\nThese are all the elements you can change within theme:\ntheme( line, rect, text, title, aspect.ratio, axis.title, axis.title.x, axis.title.x.top, axis.title.x.bottom, axis.title.y, axis.title.y.left, axis.title.y.right, axis.text, axis.text.x, axis.text.x.top, axis.text.x.bottom, axis.text.y, axis.text.y.left, axis.text.y.right, axis.ticks, axis.ticks.x, axis.ticks.x.top, axis.ticks.x.bottom, axis.ticks.y, axis.ticks.y.left, axis.ticks.y.right, axis.ticks.length, axis.ticks.length.x, axis.ticks.length.x.top, axis.ticks.length.x.bottom, axis.ticks.length.y, axis.ticks.length.y.left, axis.ticks.length.y.right, axis.line, axis.line.x, axis.line.x.top, axis.line.x.bottom, axis.line.y, axis.line.y.left, axis.line.y.right, legend.background, legend.margin, legend.spacing, legend.spacing.x, legend.spacing.y, legend.key, legend.key.size, legend.key.height, legend.key.width, legend.text, legend.text.align, legend.title, legend.title.align, legend.position, legend.direction, legend.justification, legend.box, legend.box.just, legend.box.margin, legend.box.background, legend.box.spacing, panel.background, panel.border, panel.spacing, panel.spacing.x, panel.spacing.y, panel.grid, panel.grid.major, panel.grid.minor, panel.grid.major.x, panel.grid.major.y, panel.grid.minor.x, panel.grid.minor.y, panel.ontop, plot.background, plot.title, plot.title.position, plot.subtitle, plot.caption, plot.caption.position, plot.tag, plot.tag.position, plot.margin, strip.background, strip.background.x, strip.background.y, strip.clip, strip.placement, strip.text, strip.text.x, strip.text.x.bottom, strip.text.x.top, strip.text.y, strip.text.y.left, strip.text.y.right, strip.switch.pad.grid, strip.switch.pad.wrap )\nEach one can be edited by its own element, so for a background it is a rectangle so we might say (inside theme()) but for a grid line it is a line.\nplot.background=element_rect(fill=“grey80”) this will mean the rectangular element of the plot background will be filled with the colour grey80\npanel.grid.major.y= element_line(colour = “green”,linetype = “dotdash”,linewidth =2) this will mean the axis lines element of the panel background will be a green dotdashed line of linewidth 2 with the colour grey80\nWe can add titles and subtitles or change the legend title inside the labs function (if we only change the fill title it will split the colour and fill legends),\nWe can even add geoms that are labels or texts or other shapes/lines not base on our data\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\")\n        \n        )\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\")\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1),\n        panel.grid.major.y= element_line(colour = \"green\",linetype = \"dotdash\",linewidth =2)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")+\n  geom_segment(aes(x=1960,xend=2020,y=2500,yend=-2500))\n\n\n\n\n\n\n\nggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4, shape=8)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",\n       y=\"Response Variable\",\n       fill=\"Treatment New Name\",\n       title= \"What a cool plot Title?!\", \n       subtitle = \"Is it though? The Subtitle is Better.\",\n       caption= \"I think it is cool. So is this Caption.\"\n       )+\n  theme_classic()+\n  theme(plot.background = element_rect(fill=\"grey80\"),\n        panel.background = element_rect(fill=\"yellow\"),\n        panel.grid.major.x= element_line(colour = \"red\",linetype = \"dotted\",linewidth =1),\n        panel.grid.major.y= element_line(colour = \"green\",linetype = \"dotdash\",linewidth =2),\n        panel.grid.minor.x= element_line(colour = \"blue\",linetype = \"solid\",linewidth =3),\n        panel.grid.minor.y= element_line(colour = \"pink\",linetype = \"solid\",linewidth =2.5)\n        \n        )+\n  geom_text(aes(x=1980,y=3000,label=\"this is text\"))+\n  geom_label(aes(x=1980,y=4000,label=\"this is a label (has a background)\"), colour=\"purple\", fill=\"navyblue\")+\n  geom_segment(aes(x=1960,xend=2020,y=2500,yend=-2500))\n\n\n\n\n\n\n\n\nThe above plot is horrendous but shows different important elements we might want to add or take away from plots.\nOnce we have got a plot to how we want it we can then save it as a file on our computer using the ggsave() function\nto do this we can save our plot as an object, then provide the location we want to save the file and its name and extension in quotations, and its size,\nwe will save to the current directory.\n\nPlot_To_Save&lt;-ggplot()+\n  geom_point(data=df_1,mapping=aes(x=Year,y=Response_updated,colour=Treatment),\n             alpha=0.4)+\n  geom_ribbon(data=df_1_summary_year,mapping=aes(x=Year,ymax=MeanResponse+SDResponse,\n                                                 ymin=MeanResponse-SDResponse,fill=Treatment),\n              alpha=0.4)+\n  geom_line(data=df_1_summary_year,mapping=aes(x=Year,y=MeanResponse,colour=Treatment))+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Year\",y=\"Response Variable\")+\n  theme_classic()\n\nggsave(\"This_Is_Our_First_Saved_Plot.png\", Plot_To_Save, width=10, height=10) # dont forget to put the file type at the end!!! we will use .png",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Data Visualisation in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataManipulation.html",
    "href": "BasicRTutorials/DataManipulation.html",
    "title": "Data Manipulation in R",
    "section": "",
    "text": "Packages often have their own example datasets within them, or sometimes a package can be used to store just data without functions etc.\nWe will look at the palmer penguins dataset\n\n#install.packages(\"palmerpenguins\")\n\nlibrary(palmerpenguins)\ndata(penguins)\n\n\n\n\nPenguins Image by Allison Horst\n\n\nThis becomes a ‘promise’ of a data set, we have to do something with it to get it properly, lets take a look inside\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nThis gives us two datasets in our global environment\nUsing summary() we can see which columns have NAs and which don’t.\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nThe penguins data set is fairly well organised but we can still do a bit more with it if we want\n\nsummary(penguins_raw)\n\n  studyName         Sample Number      Species             Region         \n Length:344         Min.   :  1.00   Length:344         Length:344        \n Class :character   1st Qu.: 29.00   Class :character   Class :character  \n Mode  :character   Median : 58.00   Mode  :character   Mode  :character  \n                    Mean   : 63.15                                        \n                    3rd Qu.: 95.25                                        \n                    Max.   :152.00                                        \n                                                                          \n    Island             Stage           Individual ID      Clutch Completion \n Length:344         Length:344         Length:344         Length:344        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    Date Egg          Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm)\n Min.   :2007-11-09   Min.   :32.10      Min.   :13.10     Min.   :172.0      \n 1st Qu.:2007-11-28   1st Qu.:39.23      1st Qu.:15.60     1st Qu.:190.0      \n Median :2008-11-09   Median :44.45      Median :17.30     Median :197.0      \n Mean   :2008-11-27   Mean   :43.92      Mean   :17.15     Mean   :200.9      \n 3rd Qu.:2009-11-16   3rd Qu.:48.50      3rd Qu.:18.70     3rd Qu.:213.0      \n Max.   :2009-12-01   Max.   :59.60      Max.   :21.50     Max.   :231.0      \n                      NA's   :2          NA's   :2         NA's   :2          \n Body Mass (g)      Sex            Delta 15 N (o/oo) Delta 13 C (o/oo)\n Min.   :2700   Length:344         Min.   : 7.632    Min.   :-27.02   \n 1st Qu.:3550   Class :character   1st Qu.: 8.300    1st Qu.:-26.32   \n Median :4050   Mode  :character   Median : 8.652    Median :-25.83   \n Mean   :4202                      Mean   : 8.733    Mean   :-25.69   \n 3rd Qu.:4750                      3rd Qu.: 9.172    3rd Qu.:-25.06   \n Max.   :6300                      Max.   :10.025    Max.   :-23.79   \n NA's   :2                         NA's   :14        NA's   :13       \n   Comments        \n Length:344        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nThe raw data has a lot of extra information that may or may not be important for us. The raw data has lots of difficult to deal with column names.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Data Manipulation in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataManipulation.html#what-is-tidy-data",
    "href": "BasicRTutorials/DataManipulation.html#what-is-tidy-data",
    "title": "Data Manipulation in R",
    "section": "What is ‘Tidy’ data?",
    "text": "What is ‘Tidy’ data?\nFrom the original paper discussing this: “Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).”\nThis means that Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types.\nIn tidy data:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThis may not seem that important or even intelligible now but when we start plotting data it becomes very important.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Data Manipulation in R"
    ]
  },
  {
    "objectID": "BasicRTutorials/DataManipulation.html#enough-theory---dplyr-and-tidyr",
    "href": "BasicRTutorials/DataManipulation.html#enough-theory---dplyr-and-tidyr",
    "title": "Data Manipulation in R",
    "section": "Enough Theory - dplyr and tidyr",
    "text": "Enough Theory - dplyr and tidyr\n \nFor data manipulation and organisation we will rely heavily on the dplyr and tidyr packages, which have a suite of functions that can be used in isolation or combined to perform complex data manipulation and organisation.\n\nEasy to Read Code\nFor writing easy to follow and understand code/scripts with complex sequences of functions, putting our code across multiple lines is a technique we can use.\nThis can be done by doing an enter/carriage return after a comma inside of a function between arguments.\nThis technique changes nothing of how the function works (we can check if the two outputs are equal with all.equal())\n\ndf_without_enters&lt;-data.frame(Column1=c(1.3,5.8,5.122,3.00,7.12),Column2=c(1,5,5,3,7))\n\ndf_with_enters&lt;-data.frame(\n  Column1=c(1.3,5.8,5.122,3.00,7.12),\n  Column2=c(1,5,5,3,7)\n  )\n\nall.equal(df_without_enters,df_with_enters)\n\n[1] TRUE\n\n\n\n\nPiping (Native and maggittr)\n\nWhat do we do when we want to apply multiple functions in a sequence but don’t want to create loads of objects in our global environment?\nOne option is putting one function inside of another etc (Sometimes fine). This is called nesting.\n\nNestingFunctions&lt;-summary(subset(df_with_enters, Column1==1.3))\n\nThis can be okay but generally is hard to follow, as the last function that is applied is the first one you read from left to right.\nIn R there is an operator that allows you to pass the result from one function into the next function this is called the Native Pipe |&gt;\nAgain we can check this creates the same thing with all.equal()\n\nNativePipingingFunctions&lt;-df_with_enters|&gt;\n  subset(Column1==1.3)|&gt;\n  summary()\n\nall.equal(NestingFunctions,NativePipingingFunctions)\n\n[1] TRUE\n\n\nThis operator is actually quite new and was based on another commonly used pipe (and more superior in my mind).\nThe magittr pipe %&gt;% (shift+cmd+m or shift+ctrl+m) was from a package called magittr that is automatically loaded by any tidyverse package.\nIt works very similarly to the native pipe with some subtle changes.\nAgain I feel the magittr pipe makes code that is easier to read.\n\nMaggittrPipingingFunctions&lt;-df_with_enters %&gt;% \n  subset(Column1==1.3) %&gt;% \n  summary()\n\nall.equal(MaggittrPipingingFunctions,NativePipingingFunctions)\n\n[1] TRUE\n\n\nThe pipe can be thought of as “and then”\nSo above, the df_with_enters is subset where Column1 is equal to 1.3 and then the summary function is used.\nWhen using one function it is not needed, but when using multiple functions in a row piping makes code a lot easier to read and understand what order functions have been carried out in.\nAgain this readability has drawbacks in being slower (for small data of 100s of rows this may be 0.0001 of a second but big data 10000000000s of rows it might be a few seconds)\nLater on we will use pipes in long sequences of functions and it will become clearer how useful they are.\n\n\nFiltering\nThe dplyr function filter() is a row wise subsetter, based on a statement from the dataframe.\nWhen we looked at the summary() of penguins we saw some NAs in the biometric columns and also in the sex column.\nIf we want to remove NA’s there are many ways, to be selective we can filter our dataset.\nTo subset data, we create a logic clause that then filters the dataset by that clause/statement,\nFor example if we want to select all rows of the data set where the data is from a female penguin we can do this by:\n\nfemale_penguins&lt;- penguins %&gt;% \n  filter(sex==\"female\")\n\nfemale_penguins\n\n# A tibble: 165 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.5          17.4               186        3800\n 2 Adelie  Torgersen           40.3          18                 195        3250\n 3 Adelie  Torgersen           36.7          19.3               193        3450\n 4 Adelie  Torgersen           38.9          17.8               181        3625\n 5 Adelie  Torgersen           41.1          17.6               182        3200\n 6 Adelie  Torgersen           36.6          17.8               185        3700\n 7 Adelie  Torgersen           38.7          19                 195        3450\n 8 Adelie  Torgersen           34.4          18.4               184        3325\n 9 Adelie  Biscoe              37.8          18.3               174        3400\n10 Adelie  Biscoe              35.9          19.2               189        3800\n# ℹ 155 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNotice there are two =’s!!\nThis is used to create our clause/statement, we filter (keep) the rows of the pengiuns dataset if the sex column contains “females”, if just one equals (=) is used it won’t work.\nOr we might want all the penguins above 5 kg.\n\nheavier_penguins&lt;- penguins %&gt;% \n  filter(body_mass_g&gt;= 5000)\n\nheavier_penguins\n\n# A tibble: 67 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           50            16.3               230        5700\n 2 Gentoo  Biscoe           50            15.2               218        5700\n 3 Gentoo  Biscoe           47.6          14.5               215        5400\n 4 Gentoo  Biscoe           46.7          15.3               219        5200\n 5 Gentoo  Biscoe           46.8          15.4               215        5150\n 6 Gentoo  Biscoe           49            16.1               216        5550\n 7 Gentoo  Biscoe           48.4          14.6               213        5850\n 8 Gentoo  Biscoe           49.3          15.7               217        5850\n 9 Gentoo  Biscoe           49.2          15.2               221        6300\n10 Gentoo  Biscoe           48.7          15.1               222        5350\n# ℹ 57 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThere are a range of symbols we can use such as more than (&gt;), less than (&lt;), more than or equal to (&gt;=), less than or equal to (&lt;=), is equal to (==), and (&), or (|).\nWe can even use multiple clauses or statements in one call to filter,\nSo if we want all the heavier female penguins\n\nheavier_female_penguins&lt;- penguins %&gt;% \n  filter(body_mass_g&gt;= 5000 & sex==\"female\")\n\n\nheavier_female_penguins\n\n# A tibble: 8 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo  Biscoe           45.1          14.5               215        5000\n2 Gentoo  Biscoe           42.9          13.1               215        5000\n3 Gentoo  Biscoe           45.1          14.5               207        5050\n4 Gentoo  Biscoe           49.1          14.8               220        5150\n5 Gentoo  Biscoe           44.9          13.3               213        5100\n6 Gentoo  Biscoe           46.5          14.8               217        5200\n7 Gentoo  Biscoe           50.5          15.2               216        5000\n8 Gentoo  Biscoe           45.2          14.8               212        5200\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSometimes we might want to filter with multiple answers of a categorical variable,\nFor example if we wanted all penguins from Biscoe and Torgersen island\nTo do this we can make a vector of the names we want, then filter by that vector (%in%).\n\nIslands_we_Want&lt;-c(\"Biscoe\",\"Torgersen\")\n\nBiscoe_Torgersen_penguins&lt;- penguins %&gt;% \n  filter(island%in%Islands_we_Want)\n\nBiscoe_Torgersen_penguins\n\n# A tibble: 220 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 210 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHere we will make use of !, this means the opposite of the clause (not this), a subtraction sign can also be used (-).\nWe also use %in% which is used to tell filter there are more than one elements or we can use it for NAs that we want to get rid of as NA is not classed like normal data (It is a lack of data not a character or number).\n\npenguins_someNAs&lt;-penguins %&gt;% \n  filter(!body_mass_g%in%NA)\n\nIf we now look at the number of rows of the datasets we can see only two rows were removed. (not all the NAs)\nWe just removed the rows with NA in the body_mass_g column.\n\nnrow(penguins)\n\n[1] 344\n\nnrow(penguins_someNAs)\n\n[1] 342\n\nsummary(penguins_someNAs)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :151   Biscoe   :167   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :123   Torgersen: 51   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  :  9   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n\n\nStill 9 NAs in sex\n\npenguins_noNAs&lt;-penguins_someNAs %&gt;% \n  filter(!sex%in%NA)\n\nsummary(penguins_noNAs)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :146   Biscoe   :163   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :123   1st Qu.:39.50   1st Qu.:15.60  \n Gentoo   :119   Torgersen: 47   Median :44.50   Median :17.30  \n                                 Mean   :43.99   Mean   :17.16  \n                                 3rd Qu.:48.60   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172       Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190       1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197       Median :4050                Median :2008  \n Mean   :201       Mean   :4207                Mean   :2008  \n 3rd Qu.:213       3rd Qu.:4775                3rd Qu.:2009  \n Max.   :231       Max.   :6300                Max.   :2009  \n\n\nWe might want to remove all rows where an NA is in any of the columns (not always advisable)\n\npenguins_noNAs_quickly&lt;-penguins %&gt;% \n  drop_na()\n\nall.equal(penguins_noNAs,penguins_noNAs_quickly)\n\n[1] TRUE\n\n\n\n\nSelecting\nThe dplyr function select() is a column wise subsetter based on a statement of column names.\nSo we can select or deselect a few named columns using select.\nAnd as with filter we can use - or ! to say not this column/statement.\n\nThree_Columns&lt;-penguins_raw %&gt;% \n  select(studyName,Species,Island)\n\nAll_But_Three_Columns&lt;-penguins_raw %&gt;% \n  select(-studyName,-Species,-Island)\n\nnames(Three_Columns)\n\n[1] \"studyName\" \"Species\"   \"Island\"   \n\nnames(All_But_Three_Columns)\n\n [1] \"Sample Number\"       \"Region\"              \"Stage\"              \n [4] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n [7] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[10] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[13] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\n\nWe can also use a statement for consistencies across columns (contains() or even starts_with() or ends_with())\nFor example all columns that contains() an “s” or even combining a statement with other specific selections\n\nS_Columns&lt;-penguins_raw %&gt;% \n  select(contains(\"s\"))\n\nS_Columns_No_Sex&lt;-penguins_raw %&gt;% \n  select(contains(\"s\"),-Sex)\n\nS_Columns_Plus_Region&lt;-penguins_raw %&gt;% \n  select(contains(\"s\"),Region)\n\nnames(S_Columns)\n\n[1] \"studyName\"     \"Sample Number\" \"Species\"       \"Island\"       \n[5] \"Stage\"         \"Body Mass (g)\" \"Sex\"           \"Comments\"     \n\nnames(S_Columns_No_Sex)\n\n[1] \"studyName\"     \"Sample Number\" \"Species\"       \"Island\"       \n[5] \"Stage\"         \"Body Mass (g)\" \"Comments\"     \n\nnames(S_Columns_Plus_Region)\n\n[1] \"studyName\"     \"Sample Number\" \"Species\"       \"Island\"       \n[5] \"Stage\"         \"Body Mass (g)\" \"Sex\"           \"Comments\"     \n[9] \"Region\"       \n\n\nWe can even make a vector of column names and then pass that vector to select() using the all_of() or any_of() functions.\n\nColumns_We_Want&lt;-c(\"Region\",\"Island\",\"studyName\",\"Stage\")\n\nColumns_From_Vector&lt;-penguins_raw %&gt;% \n  select(all_of(Columns_We_Want))\n\nColumns_Not_From_Vector&lt;-penguins_raw %&gt;% \n  select(-all_of(Columns_We_Want))\n\nnames(Columns_From_Vector)\n\n[1] \"Region\"    \"Island\"    \"studyName\" \"Stage\"    \n\nnames(Columns_Not_From_Vector)\n\n [1] \"Sample Number\"       \"Species\"             \"Individual ID\"      \n [4] \"Clutch Completion\"   \"Date Egg\"            \"Culmen Length (mm)\" \n [7] \"Culmen Depth (mm)\"   \"Flipper Length (mm)\" \"Body Mass (g)\"      \n[10] \"Sex\"                 \"Delta 15 N (o/oo)\"   \"Delta 13 C (o/oo)\"  \n[13] \"Comments\"           \n\n\nAnother nice feature of select is that the order is maintained, so the order of things we select is used to order the columns,\nSo if we want to move a certain column towards the beginning of the df we can do this using select() and put everything() to say everything else after the columns we put first\n\nRegion_First&lt;-penguins_raw %&gt;% \n  select(Region,everything())\n\nRegion_Then_Island_First&lt;-penguins_raw %&gt;% \n  select(Region,Island,everything())\n\nnames(penguins_raw)\n\n [1] \"studyName\"           \"Sample Number\"       \"Species\"            \n [4] \"Region\"              \"Island\"              \"Stage\"              \n [7] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n[10] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[13] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[16] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\nnames(Region_First)\n\n [1] \"Region\"              \"studyName\"           \"Sample Number\"      \n [4] \"Species\"             \"Island\"              \"Stage\"              \n [7] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n[10] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[13] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[16] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\nnames(Region_Then_Island_First)\n\n [1] \"Region\"              \"Island\"              \"studyName\"          \n [4] \"Sample Number\"       \"Species\"             \"Stage\"              \n [7] \"Individual ID\"       \"Clutch Completion\"   \"Date Egg\"           \n[10] \"Culmen Length (mm)\"  \"Culmen Depth (mm)\"   \"Flipper Length (mm)\"\n[13] \"Body Mass (g)\"       \"Sex\"                 \"Delta 15 N (o/oo)\"  \n[16] \"Delta 13 C (o/oo)\"   \"Comments\"           \n\n\n\n\nMutating\nWe have data but maybe we want to transform that data and either replace the original column or create a new column.\nWe can use dplyr’s mutate() to do this.\nLets convert the body mass column into a new column that is in kg.\n\npenguins_kgbodymass&lt;-penguins %&gt;% \n  mutate(body_mass_kg=body_mass_g/1000)\n\nsummary(penguins_kgbodymass)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year       body_mass_kg  \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007   Min.   :2.700  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007   1st Qu.:3.550  \n Median :197.0     Median :4050   NA's  : 11   Median :2008   Median :4.050  \n Mean   :200.9     Mean   :4202                Mean   :2008   Mean   :4.202  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009   3rd Qu.:4.750  \n Max.   :231.0     Max.   :6300                Max.   :2009   Max.   :6.300  \n NA's   :2         NA's   :2                                  NA's   :2      \n\n\nWe can also paste information from other columns together into another new column.\nWe shall use the paste() function then we will put what character we want to separate each element by using the sep argument.\n\npenguins_ExtraInfo&lt;-penguins_noNAs %&gt;% \n  mutate(Info=paste(species,island,sex,sep=\"_\"))\n\nunique(penguins_ExtraInfo$Info)\n\n [1] \"Adelie_Torgersen_male\"   \"Adelie_Torgersen_female\"\n [3] \"Adelie_Biscoe_female\"    \"Adelie_Biscoe_male\"     \n [5] \"Adelie_Dream_female\"     \"Adelie_Dream_male\"      \n [7] \"Gentoo_Biscoe_female\"    \"Gentoo_Biscoe_male\"     \n [9] \"Chinstrap_Dream_female\"  \"Chinstrap_Dream_male\"   \n\n\nWe can even do calculations that are based on and element in another column at the same row.\nThere are a few ways to do this, the simplest is an if_else() statement.\nWith if_else() there are three arguments, the first argument is the statement (is it female), the second argument is what to do if the statement is true and the third argument is what to do if the statement is false.\nLets pretend that when an Adelie penguin was studied they were incorrectly weighed by 200 g.\nWe shall replace the old body_mass_g with new corrected weight, but only for Adelie penguins.\nTo look at the change we will plot a histogram of body weights for both weights.\n\npenguins_if_else&lt;-penguins_noNAs %&gt;% \n  mutate(body_mass_g=if_else(species==\"Adelie\",\n                             body_mass_g+200,\n                             as.numeric(body_mass_g)))\n\nhist(penguins_noNAs$body_mass_g)\n\n\n\n\n\n\n\nhist(penguins_if_else$body_mass_g)\n\n\n\n\n\n\n\n\nWhile fine for one single statement, multiple if_else() statements can create horrible code.\nFor this we can use case_when(),\nWhere we take a statement, then use ~ to say the new column value, then a comma before the next statement\nMaybe Gentoos were also miss-measured but the other way round (too big).\n\npenguins_case_when&lt;-penguins_noNAs %&gt;% \n  mutate(body_mass_g=case_when(species==\"Adelie\"~body_mass_g+200,\n                               species==\"Gentoo\"~body_mass_g-200,\n                               species==\"Chinstrap\"~body_mass_g))\n\nhist(penguins_noNAs$body_mass_g)\n\n\n\n\n\n\n\nhist(penguins_case_when$body_mass_g)\n\n\n\n\n\n\n\n\nWith case_when we have to be careful if all of our statements don’t cover all the data.\nIf there is a condition not covered it will return NA values.\nTo avoid this we can do a final statement with TRUE~Our_Default\n\npenguins_case_when_Missing&lt;-penguins_noNAs %&gt;% \n  mutate(body_mass_g=case_when(species==\"Adelie\"~body_mass_g+200,\n                               species==\"Gentoo\"~body_mass_g-200))\n\npenguins_case_when_TRUE&lt;-penguins_noNAs %&gt;% \n  mutate(body_mass_g=case_when(species==\"Adelie\"~body_mass_g+200,\n                               species==\"Gentoo\"~body_mass_g-200,\n                               TRUE~body_mass_g))\n\nall.equal(penguins_case_when_TRUE,penguins_case_when_Missing)\n\n[1] \"Component \\\"body_mass_g\\\": 'is.NA' value mismatch: 68 in current 0 in target\"\n\n\nThis creates 68 NA values in our data, so need to be aware of this.\nBut we could use it for bug checking.\n\n\nSummarise by Groups\nOften we will want to see summaries of data across groups, using a combination of group_by() and summarise() can give use these summary stats.\nWe can group by one, two or many columns, the more groups the less data will summarised in each group.\nIf we don’t group by a column it will not be in the final dataset.\n\npenguins_noNAs %&gt;% \n  group_by(year) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g))\n\n# A tibble: 3 × 2\n   year Mean_body_mass\n  &lt;int&gt;          &lt;dbl&gt;\n1  2007          4153.\n2  2008          4263.\n3  2009          4200.\n\npenguins_noNAs %&gt;% \n  group_by(year,species) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 9 × 3\n# Groups:   year [3]\n   year species   Mean_body_mass\n  &lt;int&gt; &lt;fct&gt;              &lt;dbl&gt;\n1  2007 Adelie             3714.\n2  2007 Chinstrap          3694.\n3  2007 Gentoo             5100 \n4  2008 Adelie             3742 \n5  2008 Chinstrap          3800 \n6  2008 Gentoo             5028.\n7  2009 Adelie             3665.\n8  2009 Chinstrap          3725 \n9  2009 Gentoo             5157.\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g))\n\n`summarise()` has grouped output by 'year', 'species'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 15 × 4\n# Groups:   year, species [9]\n    year species   island    Mean_body_mass\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;\n 1  2007 Adelie    Biscoe             3620 \n 2  2007 Adelie    Dream              3708.\n 3  2007 Adelie    Torgersen          3785 \n 4  2007 Chinstrap Dream              3694.\n 5  2007 Gentoo    Biscoe             5100 \n 6  2008 Adelie    Biscoe             3628.\n 7  2008 Adelie    Dream              3756.\n 8  2008 Adelie    Torgersen          3856.\n 9  2008 Chinstrap Dream              3800 \n10  2008 Gentoo    Biscoe             5028.\n11  2009 Adelie    Biscoe             3858.\n12  2009 Adelie    Dream              3651.\n13  2009 Adelie    Torgersen          3489.\n14  2009 Chinstrap Dream              3725 \n15  2009 Gentoo    Biscoe             5157.\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island,sex) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g))\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 30 × 5\n# Groups:   year, species, island [15]\n    year species   island    sex    Mean_body_mass\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;\n 1  2007 Adelie    Biscoe    female          3470 \n 2  2007 Adelie    Biscoe    male            3770 \n 3  2007 Adelie    Dream     female          3269.\n 4  2007 Adelie    Dream     male            4102.\n 5  2007 Adelie    Torgersen female          3475 \n 6  2007 Adelie    Torgersen male            4139.\n 7  2007 Chinstrap Dream     female          3569.\n 8  2007 Chinstrap Dream     male            3819.\n 9  2007 Gentoo    Biscoe    female          4619.\n10  2007 Gentoo    Biscoe    male            5553.\n# ℹ 20 more rows\n\n\nWe can also use groups to count numbers of rows within each group.\nAlthough the table() function in base r does the same but it is harder to read and use when lots of columns selected.\n\npenguins_noNAs %&gt;% \n  group_by(year,species,island,sex) %&gt;% \n  summarise(Number=n())\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 30 × 5\n# Groups:   year, species, island [15]\n    year species   island    sex    Number\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt;\n 1  2007 Adelie    Biscoe    female      5\n 2  2007 Adelie    Biscoe    male        5\n 3  2007 Adelie    Dream     female      9\n 4  2007 Adelie    Dream     male       10\n 5  2007 Adelie    Torgersen female      8\n 6  2007 Adelie    Torgersen male        7\n 7  2007 Chinstrap Dream     female     13\n 8  2007 Chinstrap Dream     male       13\n 9  2007 Gentoo    Biscoe    female     16\n10  2007 Gentoo    Biscoe    male       17\n# ℹ 20 more rows\n\npenguins_noNAs %&gt;% \n  select(year,species,island,sex) %&gt;% \n  table()\n\n, , island = Biscoe, sex = female\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      5         0     16\n  2008      9         0     22\n  2009      8         0     20\n\n, , island = Dream, sex = female\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      9        13      0\n  2008      8         9      0\n  2009     10        12      0\n\n, , island = Torgersen, sex = female\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      8         0      0\n  2008      8         0      0\n  2009      8         0      0\n\n, , island = Biscoe, sex = male\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      5         0     17\n  2008      9         0     23\n  2009      8         0     21\n\n, , island = Dream, sex = male\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007     10        13      0\n  2008      8         9      0\n  2009     10        12      0\n\n, , island = Torgersen, sex = male\n\n      species\nyear   Adelie Chinstrap Gentoo\n  2007      7         0      0\n  2008      8         0      0\n  2009      8         0      0\n\n\n\n\nWide and Long Data\nTidy data is generally in what could be considered a long format, where each row is an individual observations often having a column that repeats itself.\nBut for some visualisation tools or for making nice looking tables it might be better to be in wide format.\nLets take some of the summaries from above to create a wide database from our last summary which was hard to read because of its length.\nTo go between wide and long data we will use pivot functions from tidyr, namely pivot_wider() and pivot_longer().\n\n#install.packages(\"tidyr\")\n\nlibrary(tidyr)\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island,sex) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g)) %&gt;% \n  pivot_wider(names_from = species, values_from = Mean_body_mass)\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 18 × 6\n# Groups:   year, island [9]\n    year island    sex    Adelie Chinstrap Gentoo\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1  2007 Biscoe    female  3470        NA   4619.\n 2  2007 Biscoe    male    3770        NA   5553.\n 3  2007 Dream     female  3269.     3569.    NA \n 4  2007 Dream     male    4102.     3819.    NA \n 5  2007 Torgersen female  3475        NA     NA \n 6  2007 Torgersen male    4139.       NA     NA \n 7  2008 Biscoe    female  3244.       NA   4627.\n 8  2008 Biscoe    male    4011.       NA   5411.\n 9  2008 Dream     female  3412.     3472.    NA \n10  2008 Dream     male    4100      4128.    NA \n11  2008 Torgersen female  3519.       NA     NA \n12  2008 Torgersen male    4194.       NA     NA \n13  2009 Biscoe    female  3447.       NA   4786.\n14  2009 Biscoe    male    4269.       NA   5511.\n15  2009 Dream     female  3358.     3523.    NA \n16  2009 Dream     male    3945      3927.    NA \n17  2009 Torgersen female  3194.       NA     NA \n18  2009 Torgersen male    3784.       NA     NA \n\n\nIt is still quite long but we could also add more info into the wider columns (e.g. year or sex)\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island,sex) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g)) %&gt;% \n  pivot_wider(names_from = c(species,year), values_from = Mean_body_mass)\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 6 × 11\n# Groups:   island [3]\n  island sex   Adelie_2007 Chinstrap_2007 Gentoo_2007 Adelie_2008 Chinstrap_2008\n  &lt;fct&gt;  &lt;fct&gt;       &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1 Biscoe fema…       3470             NA        4619.       3244.            NA \n2 Biscoe male        3770             NA        5553.       4011.            NA \n3 Dream  fema…       3269.          3569.         NA        3412.          3472.\n4 Dream  male        4102.          3819.         NA        4100           4128.\n5 Torge… fema…       3475             NA          NA        3519.            NA \n6 Torge… male        4139.            NA          NA        4194.            NA \n# ℹ 4 more variables: Gentoo_2008 &lt;dbl&gt;, Adelie_2009 &lt;dbl&gt;,\n#   Chinstrap_2009 &lt;dbl&gt;, Gentoo_2009 &lt;dbl&gt;\n\npenguins_noNAs %&gt;% \n  group_by(year,species, island,sex) %&gt;% \n  summarise(Mean_body_mass=mean(body_mass_g)) %&gt;% \n  pivot_wider(names_from = c(species,sex), values_from = Mean_body_mass)\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\n\n# A tibble: 9 × 8\n# Groups:   year, island [9]\n   year island    Adelie_female Adelie_male Chinstrap_female Chinstrap_male\n  &lt;int&gt; &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1  2007 Biscoe            3470        3770               NA             NA \n2  2007 Dream             3269.       4102.            3569.          3819.\n3  2007 Torgersen         3475        4139.              NA             NA \n4  2008 Biscoe            3244.       4011.              NA             NA \n5  2008 Dream             3412.       4100             3472.          4128.\n6  2008 Torgersen         3519.       4194.              NA             NA \n7  2009 Biscoe            3447.       4269.              NA             NA \n8  2009 Dream             3358.       3945             3523.          3927.\n9  2009 Torgersen         3194.       3784.              NA             NA \n# ℹ 2 more variables: Gentoo_female &lt;dbl&gt;, Gentoo_male &lt;dbl&gt;\n\n\nOften as ecologists we will be surveying a whole community and counting numbers of each different species at each site.\nThis data often comes to us as wide data, our summary of counts could be turned into a wide df (we will fill NAs as 0s) and we can the convert it back to a long dataframe.\n\nWideCounts&lt;-penguins_noNAs %&gt;% \n  group_by(year,species,island,sex) %&gt;% \n  summarise(Number=n()) %&gt;% \n  pivot_wider(names_from = species,values_from = Number, values_fill = 0)\n\n`summarise()` has grouped output by 'year', 'species', 'island'. You can\noverride using the `.groups` argument.\n\nWideCounts\n\n# A tibble: 18 × 6\n# Groups:   year, island [9]\n    year island    sex    Adelie Chinstrap Gentoo\n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt;     &lt;int&gt;  &lt;int&gt;\n 1  2007 Biscoe    female      5         0     16\n 2  2007 Biscoe    male        5         0     17\n 3  2007 Dream     female      9        13      0\n 4  2007 Dream     male       10        13      0\n 5  2007 Torgersen female      8         0      0\n 6  2007 Torgersen male        7         0      0\n 7  2008 Biscoe    female      9         0     22\n 8  2008 Biscoe    male        9         0     23\n 9  2008 Dream     female      8         9      0\n10  2008 Dream     male        8         9      0\n11  2008 Torgersen female      8         0      0\n12  2008 Torgersen male        8         0      0\n13  2009 Biscoe    female      8         0     20\n14  2009 Biscoe    male        8         0     21\n15  2009 Dream     female     10        12      0\n16  2009 Dream     male       10        12      0\n17  2009 Torgersen female      8         0      0\n18  2009 Torgersen male        8         0      0\n\n\nOkay so we now have a count of different sexes of species of penguins measured in different years and islands\nLets make this data long, to do this we have to tell the function which columns are to be pivoted, and what we want to call the new columns.\nWe can either tell it which columns should or should not be pivoted, or we can even say which position columns to use with numbers. (using the colon means from one thing to the other thing)\n\nLongCounts_1&lt;-WideCounts %&gt;% \n  pivot_longer(c(Adelie,Chinstrap,Gentoo),names_to = \"species\",values_to = \"Number\")\n\nLongCounts_2&lt;-WideCounts %&gt;% \n  pivot_longer(-c(year,island,sex),names_to = \"species\",values_to = \"Number\")\n\nLongCounts_3&lt;-WideCounts %&gt;% \n  pivot_longer(4:6,names_to = \"species\",values_to = \"Number\")\n\nLongCounts_4&lt;-WideCounts %&gt;% \n  pivot_longer(-c(1:3),names_to = \"species\",values_to = \"Number\")\n\nLets check they are all the same to finish off.\n\nall.equal(\n  LongCounts_1,\n  LongCounts_2\n)\n\n[1] TRUE\n\nall.equal(\n  LongCounts_3,\n  LongCounts_4\n)\n\n[1] TRUE\n\nall.equal(\n  LongCounts_1,\n  LongCounts_4\n)\n\n[1] TRUE",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Data Manipulation in R"
    ]
  },
  {
    "objectID": "posts2/Storms.html",
    "href": "posts2/Storms.html",
    "title": "Rewilding of Protected Areas Enhances Resilience of Marine Ecosystems to Extreme Climatic Events",
    "section": "",
    "text": "Link here: Sheehan et al., 2021\nMarine protected areas (MPAs) are employed as tools to manage human impacts, especially fishing pressure. By excluding the most destructive activities MPAs can rewild degraded areas of seabed habitat. The potential for MPAs to increase ecosystem resilience from storms is, however, not understood, nor how such events impact seabed habitats. Extreme storm disturbance impact was studied in Lyme Bay MPA, Southwest United Kingdom, where the 2008 exclusion of bottom-towed fishing from the whole site allowed recovery of degraded temperate reef assemblages to a more complex community. Severe storm impacts in 2013–2014 resulted in major damage to the seabed so that assemblages in the MPA were more similar to sites where fishing continued than at any point since the designation of the MPA; the communities were not dominated by species resistant to physical disturbance. Nevertheless, annual surveys since 2014 have demonstrated that the initial recovery of MPA assemblages was much quicker than that seen following the cessation of chronic towed fishing impact in 2008. Likewise, General Additive Mixed Effect Models (GAMMs) showed that inside the MPA increases in diversity metrics post-Storm were greater and more consistent over time than post-Bottom-Towed Fishing. As extreme events are likely to become more common with climate change, wave exposure observations indicated that 29% of coastal reef MPAs around the United Kingdom may be exposed to comparable wave climate extremes, and may be similarly impacted. This paper therefore provides an insight into the likely extent and magnitude of ecological responses of seabed ecosystems to future extreme disturbance events"
  },
  {
    "objectID": "posts2/LymeSummary.html",
    "href": "posts2/LymeSummary.html",
    "title": "Lessons from Lyme Bay (UK) to inform policy, management, and monitoring of Marine Protected Areas",
    "section": "",
    "text": "Link here: Renn et al., 2024\nThis decade represents a critical period to profoundly rethink human–nature interactions in order to address the interwoven climate and biodiversity crises. Marine Protected Areas (MPAs) demonstrate promise for increasing ecosystem resilience and reversing habitat and population declines, but outcomes vary considerably from context to context. Partially protected areas offer a compromise between ecological recovery and the social needs of local communities, but their success is contingent on an array of factors. This in-depth review summarizes 15 years of marine conservation research and impact in Lyme Bay (southwest UK), to serve as a model for the future adoption of partially protected MPAs. The findings from the UK’s longest integrated socioecological monitoring MPA study are presented and supplemented by an evaluation of the whole-site management approach as a core element of Lyme Bay’s achievements. The journey from research to improved monitoring and ambitious policy is illustrated within and interspersed with stories of novel discoveries, ongoing challenges, and method developments. What started as a dedicated group of community members has grown into an immense collaboration between fishers, scientists, NGOs, and regulators, and their combined efforts have sent ripple effects of positive change across the globe."
  },
  {
    "objectID": "posts2/IntertidalMPB.html",
    "href": "posts2/IntertidalMPB.html",
    "title": "Mapping intertidal microphytobenthic biomass with very high-resolution remote sensing imagery in an estuarine system.",
    "section": "",
    "text": "Link here: Roman et al., 2024\n\nAbstract\nMicrophytobenthos (MPB) contributes significantly to estuarine primary production, so that quantifying its biomass is crucial for assessing their ecosystem functioning. Conventional sampling methods are labour-intensive, logistically challenging, and cannot provide a comprehensive spatial distribution map of MPB biomass. Satellite imagery has offered a feasible alternative for mapping large areas at various temporal and spatial resolutions. However, no imaging device with a spatial resolution consistent with the few square centimetres sampled in-situ has been used in the field. This makes it challenging to accurately relate field biomass measurements with remotely sensed radiometric observations. In this study, two similar multispectral sensors were mounted on an unmanned aerial vehicle (UAV) at different altitudes, as well as on a custom-built device specifically designed to acquire images at ∼1 m altitude, in order to collect very-high spatial resolution reflectance data of MPB biofilms at the Guadalquivir Estuary (Spain) mudflats. In addition, a hyperspectral spectroradiometer acquiring in-situ field reflectance was used for validation. Simultaneously, MPB samples were collected using a 2 mm depth contact corer method, which were analysed through high-performance liquid chromatography (HPLC) to measure the concentrations of major MPB pigments. To assess the relationship between the MPB pigments and different reflectance-based spectral indices, generalised linear mixed effects models (GLMMs) were used, achieving a significant positive relationship between chlorophylls and all spectral indices tested. These models were used to map microphytobenthic biomass, yielding a mean biomass in the range of 30–50 mg Chl-a m−2 in the Guadalquivir estuary during late winter. This study demonstrates the potential of low-altitude/high spatial resolution radiometric imaging as an efficient, rapid, and non-destructive addition to in-situ measurements of MPB biomass, providing exciting perspectives for the monitoring of estuarine systems on a millimetric scale of variability."
  },
  {
    "objectID": "posts2/Ganges_2.html",
    "href": "posts2/Ganges_2.html",
    "title": "The Distribution and Characterisation of Microplastics in Air, Surface Water and Sediment within a Major River System.",
    "section": "",
    "text": "Link here: Napper et al., 2023\nRivers are key pathways for the transfer of microplastics (MP) to marine environments. However, there are considerable uncertainties about the amount of microplastics transported by rivers to the ocean; this results in inaccuracies in our understanding of microplastic quantity and transport by freshwater systems. Additionally, it has been suggested that rivers may represent long-term sinks, with microplastics accumulating in sediment due to their high density or other biological, chemical, and physical factors. The atmosphere is also an important pathway by which airborne microplastics may enter aquatic habitats. Here, we compare for first time microplastics type and concentration in these key environmental mediums (air, water and sediment) along a major river (Ganges), from sea to source to understand 1) the abundance, 2) the spatial distribution, and 3) characteristics. Mean microplastic abundance settling from the atmosphere was 41.12 MP m\\(^2\\) day\\(^{−1}\\); while concentrations in sediment were 57.00 MP kg\\(^{−1}\\) and in water were 0.05 MP L\\(^{−1}\\). Across all sites and environmental mediums, rayon (synthetically altered cellulose) was the dominant polymer (54–82 %), followed by acrylic (6–23 %) and polyester (9–17 %). Fibres were the dominant shape (95–99 %) and blue was the most common colour (48–79 %). Across water and sediment environmental mediums, the number of microplastics per sample increased from the source of the Ganges to the sea. Additionally, higher population densities correlated with increased microplastic abundance for air and water samples. We suggest that clothing is likely to be the prominent source of microplastics to the river system, influenced by atmospheric deposition, wastewater and direct input (e.g. handwashing of clothes in the Ganges), especially in high density population areas. However, we suggest that subsequent microplastic release to the marine environment is strongly influenced by polymer type and shape, with a large proportion of denser microplastics settling in sediment prior to the river discharging to the ocean."
  },
  {
    "objectID": "posts2/OysterTables.html",
    "href": "posts2/OysterTables.html",
    "title": "Mapping intertidal oyster farms using unmanned aerial vehicles (UAV) high-resolution multispectral data",
    "section": "",
    "text": "Link here: Román et al., 2023\nIn France, oyster aquaculture has been historically developed in intertidal zones, with shellfish farming areas covering much of the Atlantic coast. Monitoring these off-bottom cultures where oysters are grown in plastic mesh-bags set on trestle tables is mandatory for the maritime administration to check compliance with a Structural Plan Document (SPD), while also being important for stock assessment in relation to carrying capacity issues. However, traditional monitoring methods are time-consuming, labor-intensive, and inefficient in covering large intertidal areas. In this study, we used a new GIS-based analytical method to assess the potential of high-resolution Unmanned Aerial Vehicle (UAV) multispectral data to retrieve spatial information on oyster-farming structures using Bourgneuf Bay (France) as a case-study. A non-parametric machine learning algorithm was applied to four UAV flight orthomosaics collected at different altitudes (12, 30, 50, and 120 m) to identify oyster mesh-bags. These supervised classifications achieved overall accuracies above 95% for all tested altitudes. In addition, an accurate distinction of oyster-bag mesh sizes (4, 9 and 14 mm) was obtained for 12–50 m flights, but there was a lower accuracy at 120 m. Across all flights, the 4 mm mesh size was the least well detected (72.14% Producer Accuracy). This information can be used to identify bags with specific mesh-sizes used for spat or adult grow out. Finally, we accurately measured oyster table heights using a high-resolution Digital Surface Model (DSM) derived from Structure from Motion (SfM) photogrammetry. The 50 m flight was suggested as the best compromise to obtain precise measurements of the oyster table heights while covering larger areas than lower altitude flights. This demonstrates that UAV technology can provide a set of spatial variables relevant for shellfish farmers and coastal managers in an efficient, rapid, and non-destructive way to monitor the extent and characteristics of oyster-farming areas regularly."
  },
  {
    "objectID": "consultancy.html",
    "href": "consultancy.html",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\n\n\n\n\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects.\n\n\n\n\n\n\n\n\n\n\n\n\nI have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "consultancy.html#statistical-experience",
    "href": "consultancy.html#statistical-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\n\n\n\n\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "consultancy.html#consultancy-fieldwork-experience",
    "href": "consultancy.html#consultancy-fieldwork-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "I have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "consultancy.html#statistical-experience-1",
    "href": "consultancy.html#statistical-experience-1",
    "title": "Ecological and Statistical Consultancy",
    "section": "Statistical Experience",
    "text": "Statistical Experience\nMy statistical experiences have ranged from:\n\n\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "consultancy.html#consultancy-fieldwork-experience-1",
    "href": "consultancy.html#consultancy-fieldwork-experience-1",
    "title": "Ecological and Statistical Consultancy",
    "section": "Consultancy Fieldwork Experience",
    "text": "Consultancy Fieldwork Experience\n\n\nI have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp.",
    "crumbs": [
      "Home",
      "Consultancy Work",
      "Ecological and Statistical Consultancy"
    ]
  },
  {
    "objectID": "blogs/GreenlandTale.html",
    "href": "blogs/GreenlandTale.html",
    "title": "A Greenlandic Tale",
    "section": "",
    "text": "During the summer of 2023 I was invited to be a benthic taxonomist on board the research vessel, Tarajoq, while it undertook fishery surveys of Atlantic Cod and Shrimp for the Greenland Institute of Natural Resources. The opportunity arose through a colleague from the University of Plymouth and was perfectly timed alongside my duties as a post-doctoral researcher at the University of Nantes. In this post I would have many learning experiences, increase my knowledge of fisheries, provide assistance to the annual monitor efforts of Greenland and develop international collaborative connections."
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july",
    "href": "blogs/GreenlandTale.html#th-july",
    "title": "A Greenlandic Tale",
    "section": "10th July",
    "text": "10th July\nToday marks the first day of what I expect will be a spectacular trip. My friend Simon dropped me at the airport with good time. Yet, all the normal anxieties and doubts of going through airport security gripped me, but, as always, there were no issues. We took off from Nantes in glorious sunshine with a gleam of perspiration coating my forehead. This being partly due to my anxiety at flying and partly due to dressing for the Copenhagen climes. It was an easy short flight with quite special views across the Wadden Sea dotted with islets, banks and channels. The clear views and smooth flight quite rapidly turned into a juddering uneasy descent into the city. The approach into Copenhagen, similar to that of Dublin, Hong Kong or Vancouver, seems to be straight into the sea with the runway appearing at the very last second. Once through the airport at the other end the walk to the hotel, which I have been booked into, could not have been easier. Having freshened up and deposited my luggage I quickly turned around and was on the metro into town. I have little to no knowledge of the city but had been given one recommendation. Therefore, I headed straight for “The Little Mermaid” known locally as “Den Lille Havfrue”.\n\n\n\nDen Lille Havfrue - The Little Mermaid\n\n\nIt is a bronze sculpture of a young lady sat in a side-saddle style on a rock on the coast. The ripples of the boats’ wakes brush her feet while the tourists pose, and the pigeons pray on the treats of the unsuspecting crowd. Being a working port, Copenhagen has the industrial concrete factories overlooking the water, while also containing a plethora of quaint small wood or red brick façade houses. This quick sojourn in the Danish capital was an added bonus. The real journey starts tomorrow."
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july-1",
    "href": "blogs/GreenlandTale.html#th-july-1",
    "title": "A Greenlandic Tale",
    "section": "12th July",
    "text": "12th July\nThe approach to Narsarsuaq was the most breathtaking view I have ever seen. Flying in we descended through the low clouds that stretched over the vast north of Greenland. As far as the eye could see were pure white glaciers that gave sharp contrast to the dark crags of rock, towering above even us on our descent. The landing approach brought us gliding in over the fjord where my new home, Tarajoq, sat waiting.\n\n\nDescent into Narsarsuaq\nDescent into Narsarsuaq\n\n\nI had never seen glaciers or icebergs up close before but already within less than 24 hours I had become desensitised. Being formed by glacial movement, I see strong similarities between the Greenland landscape and the glens of the Scottish Highlands. We were alongside (in the harbour) at Narsaq waiting for the last members of the crew to join us. During this time I had the not-so-minor task of familiarising myself with Greenland Fauna.\n\n\nApproaching Tarajoq through icebergs\nApproaching Tarajoq through icebergs\n\n\n\n\n\n\n\n\n\n\n\nRV Tarajoq\n\n\n\n\n\n\n\nIcebergs in the Harbour at Narsaq\n\n\n\n\n\n\n\nIceberg in the Greenlandic Fjords"
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july-2",
    "href": "blogs/GreenlandTale.html#th-july-2",
    "title": "A Greenlandic Tale",
    "section": "16th July",
    "text": "16th July\nLife on board Tarajoq was now starting to fall into a routine, the most breath-taking, stunning and surreal routine but not without numbing fatigue at the end of each shift. The cruise personnel were split into cooks (most important), ‘fish people’ (fisheries scientists), benthos people (us), deck crew, mechanics, helmsmen, skipper and cruise leader. We sailed up the east coast of Greenland, going from predetermined station to predetermined station. At each, we carried out a fishery trawl. All personnel were split onto shifts, so, as a whole, we fished 24 hours a day. My shift was midday to midnight and alongside the fish people we sorted through each catch.\n\n\n\n\n\n\n\n\n\nCute Little (Cottunculus microps)\n\n\n\n\n\n\n\nSorting Different Species of Fish\n\n\n\n\n\n\n\n\n\nA Scary Deep Sea Fish (Caulophryne jordani)\n\n\n\n\n\n\n\nDifferent sizes of Deep Sea Fish (Careproctus reinhardti)\n\n\n\n\n\n\n\nA Rather Large and Bitey Wolf Fish\n\n\n\n\n\nThe fish people took the fish and we took everything else. After a few days the wee beasties we caught were mostly becoming known to me, meaning our job of sorting, identifying and counting was becoming easier. While a lot of what we caught was similar between trawls we were starting to get mind-blowingly cool species and specimens too.\n\n\n\n\n\n\n\n\n\nIntricate Basket Star (Gorgonocephalus sp.)\n\n\n\n\n\n\n\nSelection of Many Armed Starfish (Crossaster sp.)\n\n\n\n\n\n\n\n\n\nA Phakellia Sponge plus Me\n\n\n\n\n\n\n\nExample Tray of Benthic Specimens\n\n\n\n\n\n\n\nSea Spider (Pycnogonida sp.)\n\n\n\n\n\nSo far, we had caught and released with a tag, 3 Greenland sharks. At over 4 metres long they will all have been very old. When I say old, I cannot be sure, no one is, but it is likely they are older than the USA, maybe even alive during the Tudor times in the UK. Other amazing specimens of fish had also turned up, including multiple 30 + kg Cod, Halibut, Pollack and Rays.\n\n\n\n\n\n\n\n\n\nA Massive Greenlandic Shark (Somniosus microcephalus)\n\n\n\n\n\n\n\n\n\nA 32 kg Cod (Gadus morhua)\n\n\n\n\n\n\n\nA 28 kg Halibut (Hippoglossus hippoglossus)\n\n\n\n\n\n\n\nA 20 kg Pollack (Polliachus virens)\n\n\n\n\n\n\n\n\n\nA 24 kg Flapper Skate (Dipturus batis)\n\n\n\n\n\nOnce my shift was done, especially after processing 700kg of sponge by hand, I was in need of a cold dip. While swimming in the sea was out of the question, we did have a huge dunk tank and a tap that drew sea water at around 4°C from the water around us. This allowed cold dips before turning in for the night.\n\n\n\n\n\n\n\n\n\nA Trawl full of Sponges (Geodia sp.)\n\n\n\n\n\n\n\nMy “Swimming Pool”\n\n\n\n\n\nWe were skirting around but also through vast ice flows that were both spectacular to see in the day and terrifying to hear in the night, as we smashed our path through them.\n\n\nThe Bow of Tarajoq Gliding through the Water.\nThe Bow of Tarajoq Gliding through the Water."
  },
  {
    "objectID": "blogs/GreenlandTale.html#nd-july",
    "href": "blogs/GreenlandTale.html#nd-july",
    "title": "A Greenlandic Tale",
    "section": "22nd July",
    "text": "22nd July\nThe routine continued: wake up; take a multivitamin; grab a coffee and head to the bridge; admire either the beautiful vistas or rolling waves, whale blows, icebergs and Greenlandic mountains or feel the chill of fear looking out into impenetrable fog with icebergs looming out of the nothingness; then it would be time to eat before descending to the factory to sort, identify and count all species we caught within the trawl.\n\n\n\n\n\n\n\n\n\nBoat Lights Searching Through the Fog.\n\n\n\n\n\n\n\nImpenetrable Fog in the Middle of the Day\n\n\n\n\n\nAs we headed north along the eastern shelf of Greenland we could feel the cold intensifying. This was most apparent during my evening “swims”. The “swimming pool” was a reconstituted fishing box, previously used to store fish on ice to keep fresh; now it was filled daily with sea water of around 1-6 °C and sat in by myself and others of the crew. While the shock to the system seems extreme, the serenity I felt being sat in near-freezing water after a 12 hour shift spent lifting and moving (often entirely by hand) up to 4,500 kg of sponge was so relaxing, revitalising and reinvigorating. This cold dip then helped, while Tarajoq rocked with the swell, to send me off to the most tranquil, unbroken sleep I have ever experienced. The trawls, apart from literal tonnes of sponges, have yielded an amazing range of species from ancient Greenland sharks, monster cod and halibut to intricate deep sea Paragorgia corals. So far, the highlights had been the cute and sassy bobtail squids (Rossia sp.), grumpy octopuses (Bathypolypus sp.) and a small star covered smoothhound (Squalus acanthius). The trawls had even yielded long dead specimens: the vertebrae and a rib from some sort of whale.\n\n\n\n\n\n\n\n\n\nA sample of Deep Sea Coral (Paragorgia)\n\n\n\n\n\n\n\nA Small Squid (Rossia sp.)\n\n\n\n\n\n\n\nA Grumpy Octopus (Bathypolypus sp.)\n\n\n\n\n\n\n\n\n\nA Starry Smoothound (Squalus acanthius) about to be tagged.\n\n\n\n\n\n\n\nA Single Whale Vertebrae\n\n\n\n\n\nIn between trawls we got up to an hour of break, if the previous trawl didn’t catch too much. In that time, I took to sitting on top of the spare trawl nets, which were stored on the upper decks, looking out to sea, just waiting for the blow of a whale or the passing flight of the many sea birds that share these fishing grounds. We were gliding across almost gloss smooth waters but this was just “the calm before the storm”.\n\n\n\nMy View from the Spare Trawl Nets.\n\n\nThe cook told me we were about to meet a summer storm. If this happened the skipper’s job would be to decide where to weather the storm. As we wouldn’t be able to fish during the storm we would need to hide either inshore in one of the many fjords along Greenland’s east coast or head further offshore in deeper, calmer waters. Both scenarios would have had their advantages and disadvantages but only once the storm arrived could the decision be made.\n\n\nThe beginning of Stormy Seas.\nThe beginning of Stormy Seas."
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july-3",
    "href": "blogs/GreenlandTale.html#th-july-3",
    "title": "A Greenlandic Tale",
    "section": "27th July",
    "text": "27th July\nThankfully the weather forecasts were correct, and a quick steam up north meant we skirted the storm, avoiding the worst of the weather. That being said, we still were exposed to the odd 5-10 metre wave with consistent 4 metre rolling swells for 3 days. The rocking motion made my sleep even better, although I know many who barely slept a wink. All crew and researchers got into the habit of continuously swaying, pre-emptively moving with the ocean rather than against it. As the storm abated and the weather cleared it allowed for one of the most vividly coloured sunsets I have ever seen. The whole world turned to sepia in the orange-red glow, giving the appearance of a dramatic fire raging just out of view. With it the sunset brought thin streaks of purple cloud, forming wisps of colour across the expanse of clear blue sky. After days that felt like weeks of close fog and clouds, the clear skies all the way to the horizon gave a feeling of relief. The sea was beginning to calm, although it still held some of the energy of the storm.\n\n\n\n\n\n\n\n\n\nBeginning of Sunset after Stormy Seas.\n\n\n\n\n\n\n\nSunset Streaked sky.\n\n\n\n\n\nThis calming gave way to the now regular spouts of air and water signifying the continual migration, movement and searching for food by the biggest animals to ever live. While a professional or even an avid amateur can tell the whale from the shape, size and frequency of its breaths, I could not The only species easy to identify, which whalers took great advantage of, is the sperm whale. The elongated head and offset nostril means a sperm whales spout shoots out at 45 degrees forward and off to one side. I had now seen many spouts of air, many off to one side, yet apart from the momentary glimpse of a tail or back, I hadn’t seen any of these whales up close.\n\n\nA Whales Spout alongside Sunset.\nA Whales Spout alongside Sunset.\n\n\nMy day-to-day shift continued as before with the odd species alluding me, but most were becoming well known to me. Alongside our fishing trawls we were also deploying benthic grabs, which take a grab of all species in the sediment, and video camera sleds, which in the same way as the trawl, is dragged along the seabed and captures all in its path. However, it captures organisms in their natural habitat and more importantly captures them only on film. Comparing the video, grab and trawl results side-by-side allowed us to see both the differences and similarities between the methods. As we came to the end of the survey each trawl became more special as I savoured each new identification. Once the survey was finished our next adventure would be the crossing to Iceland. The fine and clear forecast boded well for an easy crossing and while I was keen to see Iceland it would mean an end to this trip, which I was far less keen on.\n\n\n\n\n\n\n\n\n\nA Benthic Grab.\n\n\n\n\n\n\n\nThe Video Trawl."
  },
  {
    "objectID": "blogs/GreenlandTale.html#th-july-4",
    "href": "blogs/GreenlandTale.html#th-july-4",
    "title": "A Greenlandic Tale",
    "section": "29th July",
    "text": "29th July\nToday marked the day when all sampling sites had been sampled. As with most fieldwork this trip included a few days of contingency for bad weather or equipment failure. However, through luck, efficient planning and hard work we had finished early. We still had other duties, such as cleaning, equipment maintenance, as well as trying to tag as many Cod (Gadus morhua) as possible. During this increased downtime we were sailing along the Greenlandic coast, over calm seas, under clear skies and alongside a vast array of glacial mountainsides. This route brought us closer to more icebergs, which drifted along beside us, often following our wake, driven by the currents on the surface and at other times moving at odds to local wind, waves and currents.\n\n\n\nThe Flat Waters with a Lone Gigantic Iceberg\n\n\nBeing closer in to the shore brought us nearer to the playgrounds of a range of species. Unlike when we were offshore, surrounded by Fullmars and the occasional spout of migrating whales, inshore we saw a variety of seabirds and larger pods of whales. Gannets, which I had not spotted so far, now circled us, occasionally pausing and then plummeting into the cold waves, and Petrels flitted mere centimetres above the lightly moving water, giving the illusion of dancing across the peaks and troughs of the barely shifting swell. With the days of contingency not wanting to be wasted, the ships primary goal was to find cod. Up until that point we had been trawling specific stations to assess, in a standardised way, the distribution of this highly prized fish. Now we were on the hunt. If we could catch a large quantity we could insert tags into the fish that means when that individual is caught we can see where it ends up. Each tag was a small plastic streamer with a unique ID number. This means any fisher who catches a tagged fish can tell the Greenlandic fisheries institute where and when it was caught. These types of passive tracking can allow researchers to find out where fish go and when they go there, to better understand and therefore manage these fish.\n\n\n\nThe Cod Tagger with Tags"
  },
  {
    "objectID": "blogs/GreenlandTale.html#st-july",
    "href": "blogs/GreenlandTale.html#st-july",
    "title": "A Greenlandic Tale",
    "section": "31st July",
    "text": "31st July\nAfter two and a half days, still working on 12 hour shifts, we had managed to tag over 2,000 cod. The work was short and intense; when a trawl came up, if it has cod we tagged them. If no cod came up then the trawl was emptied, reset and redeployed. When the trawl was full of cod we jumped to action stations. Firstly, all cod in the trawl were separated from the rest of the catch and placed in a holding tank. These were then left and monitored for health and survival for half an hour. Alive individuals were selected from the tank, laid out on a measuring tape to be recorded, a small plastic tag inserted into the cod alongside the dorsal fin and straight away released back into the sea.\n\n\nThe Process of Tagging a Cod\nThe Process of Tagging a Cod\n\n\nThis method was the same for all species we tagged throughout the trip, with the exception of the Greenland sharks, which were measured, tagged and released on deck as quickly as possible, to minimise time out of water and therefore stress to the animal.\n\n\n\n\n\n\n\n\n\nTagging the Starry Smoothound (Squalus acanthius)\n\n\n\n\n\n\n\n\n\nA Greenlandic Shark being Tagged and Measured on Deck\n\n\n\n\n\nHowever, the last trawl for this trip had come up; the last cod tagged; the last organism identified.\n\n\n\nThe Last Trawl Coming onto Deck.\n\n\nSometime that day we would set sail for Iceland, crossing the Denmark strait and bound for Reykjavik. The crossing would take around 24 hours and we cleaned and tidied to keep busy. As the shift structure ended for all scientists, we played a game of ‘who can stay up latest’ for the night shifters and ‘who can wake up earliest’ for the day shifters. Over the last few days, we had slowly been transitioning to Icelandic time from Greenlandic time. With the last trawl came two huge and beautiful Greenland sharks.\n\n\n\nThe Two Greenlandic Sharks from the Last Trawl.\n\n\nOver the last 21 days at sea we had caught 7 of these slow, hardy giants, yet, I was still in awe of them every time. Their size and age just made me wonder what had happened over their lifetimes: what ships had sailed above them? Unbeknownst to them, did they cross the path of the Titanic or witness the U-boats of World War 2? With a potential age of more than 500 years, did they even see the Niña, Pinta and Santa Maria as they sailed to discover the people already living in North America? Very unlikely given the route Columbus took but the incredible age of these creatures and their relatively unknown behaviour highlights how little we still know about our oceans.\n\n\n\nA Greenlandic Shark."
  },
  {
    "objectID": "blogs/GreenlandTale.html#rd-august",
    "href": "blogs/GreenlandTale.html#rd-august",
    "title": "A Greenlandic Tale",
    "section": "3rd August",
    "text": "3rd August\nIt was my last night in the North. We landed in Iceland in Hafnarfjörður the day before and, after being cleared by customs, were allowed to get our feet on dry land. After almost 3 weeks on board, being ashore was a big shock. It was interesting to realise how continuously noisy the boat had been: the constant thrum of the engines, the crashing of waves, the booming sound of our hull hitting icebergs. All these noises just faded into the background. This made the still, calm, quiet of land feel almost eerie, perhaps too quiet! Hafnarfjörður is only 6 km south of Reykjavik, so we quickly made the short trip to the capital. After visiting the magnificent cathedral, which my sister tells me was informed by the ‘post-industrial rationalist movement in Holland and Southern Germany’, we decided to wander around Reykjavik but somehow got continually waylaid by liquid refreshment. The dry ship rules of the three weeks had produced a healthy thirst. From some extensive tasting I could conclude that the brewing culture in Reykjavik is amazing, the only drawback being the eye-watering price tag for a pint: £8-£10!!\n\n\n\nA Pint Outside in the Reykjavik Sun.\n\n\nThis was my last night as a scientist on board the research vessel Tarajoq. My travel home would see me fly from Reykjavik to Paris then catch a train from Paris back home to Nantes. I would be lying if I said I wasn’t keen to be heading home but it would also be a huge lie to say I hadn’t loved this trip. It kept me on my toes with hundreds of new experiences. I had been so active carrying baskets of sponges to identify; washing down the grab we had used to sample mud from over five hundred metres depth; helping tag cod or counting and recording hundreds of jellyfish at a time. When not physically active the intellectual tasks had been just as rewarding, working out the taxonomic identities of different deep-sea spiders, squid, starfish or any number of other groups of species. The work itself had been extremely fun, with brand new species almost every trawl.\n\n\n\nMy Last View of Tarajoq.\n\n\nHowever, as fun and interesting as all these elements had been, the overwhelming comfort I felt at being on a boat, at sea, travelling across the great expanse of the ocean outstrips them all. It makes me realise that wherever I am, whatever I do, it must include being on, in or under the sea.\n\n\n\nA Moment of Contemplation? Or Exhaustion?\n\n\n\n\n\n\nDen Lille Havfrue - The Little Mermaid Statue in Copenhagen Harbour.\n\n\nApproach into Narsarsuaq descending over mountains, glaciers and fjords.\n\n\nApproaching Tarajoq through icebergs across the fjord.\n\n\nResearch Vessel Tarajoq as I approached it to board.\n\n\nIcebergs drifting past the town of Narsaq.\n\n\nIceberg in the Greenlandic Fjords between Narsarsuaq and Narsaq"
  },
  {
    "objectID": "posts2/Mussels.html",
    "href": "posts2/Mussels.html",
    "title": "The restoration potential of offshore mussel farming on degraded seabed habitat",
    "section": "",
    "text": "Link here: Bridger et al., 2022\nThe United Kingdom’s first large-scale, offshore, long-line mussel farm deployed its first ropes in 2013 in Lyme Bay, southwest United Kingdom, located in an area of seabed that was heavily degraded due to historic bottom-towed fishing. It was hypothesised that due to the artificial structures that accumulate mussels and exclude destructive fishing practices, the seabed could be restored. To assess the restoration potential of the farm and its ecosystem interactions over time, a multi-method, annual monitoring approach was undertaken. Here, we tested the effects of the farm trial stations on the seabed habitat, epifauna and demersal species over 5 years. Responses of % mussel cover, sessile and sedentary, and mobile taxa were measured using three video methods. Within 2 years of infrastructure deployment, mussel clumps and shells were detected below the headlines, increasing the structural complexity of the seabed. After 4 years, there was a significantly greater abundance of mobile taxa compared to the Controls that remained open to trawling. Commercial European lobster and edible crab were almost exclusively recorded within the farm. We discuss whether these findings can be considered a restoration of the seabed and how these data can be used to inform the future management of offshore mariculture globally."
  },
  {
    "objectID": "posts2/DISCOV.html",
    "href": "posts2/DISCOV.html",
    "title": "Discriminating Seagrasses from Green Macroalgae in European Intertidal Areas Using High-Resolution Multispectral Drone Imagery",
    "section": "",
    "text": "Link here: Oiry et al., 2024\n\nAbstract\nCoastal areas support seagrass meadows, which offer crucial ecosystem services, including erosion control and carbon sequestration. However, these areas are increasingly impacted by human activities, leading to habitat fragmentation and seagrass decline. In situ surveys, traditionally performed to monitor these ecosystems, face limitations on temporal and spatial coverage, particularly in intertidal zones, prompting the addition of satellite data within monitoring programs. Yet, satellite remote sensing can be limited by too coarse spatial and/or spectral resolutions, making it difficult to discriminate seagrass from other macrophytes in highly heterogeneous meadows. Drone (unmanned aerial vehicle—UAV) images at a very high spatial resolution offer a promising solution to address challenges related to spatial heterogeneity and the intrapixel mixture. This study focuses on using drone acquisitions with a ten spectral band sensor similar to that onboard Sentinel-2 for mapping intertidal macrophytes at low tide (i.e., during a period of emersion) and effectively discriminating between seagrass and green macroalgae. Nine drone flights were conducted at two different altitudes (12 m and 120 m) across heterogeneous intertidal European habitats in France and Portugal, providing multispectral reflectance observation at very high spatial resolution (8 mm and 80 mm, respectively). Taking advantage of their extremely high spatial resolution, the low altitude flights were used to train a Neural Network classifier to discriminate five taxonomic classes of intertidal vegetation: Magnoliopsida (Seagrass), Chlorophyceae (Green macroalgae), Phaeophyceae (Brown algae), Rhodophyceae (Red macroalgae), and benthic Bacillariophyceae (Benthic diatoms), and validated using concomitant field measurements. Classification of drone imagery resulted in an overall accuracy of 94% across all sites and images, covering a total area of 467,000 m2. The model exhibited an accuracy of 96.4% in identifying seagrass. In particular, seagrass and green algae can be discriminated. The very high spatial resolution of the drone data made it possible to assess the influence of spatial resolution on the classification outputs, showing a limited loss in seagrass detection up to about 10 m. Altogether, our findings suggest that the MultiSpectral Instrument (MSI) onboard Sentinel-2 offers a relevant trade-off between its spatial and spectral resolution, thus offering promising perspectives for satellite remote sensing of intertidal biodiversity over larger scales."
  },
  {
    "objectID": "posts2/Ganges.html",
    "href": "posts2/Ganges.html",
    "title": "The Abundance and Characteristics of Microplastics in Surface Water in the Transboundary Ganges River",
    "section": "",
    "text": "Link here: Napper et al., 2021\nMicroplastics (plastic &lt; 5 mm in size) are now known to contaminate riverine systems but understanding about how their concentrations vary spatially and temporally is limited. This information is critical to help identify key sources and pathways of microplastic and develop management interventions. This study provides the first investigation of microplastic abundance, characteristics and temporal variation along the Ganges river; one of the most important catchments of South Asia. From 10 sites along a 2575 km stretch of the river, 20 water samples (3600 L in total) were filtered (60 samples each from pre- and post-monsoon season). Overall, 140 microplastic particles were identified, with higher concentrations found in the pre-monsoon (71.6%) than in post-monsoon (61.6%) samples. The majority of microplastics were fibres (91%) and the remaining were fragments (9%). We estimate that the Ganges, with the combined flows of the Brahmaputra and Meghna rivers (GBM), could release up to 1–3 billion (10\\(^9\\)) microplastics into the Bay of Bengal (north-eastern portion of the Indian Ocean) every day. This research provides the first step in understanding microplastic contamination in the Ganges and its contribution to the oceanic microplastic load."
  },
  {
    "objectID": "posts2/BrentGoose.html",
    "href": "posts2/BrentGoose.html",
    "title": "Remote sensing in seagrass ecology: coupled dynamicsbetween migratory herbivorous birds and intertidalmeadows observed by satellite during four decades",
    "section": "",
    "text": "Link here: Zoffoli et al., 2022\nTaking into account trophic relationships in seagrass meadows is crucial toexplain and predict seagrass temporal trajectories, as well as for implementingand evaluating seagrass conservation policies. However, this type of interactionhas been rarely investigated over the long term and at the scale of the wholeseagrass habitat. In this work, reciprocal links between an intertidal seagrassspecies,Zostera noltei, and a herbivorous bird feeding on this seagrass species,the migratory gooseBranta bernicla bernicla, were investigated using an originalcombination of long-term Earth Observation (EO) and bird census data. Sea-grass Essential Biodiversity Variables (EBVs) such as seagrass abundance andphenology were measured from 1985 to 2020 using high-resolution satelliteremote sensing over Bourgneuf Bay (France), and cross-analysed within situmeasurements of bird population size during the goose wintering season. Ourresults showed a mutual relationship between seagrass and Brent geese over thefour last decades, suggesting that the relationship between the two speciesextends beyond a simple grass—herbivore consumptive effect. We provided evi-dence of two types of interactions: (i) a bottom-up control where the late-summer seagrass abundance drives the wintering population of herbivorousgeese and (ii) an indirect top-down effect of Brent goose on seagrass habitat,where seagrass development is positively influenced by the bird population dur-ing the previous wintering season. Such a mutualistic relationship has strongimplications for biodiversity conservation because protecting one species isbeneficial to the other one, as demonstrated here by the positive trajectoriesobserved from 1985 to 2020 in both seagrass and bird populations. Importantly,we also demonstrated here that exploring the synergy between EO andin situbird data can benefit seagrass ecology and ecosystem management."
  },
  {
    "objectID": "posts2/Everest.html",
    "href": "posts2/Everest.html",
    "title": "Reaching New Heights in Plastic Pollution—Preliminary Findings of Microplastics on Mount Everest",
    "section": "",
    "text": "Link here: Napper et al., 2020\nMount Everest was once a pristine environment. However, due to increased tourism, waste is accumulating on the mountain, with a large proportion being made of plastic. This research aimed to identify and characterize microplastic (MP) pollution near the top of highest mountain on Earth and could illustrate the implications for the environment and the people living below. Stream water and snow were collected from multiple locations leading up to, and including, the Balcony (8,440 m.a.s.l). MPs were detected at an ~30 MP L\\(^−1\\) in snow and ~1 MP L\\(^−1\\) in stream water, and the majority were fibrous. Therefore, with increased tourism, deposition of MP near Mt. Everest is expected to rise. At a pivotal point in the exploration of remote areas, environmental stewardship should focus on technological and other advances toward minimizing sources of MP pollution."
  },
  {
    "objectID": "consultancy_Desktop.html",
    "href": "consultancy_Desktop.html",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\n\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects."
  },
  {
    "objectID": "consultancy_Desktop.html#statistical-experience",
    "href": "consultancy_Desktop.html#statistical-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\n\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects."
  },
  {
    "objectID": "consultancy_Desktop.html#consultancy-fieldwork-experience",
    "href": "consultancy_Desktop.html#consultancy-fieldwork-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "Consultancy Fieldwork Experience",
    "text": "Consultancy Fieldwork Experience\n\n\nI have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp."
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html",
    "href": "BasicRTutorials/IntroductionR.html",
    "title": "Introduction to R",
    "section": "",
    "text": "To install R we can install it from the internet by either googling or\nFor Windows: https://cran.r-project.org/bin/windows/base/\nFor Mac: https://cran.r-project.org/bin/macosx/\nFor Linux: https://cran.r-project.org/bin/linux/\n\n\n\n\nR is an open source statistical programming language. It can be used through many Graphic User Interfaces (GUI) my preference is to use RStudio but VSCode is good and you can also code in base R (as well as many others).\nWe can install RStudio from https://posit.co/download/rstudio-desktop/",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#simple-mathematics",
    "href": "BasicRTutorials/IntroductionR.html#simple-mathematics",
    "title": "Introduction to R",
    "section": "Simple Mathematics",
    "text": "Simple Mathematics\n\n6*6\n\n[1] 36\n\n4*3/4\n\n[1] 3\n\n4+3/5\n\n[1] 4.6\n\n(4+3)/5 # mathematical ordering matters!\n\n[1] 1.4\n\nsqrt(144)\n\n[1] 12\n\npi\n\n[1] 3.141593\n\n\nWe can use either &lt;- or = to assign a value, list or dataframe into an object, thus saving it to R’s global environment for use later\nAn object is something (usually some sort of data) that is saved in temporary memory (global environment)\n\na&lt;- 17",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#functions",
    "href": "BasicRTutorials/IntroductionR.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\nIn R we can use functions to do tasks for us, they normally precede a parenthesis (), some are named after what they do and some are less well named,\nWithin functions there are ‘arguments’ (like options), what you put into these arguments will define how they perform.\nOne function used very often is c(),\nWe use c() to concatenate elements together, which means combine them into a vector, which is a series of values\n\nb&lt;- c(1,5,5,3,7)\n\nWe can apply functions to an object\n\nmean(b)\n\n[1] 4.2\n\n\nIf we want to check the documentation for a package we can go to the Help window, or type ? before the function name.\n\n?mean()\n\nWe can then perform different functions between objects\n\na*b\n\n[1]  17  85  85  51 119\n\nmean(a*b)\n\n[1] 71.4\n\n\nWe can even save the results to a new object\n\np&lt;-a*b\n\nThen we can look at what is in the new object by running the object or printing it (print())\n\np\n\n[1]  17  85  85  51 119\n\nprint(p)\n\n[1]  17  85  85  51 119\n\n\nWe can also create data systematically with R\nFor example a sequence of 10 values going up by 1\n\nSequence&lt;-seq(from=1,to=10,by=0.5)\n\nSequence\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\nAnotherSequence&lt;-c(1:200)\n\nAnotherSequence\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200\n\n\nWe will come back to generating data systematically later.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#data-types",
    "href": "BasicRTutorials/IntroductionR.html#data-types",
    "title": "Introduction to R",
    "section": "Data Types",
    "text": "Data Types\nIn R there are many different types of data, the most common four are Numeric, Integer, Character and Factor.\nLogical and Complex are also data types but very rarely used explicitly. (logical is used a lot by functions but we rarely use it ourselves)\nNumeric data is any real numbers so 8 or 12.3 or 1.00000002 etc, while Integer data is just whole numbers 3, 4, 111 etc\nCharacter data are words or letters surrounded by quotations (either ” or ‘) such as “A”, “Red”, ’Treated’, Character data has no order to it in Rs ‘mind’\nFactor data is like character data but r (or you) have assigned an order to it e.g. “A”, “B”, “C”",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#objects",
    "href": "BasicRTutorials/IntroductionR.html#objects",
    "title": "Introduction to R",
    "section": "Objects",
    "text": "Objects\nAs we saw above we can store data in R as an Object, these can be many different types and combinations,\nThe most common Object types are Vectors, Lists, Matrices, DataFrames and Arrays,\nThe main differences of these Object types are what types and combinations of data can be stored in them and how many dimensions they have,\n\nVector\nA single group of one data type (it could be Numeric, Character, Integer, Factor), with one dimension is called a Vector.\n\nVector_Numeric&lt;- c(1.3,5.8,5.122,3.00,7.12)\n\nVector_Integer&lt;- as.integer(c(1,5,5,3,7)) \n# we change between data types with these functions \n\nVector_Character&lt;- c(\"This\",\"is\",\"A\",\"Character Vector\")\n\nVector_Factor&lt;-as.factor(c(\"This\",\"is\",\"A\",\"Character\", \"Vector\")) \n# Notice how r automatically orders alphabetically if we don't tell it the order\n\nWe can also change between types (even if they don’t fit that description)\n\nConvert_Numbers_To_Characters&lt;-as.character(Vector_Numeric)\n\nConvert_Numbers_To_Characters\n\n[1] \"1.3\"   \"5.8\"   \"5.122\" \"3\"     \"7.12\" \n\n\nNow our numbers are thought of as characters, so we can’t apply numeric operations to them!\n\n\nMatrix\nMultiple groups of one data type (it could be Numeric, Character, Integer, Factor), with two dimensions is called a Matrix.\n\nMatrix_Numeric&lt;- as.matrix(c(1.3,5.8,5.122,3.00,7.12))\n\nMatrix_Character&lt;- as.matrix(c(\"This\",\"is\",\"A\",\"Character Matrix\"))\n\n\n\nDataFrame\nMultiple groups of a combination of data types (it could be Numeric, Character, Integer, Factor), with two dimensions is called a Dataframe. Each element of a dataframe must be the last length as the other elements.\n\ndf&lt;-data.frame(Column1=c(1.3,5.8,5.122,3.00,7.12),Column2=c(1,5,5,3,7),Column3=Vector_Factor)\n\n\n\nList\nMultiple groups of a combination of data types or object types (it could be Numeric, Character, Integer, Factor or vectors, dataframes or matrices of these), with two dimensions is called a List. Each element in a list can be a different length to the other elements.\n\nList_Numeric&lt;-list(c(1.3,5.8,5.122,3.00,7.12),\n                   c(1,5,5,3,7))\n\nList_From_Vectors&lt;-list(Vector_Character,Matrix_Numeric,Matrix_Numeric)\n\nList_Different_Lengths&lt;-list(Item1=c(1,2,3,4,5,6),Item2=c(\"a\",\"B\",\"C\",\"D\"), Item3=seq(from=1,to=100,by=1))\n\nList_Different_Lengths\n\n$Item1\n[1] 1 2 3 4 5 6\n\n$Item2\n[1] \"a\" \"B\" \"C\" \"D\"\n\n$Item3\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\n\n\nArray\nMultiple groups of one data type (it could be Numeric, Character, Integer, Factor or vectors or matrices), with more than two dimensions is called an Array.\n\nArray_1d&lt;-array(c(Matrix_Numeric,c(1.3,5.8,5.122,3.00,7.12)),dim=c(5))\nArray_2d&lt;-array(c(Matrix_Numeric,c(1.3,5.8,5.122,3.00,7.12)),dim=c(5,2))\nArray_3d&lt;-array(c(Matrix_Numeric,c(1.3,5.8,5.122,3.00,7.12)),dim=c(5,2,2))\n\nArrays are rarely used so probably won’t discuss much further.\n\n\n\nTable of Object Types and Their Dimensions",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#indexing",
    "href": "BasicRTutorials/IntroductionR.html#indexing",
    "title": "Introduction to R",
    "section": "Indexing",
    "text": "Indexing\nObjects have dimensions and we can use a technique called indexing to select specific elements of an object\nWe use square brackets to do this,\nIf the object is 1 dimensional, one number will return one value\n\nVector_Numeric[4]\n\n[1] 3\n\n\nIf the object is 2 dimensional, one number will return that column\n\ndf[2]\n\n  Column2\n1       1\n2       5\n3       5\n4       3\n5       7\n\n\nby adding a comma we can select the from both dimensions (rows first, then columns)\n\ndf[4,2]\n\n[1] 3\n\n\nIf we want all rows but only a specific column we add a comma without a number\n\ndf[,2]\n\n[1] 1 5 5 3 7\n\n\nAnd vice verse\n\ndf[2,]\n\n  Column1 Column2 Column3\n2     5.8       5      is\n\n\nWe can also use multiple numbers inside c() to select multiple elements\nFor example, row 4 and 2 of all columns\n\ndf[c(2,4),]\n\n  Column1 Column2   Column3\n2     5.8       5        is\n4     3.0       3 Character\n\n\nOr we can use -c() to select all but the mentioned elements\nFor example, all rows but not columns 2 and 4\n\ndf[,-c(2,4)]\n\n  Column1   Column3\n1   1.300      This\n2   5.800        is\n3   5.122         A\n4   3.000 Character\n5   7.120    Vector",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#packages",
    "href": "BasicRTutorials/IntroductionR.html#packages",
    "title": "Introduction to R",
    "section": "Packages",
    "text": "Packages\nR relies upon packages, groups of specific functions, which can be installed from the internet and then loaded into a script.\nBase R, a package already installed and loaded within R, is very powerful and useful but less user friendly for some tasks.\nFrom Base R we can use the install.packages() function to install a package from online repositories.\nR assumes you want to download packages from CRAN (the official online repository but sometimes you might want to download from other repositories)\n\n#install.packages(\"dplyr\") \n\nYou only have to do this when you first want the package or want to update it or when you have updated r.\nOnce a package is installed we have to tell R that we want to use functions from this package so we load it\n\nlibrary(dplyr) \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nThis needs to be run every new R session when this package is used.\nWe can now run functions from the dplyr library, specifically dplyr is a package, which is part of a group or ‘ecosystem’ of packages called the tidyverse\nWe will use this group of packages for reading and writing data into and out of R (readr), manipulating and organising data (dplyr and tidyr) and visualisng data (ggplot2)",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#data-inspection",
    "href": "BasicRTutorials/IntroductionR.html#data-inspection",
    "title": "Introduction to R",
    "section": "Data Inspection",
    "text": "Data Inspection\nFirst we can make some data into a dataframe, explore this data, then save it as a file and then read the file back into r.\nR has some very useful random and non-random data generation functions\n\nYear &lt;- seq(from=1950,to=2023,by=1)\nTreatment &lt;- c(\"Control\",\"Treatment 1\",\"Treatment 2\")\nRep&lt;- seq(from=1,to=10,by=1)\n\nThese are three vectors, which we can check information about them with a few simple functions\n\nlength(Year)\n\n[1] 74\n\nsummary(Year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1950    1968    1986    1986    2005    2023 \n\nlength(Treatment)\n\n[1] 3\n\nsummary(Treatment)\n\n   Length     Class      Mode \n        3 character character \n\nlength(Rep)\n\n[1] 10\n\nsummary(Rep)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00 \n\n\nWe want to combine these vectors so we have a row for each rep, year and treatment, we can do this by expanding the grid and create a new dataframe called df.\nWe can inspect specific elements of a dataframe too\n\ndf&lt;-expand.grid(Year=Year,Treatment=Treatment,Rep=Rep)\n\nclass(df) # type of object\n\n[1] \"data.frame\"\n\nnrow(df) # number of rows\n\n[1] 2220\n\nncol(df) # number of columns\n\n[1] 3\n\ndim(df) # dimensions of object\n\n[1] 2220    3\n\nhead(df) # the first 6 rows of the df\n\n  Year Treatment Rep\n1 1950   Control   1\n2 1951   Control   1\n3 1952   Control   1\n4 1953   Control   1\n5 1954   Control   1\n6 1955   Control   1\n\ntail(df) # the last 6 rows of the df\n\n     Year   Treatment Rep\n2215 2018 Treatment 2  10\n2216 2019 Treatment 2  10\n2217 2020 Treatment 2  10\n2218 2021 Treatment 2  10\n2219 2022 Treatment 2  10\n2220 2023 Treatment 2  10\n\n\nThis df is all the meta data we want for our dataframe that we want to now make up some response data\n\nResponse&lt;-rnorm(n=nrow(df),mean = 15,sd=8) # we need the response to be same length as the df so we use nrow() for the number of values we want.\n\nWe can then combine this to our df, the dollar sign is used to select one dimension (column) from within an object (here a dataframe)\nthe column Response isn’t present in the data but by assigning our Response vector to it with df$Response &lt;- Response it adds a new column called Response to the dataframe df.\n\ndf$Response&lt;-Response",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "BasicRTutorials/IntroductionR.html#saving-and-loading-data",
    "href": "BasicRTutorials/IntroductionR.html#saving-and-loading-data",
    "title": "Introduction to R",
    "section": "Saving and Loading Data",
    "text": "Saving and Loading Data\n\nData Writing\nOnce we have our data set we can save it to our computer, but where that is on our computer is important.\nTo do this we need to know where R is looking for files on your computer. This is called your current working directory.\nThis information is displayed at the top of the console in Rstudio or you can use the base R function getwd().\nWe can set our working directory to change where this is in r (not recommended normally), or we can use our saving/loading functions to look in the correct folders (recommended).\nSide Note: there is a method for not really needing either of these called using Projects (highly recommended) but that is a bit more advanced so lets leave that for now.\nLets find out where our current working directory is, we can then create a new folder in that location, then save our df to that location.\n\ngetwd()\n\ndir.create(\"NewFolderName/\")\n\nWe now can save the df we created to this new folder using the write.csv() function from base r, or even better the write_csv() function from the readr package\nTo save to our folder we only need to say the directory we want the file saved to, followed by a /, then the name of the file with file extension.\nInside reading and writing functions such as write_csv() the main argument will be where is the file to be save to or taken from and we write this out as a character string inside quotations.\n\n# install.packages(\"readr\")\n\nlibrary(readr)\n\nwrite_csv(df,\"NewFolderName/OurNewFile.csv\")\n\n\n\nData Reading\nOften we don’t want to make fake data as done here, but we will have our own data set that we want to read in from our computer to then clean, organise, manipulate, visualise, analyse and report on.\nThese data are normally saved as excel spreadsheets. However, Excel is awful and should never be used for reproducible science! That being said it is often where a data spreadsheet starts before we bring it into R.\nExcel spreadsheets (.xsl) have lots of added information that actually is not needed and becomes complicated to work with so the easiest file format to read into R is a Comma Separated Values sheet (.csv)\nWe can convert our spreadsheet in Excel to a .csv file, then we read in the .csv file with the base function read.csv() or even better the readr function read_csv().\nAgain, inside reading and writing functions such as read_csv() the main argument will be where the file is. we write this out with a character string inside quotations.\nTo navigate up or down inside folders on your computer you use / to signify a folder, with the highest level folder on the far left\nFor example:\n\nMy_DF&lt;-read_csv(\"NewFolderName/OurNewFile.csv\")",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Introduction to R"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html",
    "href": "StatisticsMLTutorials/RemoteSensingML.html",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "",
    "text": "For this tutorial we will be trying to take some known landcover classes, associate them with the surface reflectance of those areas from Sentinel-2, then we will use tidymodels to create a supervised prediction model that can use the surface reflectance of Sentinel-2 to predict habitat from another Sentinel-2 image provided by the European Space Agency (ESA). We will then validate this new prediction using separate data.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#read-data",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#read-data",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Read Data",
    "text": "Read Data\nWe will plot the RGB alongside the landcover classification.\nThe classes are:\n1: Water\n2: Trees\n4: Flooded vegetation\n5: Crops\n7: Built Area\n8: Bare ground 9: Snow/Ice\n10: Clouds\n11: Rangeland\nWe will covert the numbers to their classification and then turn them into a factor, and assign them a nice (ish) colour scheme. There already is a colortab associated with the raster, which would plot the colours assigned by ESRI to the tif, but it causes issues with factor alteration so we remove it by setting it as null.\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(sf)\nlibrary(patchwork)\nlibrary(tidyterra)\n\nlandcover_Bordeaux&lt;-as.factor(rast(\"S2Data/Bordeaux_S2/T30TXQ_20230821T105629_Classification.tif\"))\n\nBordeaux_FileNames_10m&lt;-list.files(\"S2Data/Bordeaux_S2/\",full.names = T,pattern = \"10m\")\n\nBordeaux_FileNames_20m&lt;-list.files(\"S2Data/Bordeaux_S2/\",full.names = T,pattern = \"20m\")\n\nBordeaux_10m&lt;-rast(Bordeaux_FileNames_10m)\n\nBordeaux_20m&lt;-rast(Bordeaux_FileNames_20m)\n\np1&lt;-ggplot()+\n  geom_spatraster_rgb(data=Bordeaux_10m,\n                      r=3,g=2,b=1,\n                      max_col_value = 5000,\n                      interpolate=T)+\n  theme_classic()\n\nlandcover_Bordeaux_fct&lt;-landcover_Bordeaux %&gt;% \n  mutate(`30T_20230101-20240101`=factor(as.factor(case_when(`30T_20230101-20240101`==1~\"Water\", \n                                   `30T_20230101-20240101`==2~\"Trees\",\n                                   `30T_20230101-20240101`==4~\"Flooded\",\n                                   `30T_20230101-20240101`==5~\"Crops\",\n                                   `30T_20230101-20240101`==7~\"Built\",\n                                   `30T_20230101-20240101`==8~\"Bare\",\n                                   `30T_20230101-20240101`==9~\"Snow\",\n                                   `30T_20230101-20240101`==10~\"Clouds\",\n                                   `30T_20230101-20240101`==11~\"Rangeland\")),\n                                   levels=c(\"Water\",    \n                                            \"Trees\",\n                                            \"Flooded\",\n                                            \"Crops\",\n                                            \"Built\",\n                                            \"Bare\",\n                                            \"Snow\",\n                                            \"Clouds\",\n                                            \"Rangeland\")\n                                   ))\n\ncoltab(landcover_Bordeaux_fct[[1]])&lt;-NULL\n\n\np2&lt;-ggplot()+\n  geom_spatraster(data=landcover_Bordeaux_fct)+\n  scale_fill_manual(values=c(\"#2494a2\",\n                             \"#389318\",\n                             \"#3d26ab\",\n                             \"#DAA520\",\n                             \"#e4494f\",\n                             \"#bec3c5\",\n                             \"#f5f8fd\",\n                             \"#70543e\"))+\n  labs(fill=\"Class\")+\n  theme_classic()\n\np1/p2",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#resample-bands",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#resample-bands",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Resample Bands",
    "text": "Resample Bands\nBefore we can combine all bands together in one multiband raster we need to resample the 20 m resolution bands to be 10 m. We will use the function we used previously. Lets also plot this to visualise.\nThe intensity of spectral bands normally increase with increased wavelength but for Sentinel-2 this ranges around 0-10,000. I set a colourscale limit of 5000 to see the majority of the data, only a few pixels have a much higher value than that.\n\nBordeaux_20m_resampled&lt;-resample(Bordeaux_20m,Bordeaux_10m,method=\"average\")\n\n\nBordeaux_Multispec&lt;-c(Bordeaux_10m,Bordeaux_20m_resampled)\n\nggplot()+\n  geom_spatraster(data=Bordeaux_Multispec)+\n  labs(fill=\"\")+\n  facet_wrap(~lyr,ncol = 2)+\n  scale_fill_whitebox_c(\"viridi\",na.value = NA,limits=c(0,5000))+\n  theme_classic()\n\n&lt;SpatRaster&gt; resampled to 500955 cells for plotting",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#align-classification",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#align-classification",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Align Classification",
    "text": "Align Classification\nSo now we can add the classification as another band in the raster using the same method of using c() from above. The bands are also not named in an easy way to read so lets rename them so that they are easier to work with. Note that the original name of the classification tif was starting with a number and had a hyphen, which is generally bad naming style. We can use little ticks `` to tell r that it is a name of a column. We also remove NA values to make life easier. These tidyverse functions being applied to spatRasters are made available by the tidyterra package, which we have previously used. They make code easier to read but are slower than terra functions when dealing with very big spatrasters.\n\nBordeaux_Multispec_cleaned&lt;-c(Bordeaux_Multispec,landcover_Bordeaux) %&gt;% \n  rename(B02=T30TXQ_20230821T105629_B02_10m, \n         B03=T30TXQ_20230821T105629_B03_10m, \n         B04=T30TXQ_20230821T105629_B04_10m, \n         B08=T30TXQ_20230821T105629_B08_10m, \n         B05=T30TXQ_20230821T105629_B05_20m, \n         B06=T30TXQ_20230821T105629_B06_20m,\n         B07=T30TXQ_20230821T105629_B07_20m, \n         B11=T30TXQ_20230821T105629_B11_20m, \n         B12=T30TXQ_20230821T105629_B12_20m, \n         B8A=T30TXQ_20230821T105629_B8A_20m,\n         Class=`30T_20230101-20240101`) %&gt;% \n  drop_na()\n  \nnames(Bordeaux_Multispec_cleaned)\n\n [1] \"B02\"   \"B03\"   \"B04\"   \"B08\"   \"B05\"   \"B06\"   \"B07\"   \"B11\"   \"B12\"  \n[10] \"B8A\"   \"Class\"\n\nncell(Bordeaux_Multispec_cleaned)\n\n[1] 21298846\n\n\nNow this could be our training data but this is a lot of pixels to train a model with (~20,000,000 training pixels). We will therefore take a random subset from here to create our training data. Using spatSample will take a random sample of our raster and return a dataframe. Lets create a dataset of 10,000 rows. We will also change the Class column to be a factor and be readable. Note that the spatSample function returns a dataframe.\n\nset.seed(2345)\n\nBordeaux_Multispec_cleaned_Sample&lt;-spatSample(Bordeaux_Multispec_cleaned,size=10000,xy=TRUE) %&gt;% \n  mutate(Class=as.factor(case_when(Class==1~\"Water\",    \n                                   Class==2~\"Trees\",\n                                   Class==4~\"Flooded\",\n                                   Class==5~\"Crops\",\n                                   Class==7~\"Built\",\n                                   Class==8~\"Bare\",\n                                   Class==9~\"Snow\",\n                                   Class==10~\"Clouds\",\n                                   Class==11~\"Rangeland\")))",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#data-splitting",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#data-splitting",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Data Splitting",
    "text": "Data Splitting\nFirst we will split our model building data from Bordeaux into training, different cross validation folds of the training data and testing data.\n\nlibrary(tidymodels)\n\ninit&lt;-initial_split(Bordeaux_Multispec_cleaned_Sample,strata = Class)\n\ntrain&lt;-training(init)\n\nfolds&lt;-vfold_cv(train,strata = Class)\n\ntest&lt;-testing(init)",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#recipe",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#recipe",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Recipe",
    "text": "Recipe\nWe can now write a recipe with any transformations and updates of roles we want. We will keep this simple but we could add in all sorts of updates or transformations if we wanted to.\n\nrecipe_Class&lt;-recipe(Class~B02+B03+B04+B08+B05+B06+B07+B11+B12+B8A,data=train)\n\nclass_juiced &lt;- prep(recipe_Class) %&gt;% \n  juice()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#specification",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#specification",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Specification",
    "text": "Specification\nWe will create a Random forest model and try and tune the mtry, trees and min_n over all the folds. But we could do a different model at this stage and the rest of the code stays the same. Or we could even create a model stack like in the Comparing and Ensembling ML tutorial.\n\ntune_spec &lt;- rand_forest(\n  mtry = tune(),\n  trees = tune(),\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#workflow",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#workflow",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Workflow",
    "text": "Workflow\nOnce we have the recipe and the specification we can then add them to a workflow and tune the hyperparameters of the model across the folded training data. We will use a different method to tune the hyperparameters here, tune_race_anova() from the finetune package. This will search for best hyperpararmeters but will not use values that have been poor previously. We will also set up some simple parallel processing to speed up this process using the parallel and doParallel packages.\n\nlibrary(finetune)\nlibrary(parallel)\nlibrary(doParallel)\n\ntune_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe_Class) %&gt;%\n  add_model(tune_spec)\n\nset.seed(123)\n\ncores &lt;- detectCores(logical = FALSE)\ncl &lt;- makePSOCKcluster(cores)\nregisterDoParallel(cl)\n\ntune_res &lt;- tune_race_anova(\n  tune_wf,\n  resamples = folds\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntune_res\n\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 5\n   splits             id     .order .metrics          .notes           \n   &lt;list&gt;             &lt;chr&gt;   &lt;int&gt; &lt;list&gt;            &lt;list&gt;           \n 1 &lt;split [6747/752]&gt; Fold01      2 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt; \n 2 &lt;split [6747/752]&gt; Fold03      3 &lt;tibble [20 × 7]&gt; &lt;tibble [10 × 3]&gt;\n 3 &lt;split [6750/749]&gt; Fold06      1 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt; \n 4 &lt;split [6751/748]&gt; Fold10      4 &lt;tibble [18 × 7]&gt; &lt;tibble [0 × 3]&gt; \n 5 &lt;split [6750/749]&gt; Fold07      5 &lt;tibble [16 × 7]&gt; &lt;tibble [0 × 3]&gt; \n 6 &lt;split [6751/748]&gt; Fold09      6 &lt;tibble [16 × 7]&gt; &lt;tibble [0 × 3]&gt; \n 7 &lt;split [6747/752]&gt; Fold02      7 &lt;tibble [14 × 7]&gt; &lt;tibble [0 × 3]&gt; \n 8 &lt;split [6749/750]&gt; Fold04      8 &lt;tibble [12 × 7]&gt; &lt;tibble [6 × 3]&gt; \n 9 &lt;split [6750/749]&gt; Fold08      9 &lt;tibble [12 × 7]&gt; &lt;tibble [0 × 3]&gt; \n10 &lt;split [6749/750]&gt; Fold05     10 &lt;tibble [12 × 7]&gt; &lt;tibble [0 × 3]&gt; \n\nThere were issues with some computations:\n\n  - Warning(s) x6: No observations were detected in `truth` for level(s): 'Bare', 'F...\n  - Warning(s) x10: No observations were detected in `truth` for level(s): 'Flooded' ...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nLets plot the results of each fold using the ROC_AUC.\n\ntune_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  select(mean, min_n,trees, mtry) %&gt;%\n  pivot_longer(min_n:mtry,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %&gt;%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo we see varying accuracy based on our hyperparameters but all very similar around/above 0.8 or so regardless. Lets select the best and finalise our model.\n\nbest_auc &lt;- select_best(tune_res, metric = \"roc_auc\")\n\nfinal_rf &lt;- finalize_model(\n  tune_spec,\n  best_auc\n)\n\nfinal_rf\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 8\n  trees = 1518\n  min_n = 37\n\nComputational engine: ranger \n\n\nSo this is our model, lets investigate the different features and their importance using the vip package. This is where we use the juiced data set.\n\nlibrary(vip)\n\nset.seed(234)\n\nfinal_rf %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  fit(Class~B02+B03+B04+B08+B05+B06+B07+B11+B12+B8A,data=class_juiced\n  ) %&gt;%\n  vip(geom = \"point\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#validate-model-on-testing-data",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#validate-model-on-testing-data",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Validate Model on testing data",
    "text": "Validate Model on testing data\nLets prepare the workflow with our final model then apply the model to the testing data, and collect the accuracy metrics.\n\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe_Class) %&gt;%\n  add_model(final_rf)\n\nfinal_res &lt;- final_wf %&gt;%\n  last_fit(init)\n\nfinal_model &lt;- final_res %&gt;%\n  extract_workflow()\n\nfinal_res %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass     0.768 Preprocessor1_Model1\n2 roc_auc  hand_till      0.899 Preprocessor1_Model1\n\n\nThis is pretty good accuracy generally. We can also create a confusion matrix to compare where inaccuracies lay (i.e. false positives, false negatives?).\n\nfinal_res  %&gt;% \n  unnest(cols=.predictions) %&gt;% \n  conf_mat(.pred_class,Class)\n\n           Truth\nPrediction  Bare Built Crops Flooded Rangeland Trees Water\n  Bare         2     0     0       0         0     0     0\n  Built        1   451    46       0        14    60     0\n  Crops        1    64   164       0        36   127     0\n  Flooded      0     0     0       0         0     1     0\n  Rangeland    3    36    71       0        63    28     0\n  Trees        0    44    29       0        12   963     0\n  Water        2     2     0       0         0     3   278",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#read-in-new-data",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#read-in-new-data",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Read in New Data",
    "text": "Read in New Data\nAbove we used the testing data set to evaluate the model accuracy. These are data randomly split from the training data, which in our case means it is the same satellite image, same day and geographically similar location. But we may be wanting a model that can be applied at scale, across large geographical ranges and different images. To do this we want to validate our model on another image. We will take our bilbao image and its labels (again we are using labels generated by another model so a bit flawed but lets assume the labels are correct) then apply our model, comparing our predictions with the ‘true’ labels. I have used the full tif to predict on but a sample could be taken as above. This is the code that is commented out.\n\nlandcover_Bilbao&lt;-as.factor(rast(\"S2Data/Bilbao_S2/T30TWN_20230925T105801_Classification.tif\"))\n\nBilbao_FileNames_10m&lt;-list.files(\"S2Data/Bilbao_S2/\",full.names = T,pattern = \"10m\")\n\nBilbao_FileNames_20m&lt;-list.files(\"S2Data/Bilbao_S2/\",full.names = T,pattern = \"20m\")\n\nBilbao_10m&lt;-rast(Bilbao_FileNames_10m)\n\nBilbao_20m&lt;-rast(Bilbao_FileNames_20m)\n\nBilbao_20m_resampled&lt;-resample(Bilbao_20m,Bilbao_10m,method=\"average\")\n\n\nBilbao_Multispec&lt;-c(Bilbao_10m,Bilbao_20m_resampled)\n\n\nBilbao_Multispec_cleaned&lt;-c(Bilbao_Multispec,landcover_Bilbao) %&gt;% \n  rename(B02=T30TWN_20230925T105801_B02_10m, \n         B03=T30TWN_20230925T105801_B03_10m, \n         B04=T30TWN_20230925T105801_B04_10m, \n         B08=T30TWN_20230925T105801_B08_10m, \n         B05=T30TWN_20230925T105801_B05_20m, \n         B06=T30TWN_20230925T105801_B06_20m,\n         B07=T30TWN_20230925T105801_B07_20m, \n         B11=T30TWN_20230925T105801_B11_20m, \n         B12=T30TWN_20230925T105801_B12_20m, \n         B8A=T30TWN_20230925T105801_B8A_20m,\n         Class=`30T_20230101-20240101`) %&gt;% \n  drop_na()\n\nset.seed(2345)\n\nBilbao_Multispec_cleaned_Sample&lt;-as.data.frame(Bilbao_Multispec_cleaned,xy=TRUE)%&gt;% \n  mutate(Class=as.factor(case_when(Class==1~\"Water\",    \n                                   Class==2~\"Trees\",\n                                   Class==4~\"Flooded\",\n                                   Class==5~\"Crops\",\n                                   Class==7~\"Built\",\n                                   Class==8~\"Bare\",\n                                   Class==9~\"Snow\",\n                                   Class==10~\"Clouds\",\n                                   Class==11~\"Rangeland\")))\n\n\n#Bilbao_Multispec_cleaned_Sample&lt;-spatSample(Bilbao_Multispec_cleaned,size=1000000,xy=TRUE) %&gt;% \n#  mutate(Class=as.factor(case_when(Class==1~\"Water\",   \n#                                   Class==2~\"Trees\",\n#                                   Class==4~\"Flooded\",\n#                                   Class==5~\"Crops\",\n#                                   Class==7~\"Built\",\n#                                   Class==8~\"Bare\",\n#                                   Class==9~\"Snow\",\n#                                   Class==10~\"Clouds\",\n#                                   Class==11~\"Rangeland\")))",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#predict-on-new-data",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#predict-on-new-data",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Predict on New Data",
    "text": "Predict on New Data\nWe will predict on these new data, creating a new column. To calculate the confusion matrix and accuracy we need the two columns to have the same factor levels, but they are not all in both columns. Se we will set the levels of the factor columns to include all potential levels.\n\nBilbaoPrediction&lt;-Bilbao_Multispec_cleaned_Sample %&gt;% \n  mutate(predict(final_model,.),\n         .pred_class=factor(.pred_class,levels=c(\"Water\",   \n                                                 \"Trees\",\n                                                 \"Flooded\",\n                                                 \"Crops\",\n                                                 \"Built\",\n                                                 \"Bare\",\n                                                 \"Snow\",\n                                                 \"Clouds\",\n                                                 \"Rangeland\")),\n         Class=factor(Class,levels=c(\"Water\",   \n                                     \"Trees\",\n                                     \"Flooded\",\n                                     \"Crops\",\n                                     \"Built\",\n                                     \"Bare\",\n                                     \"Snow\",\n                                     \"Clouds\",\n                                     \"Rangeland\")))\n\nBilbaoPrediction%&gt;% \n  conf_mat(truth=Class,estimate=.pred_class)\n\n           Truth\nPrediction   Water  Trees Flooded  Crops  Built   Bare   Snow Clouds Rangeland\n  Water       9869   9551       0     21   9546      0      0      0       248\n  Trees        436 473788       0   2498  84310      3      0      0     36686\n  Flooded        0      0       0      0      0      0      0      0         0\n  Crops         39  31138       0   4970  45239     15      0      0     65353\n  Built       1283  16852       0   1994 265142    529      0      2     10960\n  Bare           0      0       0      0    442      0      0      0         0\n  Snow           0      0       0      0      0      0      0      0         0\n  Clouds         0      0       0      0      0      0      0      0         0\n  Rangeland      0  12592       0   1409   7418      1      0      0     39848\n\nBilbaoPrediction%&gt;% \n  accuracy(truth=Class,estimate=.pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.701\n\n\nOkay so we can see that the accuracy is 0.701, not amazing but also not awful.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/RemoteSensingML.html#plotting-prediction",
    "href": "StatisticsMLTutorials/RemoteSensingML.html#plotting-prediction",
    "title": "Using Machine Learning with Satellite Imagery",
    "section": "Plotting Prediction",
    "text": "Plotting Prediction\nSo lets look on a map, comparing the new prediction with the ‘true’ class. We will clean the prediction dataset, convert the predictions to rasters then plot them alongside the original ‘true’ classification. Again as before we will remove the colortab as it make mutation difficult.\n\nBilbaoPrediction_tif&lt;-BilbaoPrediction %&gt;% \n  mutate(.pred_class=case_when(.pred_class==\"Water\"~1,  \n                                   .pred_class==\"Trees\"~2,\n                                   .pred_class==\"Flooded\"~4,\n                                   .pred_class==\"Crops\"~5,\n                                   .pred_class==\"Built\"~7,\n                                   .pred_class==\"Bare\"~8,\n                                   .pred_class==\"Snow\"~9,\n                                   .pred_class==\"Clouds\"~10,\n                                   .pred_class==\"Rangeland\"~11))%&gt;% \n  dplyr::select(x,y,.pred_class) %&gt;% \n  rast(type=\"xyz\",crs=\"EPSG:32630\")\n\nlandcover_Bilbao_True&lt;-as.factor(rast(\"S2Data/Bilbao_S2/T30TWN_20230925T105801_Classification.tif\"))\n\nBilbao_Comparison&lt;-c(landcover_Bilbao_True,BilbaoPrediction_tif) %&gt;% \n  rename(Truth=`30T_20230101-20240101`, \n         Prediction=.pred_class)\n\ncoltab(Bilbao_Comparison[[1]])&lt;-NULL\n\nBilbao_Comparison_fct&lt;-Bilbao_Comparison %&gt;% \n  mutate(Prediction=factor(case_when(Prediction==1~\"Water\", \n                                   Prediction==2~\"Trees\",\n                                   Prediction==4~\"Flooded\",\n                                   Prediction==5~\"Crops\",\n                                   Prediction==7~\"Built\",\n                                   Prediction==8~\"Bare\",\n                                   Prediction==9~\"Snow\",\n                                   Prediction==10~\"Clouds\",\n                                   Prediction==11~\"Rangeland\"),\n         levels=c(\"Water\",  \n                  \"Trees\",\n                  \"Flooded\",\n                  \"Crops\",\n                  \"Built\",\n                  \"Bare\",\n                  \"Snow\",\n                  \"Clouds\",\n                  \"Rangeland\")),\n         Truth=factor(case_when(Truth==1~\"Water\",   \n                                Truth==2~\"Trees\",\n                                Truth==4~\"Flooded\",\n                                Truth==5~\"Crops\",\n                                Truth==7~\"Built\",\n                                Truth==8~\"Bare\",\n                                Truth==9~\"Snow\",\n                                Truth==10~\"Clouds\",\n                                Truth==11~\"Rangeland\"),\n         levels=c(\"Water\",  \n                  \"Trees\",\n                  \"Flooded\",\n                  \"Crops\",\n                  \"Built\",\n                  \"Bare\",\n                  \"Snow\",\n                  \"Clouds\",\n                  \"Rangeland\")))\n\nggplot()+\n  geom_spatraster(data=Bilbao_Comparison_fct,\n                  maxcell = 10000000)+\n  scale_fill_manual(values=c(\"#2494a2\",\n                             \"#389318\",\n                             \"#DAA520\",\n                             \"#e4494f\",\n                             \"#bec3c5\",\n                             \"#f5f8fd\",\n                             \"#70543e\"))+\n  facet_wrap(~lyr,ncol = 1)+\n  labs(fill=\"Class\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay so these two images look very similar generally but with some big differences, especially in the amount of rangeland predicted. We can also see whether certain areas were better predicted or not.\n\nCorrect&lt;-Bilbao_Comparison_fct %&gt;% \n  mutate(Correct=as.factor(case_when(Truth==Prediction~\"Correct\",\n                           TRUE~\"Incorrect\"))) %&gt;% \n  select(Correct)\n\nggplot()+\n  geom_spatraster(data=Correct,\n                  maxcell = 10000000)+\n  scale_fill_manual(values=c(\"#cccc00\",\n                             \"#b3002d\"))+\n  labs(fill=\"Correct?\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay so there are some clear patterns here of where we have correctly identified and where we haven’t. This may be acceptable to us or not.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Using Machine Learning with Satellite Imagery"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "So far we have used models to describe data but a lot of times we might want to create a model that is able to predict new data. This may be new data outside of our temporal or spatial range, or outside of our current knowledge base. The best set of tools for predicting new data is generally Machine Learning (ML) methods. For a pretty full and comprehensive discussion, with advantages and disadvantages go here: https://ml-science-book.com/\n\n\nOne group of ML models, requiring minimum input, are Unsupervised models do not required training/labelled data and thus require less initial work. These methods will group similar data into groups or clusters. As the vast majority of data is unlabelled Unsupervised models appeal in that they can be applied with no prior work. However, the user will normally define how many groups to split the data into or at what level of similarity to split, and thus may influence the model itself. Generally, these methods are poor at complex prediction tasks and can be easily outperformed by Supervised ML.\n\n\n\nSupervised learning algorithms or models are models where we ‘train’ a model with known and labelled data so that the model can discern patterns that when it is given unknown/unlabelled data it will be able to, hopefully, predict what the label should be. These models can perform both regression (continuous data) or classification (categorical data), but will be highly reliant on the data used to train the model, as values or classes outside of what it has already seen will have much higher inaccuracy. There are a few different types of Supervised ML with varying levels of complexity from General Linear Models (GLMs), K-Nearest Neighbours (KNN), Random Forests (RF), Support Vector Machines (SVMs), eXtreme Gradient Boosting (XGBoost) and Neural Networks (NN).",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html#unsupervised-ml",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html#unsupervised-ml",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "One group of ML models, requiring minimum input, are Unsupervised models do not required training/labelled data and thus require less initial work. These methods will group similar data into groups or clusters. As the vast majority of data is unlabelled Unsupervised models appeal in that they can be applied with no prior work. However, the user will normally define how many groups to split the data into or at what level of similarity to split, and thus may influence the model itself. Generally, these methods are poor at complex prediction tasks and can be easily outperformed by Supervised ML.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html#supervised-ml",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html#supervised-ml",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Supervised learning algorithms or models are models where we ‘train’ a model with known and labelled data so that the model can discern patterns that when it is given unknown/unlabelled data it will be able to, hopefully, predict what the label should be. These models can perform both regression (continuous data) or classification (categorical data), but will be highly reliant on the data used to train the model, as values or classes outside of what it has already seen will have much higher inaccuracy. There are a few different types of Supervised ML with varying levels of complexity from General Linear Models (GLMs), K-Nearest Neighbours (KNN), Random Forests (RF), Support Vector Machines (SVMs), eXtreme Gradient Boosting (XGBoost) and Neural Networks (NN).",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html#which-model-type-to-chose-and-why-is-the-data-more-important",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html#which-model-type-to-chose-and-why-is-the-data-more-important",
    "title": "Introduction to Machine Learning",
    "section": "Which model type to chose? and why is the data more important?",
    "text": "Which model type to chose? and why is the data more important?\nThe most popular ML models are Random Forests (RFs), which require very little hyper parameter training and usually provide good accuracy “off the shelf”. XGBoost models perform to higher accuracy once their hyper parameters are effectively trained but are not as good as RF off the shelf, similarly Neural Networks are generally the most accurate and powerful models available but require far more hyper parameter and model architecture to perfect. With all the different ML models the quantity and quality of training data will dictate the eventually ability of the models. For example, none of these models are able to predict a new class when predicting a classification problem, likewise many of these models (but not all) will be unable or very poor at extrapolating outside of the range of the label training values.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html#theory",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html#theory",
    "title": "Introduction to Machine Learning",
    "section": "Theory",
    "text": "Theory\nWe won’t go into depth of what each of these models is doing inside these functions as there are very many useful and clear tutorials on the theory behind each different machine learning algorithm. Here we will focus on their application, common mistakes, tricks and challenges. For further reading I would recommend: Julia Silge for tidymodels tutorials (Where I learnt): https://juliasilge.com/, for more complicated theory of random forest (although with python code examples): https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/ and for indepth explanation of neural networks I would recommend Jeremy Howards Youtuube channel: https://www.youtube.com/@howardjeremyp (again this is mostly from a python coding approach).",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html#ml-modelling-steps",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html#ml-modelling-steps",
    "title": "Introduction to Machine Learning",
    "section": "ML Modelling steps:",
    "text": "ML Modelling steps:\n\nPre-process: tidying, splitting and transforming data\nTrain: Training the model (with or without hyperparameter training)\nValidate: Testing the model on the testing data and calculating accuracy",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html#pre-processing",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html#pre-processing",
    "title": "Introduction to Machine Learning",
    "section": "Pre-Processing",
    "text": "Pre-Processing\nThe data are already tidy so lets split and then carry out any transformations we might want to do.\n\npenguins_noNA&lt;-penguins %&gt;% \n  drop_na()\n\nsplitdata&lt;-initial_split(penguins_noNA)\n\ndf_train&lt;-splitdata %&gt;% \n  training()\n\ndf_test&lt;-splitdata %&gt;% \n  testing()\n\nWe will be tuning hyper parameters later so we will also split the training data further into 5 cross validation folds. When we look at the fold object we can see that there are 10 sets of the same training data where the split is randomly different each time. As this element has randomisation involved we set a seed to make sure we get consistent results each time the code is run. We also want to make sure that the different levels of the response variable is present in both sides of each fold so we set strata to be species.\n\nset.seed(234)\n\ndf_train_folds&lt;-vfold_cv(df_train,strata = species)\n\ndf_train_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [223/26]&gt; Fold01\n 2 &lt;split [223/26]&gt; Fold02\n 3 &lt;split [223/26]&gt; Fold03\n 4 &lt;split [223/26]&gt; Fold04\n 5 &lt;split [223/26]&gt; Fold05\n 6 &lt;split [224/25]&gt; Fold06\n 7 &lt;split [225/24]&gt; Fold07\n 8 &lt;split [225/24]&gt; Fold08\n 9 &lt;split [226/23]&gt; Fold09\n10 &lt;split [226/23]&gt; Fold10\n\n\nWith tidymodels we use recipes to integrate transformations and model formula\nFor our categorical predictors we will convert them to dummy variables. We will also “prep” and “juice” datasets, which creates a dataset where the recipe hase been applied to it for later on.\n\npenguin_recipe&lt;-recipe(species~.,data=df_train)%&gt;% \n  step_dummy(island,sex) \n\n\npenguin_juiced &lt;- prep(penguin_recipe) %&gt;% \n  juice()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html#model-building",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html#model-building",
    "title": "Introduction to Machine Learning",
    "section": "Model Building",
    "text": "Model Building\nWe now create a model object, we can add specific values to the three hyper parameters, or we can tell the model we will try to tune them across our data folds. Here we create the random forest model and tell tidy models it is a classification mode model and we want to use the ranger “engine”.\n\ntune_spec &lt;- rand_forest(\n  mtry = tune(),\n  trees = 500,\n  min_n = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nOnce we have the recipe and the model we can then add them to a workflow and tune the model on the folded training data.\n\ntune_wf &lt;- workflow() %&gt;%\n  add_recipe(penguin_recipe) %&gt;%\n  add_model(tune_spec)\n\nset.seed(234)\n\ntune_res &lt;- tune_grid(\n  tune_wf,\n  resamples = df_train_folds,\n  grid = 10\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntune_res\n\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [223/26]&gt; Fold01 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [223/26]&gt; Fold02 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [223/26]&gt; Fold03 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [223/26]&gt; Fold04 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [223/26]&gt; Fold05 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [224/25]&gt; Fold06 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [225/24]&gt; Fold07 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [225/24]&gt; Fold08 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [226/23]&gt; Fold09 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [226/23]&gt; Fold10 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nLets plot the results of each fold using the ROC_AUC.\n\ntune_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  select(mean, min_n, mtry) %&gt;%\n  pivot_longer(min_n:mtry,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %&gt;%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can see that the best values are low min_n and low mtry. But we can use a function to select the best hyper parameters from this AUC value, then that is our final model.\n\nbest_auc &lt;- select_best(tune_res, metric = \"roc_auc\")\n\nfinal_rf &lt;- finalize_model(\n  tune_spec,\n  best_auc\n)\n\nfinal_rf\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 2\n  trees = 500\n  min_n = 30\n\nComputational engine: ranger \n\n\nSo this is our model, lets investigate the different features and their importance using the vip package. This is where we use the juiced data set.\n\nlibrary(vip)\n\nset.seed(234)\n\nfinal_rf %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  fit(species~.,data=penguin_juiced\n  ) %&gt;%\n  vip(geom = \"point\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nFrom this we can see that bill length is the best predictor of species, followed by flipper length.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsMLTutorials/IntroMachineLearning.html#validate-model-on-testing-data",
    "href": "StatisticsMLTutorials/IntroMachineLearning.html#validate-model-on-testing-data",
    "title": "Introduction to Machine Learning",
    "section": "Validate Model on testing data",
    "text": "Validate Model on testing data\nLets prepare the workflow with our final model then apply the model to the testing data, and collect the accuracy metrics.\n\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(penguin_recipe) %&gt;%\n  add_model(final_rf)\n\nfinal_res &lt;- final_wf %&gt;%\n  last_fit(splitdata)\n\nfinal_res %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass     0.952 Preprocessor1_Model1\n2 roc_auc  hand_till      0.995 Preprocessor1_Model1\n\n\nSo in this very specific example we have got ridiculously good results in our model. This is not always the case.\nWe can also create a confusion matrix to compare where inaccuracies lay (i.e. false positives, false negatives?).\n\nfinal_res  %&gt;% \n  unnest(cols=.predictions) %&gt;% \n  conf_mat(.pred_class,species)\n\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        30         1      0\n  Chinstrap      3        17      0\n  Gentoo         0         0     33\n\n\nSo we can see from this that almost all the predictions are the same as the truth. If you want to see how much the random number generation from before effects the results you can rerun this script with different set.seed() numbers.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Introduction to Machine Learning"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GammaGLMs.html",
    "href": "StatisticsTutorials/GammaGLMs.html",
    "title": "Gamma GLMs",
    "section": "",
    "text": "Lets use the real-world dataset Loblolly Pine trees, which contains the Height of Pine trees (in feet) over Ages (in years) for different individuals (Seeds).\nThis will be quite a simple example assessing change with age of pine trees. Here we will not be telling the model that are data are repeat measures. This means our data are not independent. To take this into consideration we could use a General Linear Mixed Effects Model with Seed as a random factor. However, for this example we shall pretend that there isn’t this structure in our data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(performance)\nlibrary(patchwork)\n\ndata(\"Loblolly\")\n\nglimpse(Loblolly)\n\nRows: 84\nColumns: 3\n$ height &lt;dbl&gt; 4.51, 10.89, 28.72, 41.74, 52.70, 60.92, 4.55, 10.92, 29.07, 42…\n$ age    &lt;dbl&gt; 3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 2…\n$ Seed   &lt;ord&gt; 301, 301, 301, 301, 301, 301, 303, 303, 303, 303, 303, 303, 305…\n\n\n\n\n\nAs we mentioned above we are simplifying this example into the change in Height of Pine trees with age.\nThis is a very simple model with just one fixed effect and can be written as:\nHeight of Pine Tree ~ Age\n\n\n\nThe height of the tree must be positive and is continuous, therefore we should technically use a Gamma Model. Again as with most Gamma examples we could use a Gaussian distribution, which is simpler mathematically, but when using a Gaussian model on a transformed response variable we will get incorrect estimates of effects meaning our inference will be incorrect.\n\n\n\nAs a fully experimental data set we actually have the same count of each age category: 3, 5, 10, 15, 20, 25. This means we should be able to use the raw fixed effect value with little issue.\n\nggplot(Loblolly,aes(x=age))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\n\n\n\n\n\n\nAs we are scientists so should use logical units we shall convert our height column to metres before modelling. We can do this by multiplying our feet by 0.3048.\n\ndf&lt;-Loblolly %&gt;% \n  mutate(Height=height*0.3048)\n\nglm1&lt;-glm(Height~age,data=df, family=Gamma(link = \"identity\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nWe see some fairly mixed results here. We can see out Homogeneity of Variance isn’t flat and horizontal, but there are no clear patterns at high or low fitted values. The only pattern we do see is vertical banding, this is because we have actually got repeat measurements of the same trees over time. We shall ignore this for this example but we really should have run a GLMM to take into account all the hierarchy of the data.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = Height ~ age, family = Gamma(link = \"identity\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.32432    0.05556  -23.84   &lt;2e-16 ***\nage          0.88058    0.01246   70.68   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.008982799)\n\n    Null deviance: 57.46161  on 83  degrees of freedom\nResidual deviance:  0.73641  on 82  degrees of freedom\nAIC: 173.7\n\nNumber of Fisher Scoring iterations: 3\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Height \\;of \\;Pine \\;Tree = Gamma(y',\\alpha)\\]\n\\[y'=y\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Age + Intercept\n\\end{aligned}\n\\]\nAs the Gamma distribution requires two shape parameters (\\(y'\\) and \\(\\alpha\\)), where \\(y'\\) must be above zero, we must convert out linear equation results (\\(y\\)) so that it is positive. This means we use the link function, which for Gamma models is by default a inverse. We can use a different link function if we want, and here we did, we used the identity link function. The identity function does nothing: it just uses the raw data, this is simpler mathematically and computationally but may be the incorrect decision depending on the situation.\n\nglm1$family$link\n\n[1] \"identity\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nAs always we then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just age, the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Height of Pine Tree based on those ages.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\n\nNewData_1&lt;-data.frame(age=seq(min(df$age),max(df$age),length.out=50))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=age,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=age,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Age\",y=\"Response Variable (Height of Pine Tree)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=df,aes(x=age,\n                         y=Height),\n              alpha=0.3,\n             size=0.8,\n             colour=\"darkcyan\")+\n  geom_ribbon(aes(x=age,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=age,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Age\",y=\"Response Variable (Height of Pine Tree)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good. But we have been honest about making sure we model the full hierarchy of our data set, which we did not do here! However, we can see the model got generally the correct pattern with an almost 1:1 relationship with meters and years. We could have seen that from the summary table, where the estimate for age was 0.881 while the estimate for the intercept was -1.324. Fed into our equation that gives use a line of Height = 0.881 * Age + -1.324.\n\n\n\n\nThis dataset is an experimental dataset where weights (g) of chicks were measured from birth until day 21 (Time) based on 4 different dietary regimes. Again as above there is correlation element of each chick being correlated with its previous weight but again as above we will ignore this issue. This type of hierarchy or repeat measurement is highly important and as researchers faced with this structure of data we should really use a GLMM (sometimes called hierarchy model or multilevel model). This should be fine for our example but again: always model the structure you know or understand about your data.\n\n\ndata(\"ChickWeight\")\n\nsummary(ChickWeight)\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n\n\nWe will assess the Weight over time of Chicks depending on different diet types. We will assess if the change in weight over time is different across the different diets.\nThis is a bit more complex model with two interacting fixed effect, and can be written as:\nChick Weight ~ Age*Diet\n\n\n\nAgain, our value is a measurement that is always positive and continuous, thus we will use the Gamma distribution. While, the values in our response variable do not have decimal points the weight of a chick in grams could feasibly have 0.5 of a gram etc. This data not having decimals doesn’t matter and is more related to the measurement style (precision of the scale used), the data still come from a Gamma distribution.\n\n\n\nLets check all our fixed effects.\n\np1&lt;-ggplot(ChickWeight,aes(x=Time))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(ChickWeight,aes(x=Diet))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\n(p1+p2)\n\n\n\n\n\n\n\n\nOur factors are all evenly grouped as this is more of a traditional experimental set up. There are more chicks on diet 1 but still high values for the other diet types so it should be fine. We have less and less chicks at higher times of the experiment, I don’t know why but I would guess at some mortality issues.\n\nglm2&lt;-glm(weight~Time*Diet,data=ChickWeight, family=Gamma())\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs earlier, we see some fairly mixed results here. The normality of residuals is not perfect with many small and high value points not following the line, whereas our homogenerity of variance is pretty good apart from less variation at lower values than larger values.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = weight ~ Time * Diet, family = Gamma(), data = ChickWeight)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.892e-02  4.949e-04  38.234  &lt; 2e-16 ***\nTime        -6.796e-04  2.882e-05 -23.576  &lt; 2e-16 ***\nDiet2       -1.857e-03  7.830e-04  -2.371 0.018065 *  \nDiet3       -2.703e-03  7.457e-04  -3.625 0.000315 ***\nDiet4       -3.463e-03  7.391e-04  -4.685  3.5e-06 ***\nTime:Diet2   4.940e-05  4.484e-05   1.102 0.271037    \nTime:Diet3   4.714e-05  4.208e-05   1.120 0.263050    \nTime:Diet4   1.031e-04  4.249e-05   2.426 0.015570 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.07498304)\n\n    Null deviance: 193.022  on 577  degrees of freedom\nResidual deviance:  42.305  on 570  degrees of freedom\nAIC: 5512.8\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Chick \\;Weight = Gamma(y',\\alpha)\\]\n\\[y'=y^{-1}\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Time:Diet \\;1 \\;vs \\;4\\\\\n+ \\beta_{2} Time:Diet \\;1 \\;vs \\;3\\\\\n+ \\beta_{3} Time:Diet \\;1 \\;vs \\;2\\\\\n+ \\beta_{4} Diet \\;1 \\;vs \\;4\\\\\n+ \\beta_{5} Diet \\;1 \\;vs \\;3\\\\\n+ \\beta_{6} Diet \\;1 \\;vs \\;2\\\\\n+ \\beta_{7} Time\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Gamma distribution requires two shape parameters (\\(y'\\) and \\(\\alpha\\)), where \\(y'\\) must be above zero, we must convert out linear equation results (\\(y\\)) so that it is positive. This means we use the link function, which for Gamma models is by default a inverse. We can use a different link function if we want, for this example we used this default.\n\nglm2$family$link\n\n[1] \"inverse\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age and diet the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average weight based on those ages and diets.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(Time=seq(min(ChickWeight$Time),max(ChickWeight$Time),length.out=100),\n                       Diet=as.factor(c(1:4)))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Time,ymax=Upr,ymin=Lwr,fill=Diet),\n              alpha=0.6)+\n  geom_line(aes(x=Time,y=response,colour=Diet,linetype=Diet))+\n  labs(x=\"Age\",y=\"Predicted Chick Weight (g)\",\n       fill=\"Diet\",colour=\"Diet\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=ChickWeight,aes(x=Time,y=weight,colour=Diet),\n             alpha=0.5,\n             size=0.6)+\n  geom_ribbon(aes(x=Time,ymax=Upr,ymin=Lwr,fill=Diet),\n              alpha=0.6)+\n  geom_line(aes(x=Time,y=response,colour=Diet,linetype=Diet))+\n  labs(x=\"Age\",y=\"Predicted Chick Weight (g)\",\n       fill=\"Diet\",colour=\"Diet\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo again our model seems pretty good, with minimal differences between treatments and strong increase in weight with Age of Chick. But some of this isn’t great, such as very low values being over predicted, plus some clear lines of data well outside the models.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Gamma GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GammaGLMs.html#data-loading-simple---loblolly-pine-trees",
    "href": "StatisticsTutorials/GammaGLMs.html#data-loading-simple---loblolly-pine-trees",
    "title": "Gamma GLMs",
    "section": "",
    "text": "Lets use the real-world dataset Loblolly Pine trees, which contains the Height of Pine trees (in feet) over Ages (in years) for different individuals (Seeds).\nThis will be quite a simple example assessing change with age of pine trees. Here we will not be telling the model that are data are repeat measures. This means our data are not independent. To take this into consideration we could use a General Linear Mixed Effects Model with Seed as a random factor. However, for this example we shall pretend that there isn’t this structure in our data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(performance)\nlibrary(patchwork)\n\ndata(\"Loblolly\")\n\nglimpse(Loblolly)\n\nRows: 84\nColumns: 3\n$ height &lt;dbl&gt; 4.51, 10.89, 28.72, 41.74, 52.70, 60.92, 4.55, 10.92, 29.07, 42…\n$ age    &lt;dbl&gt; 3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 25, 3, 5, 10, 15, 20, 2…\n$ Seed   &lt;ord&gt; 301, 301, 301, 301, 301, 301, 303, 303, 303, 303, 303, 303, 305…\n\n\n\n\n\nAs we mentioned above we are simplifying this example into the change in Height of Pine trees with age.\nThis is a very simple model with just one fixed effect and can be written as:\nHeight of Pine Tree ~ Age\n\n\n\nThe height of the tree must be positive and is continuous, therefore we should technically use a Gamma Model. Again as with most Gamma examples we could use a Gaussian distribution, which is simpler mathematically, but when using a Gaussian model on a transformed response variable we will get incorrect estimates of effects meaning our inference will be incorrect.\n\n\n\nAs a fully experimental data set we actually have the same count of each age category: 3, 5, 10, 15, 20, 25. This means we should be able to use the raw fixed effect value with little issue.\n\nggplot(Loblolly,aes(x=age))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\n\n\n\n\n\n\nAs we are scientists so should use logical units we shall convert our height column to metres before modelling. We can do this by multiplying our feet by 0.3048.\n\ndf&lt;-Loblolly %&gt;% \n  mutate(Height=height*0.3048)\n\nglm1&lt;-glm(Height~age,data=df, family=Gamma(link = \"identity\"))\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nWe see some fairly mixed results here. We can see out Homogeneity of Variance isn’t flat and horizontal, but there are no clear patterns at high or low fitted values. The only pattern we do see is vertical banding, this is because we have actually got repeat measurements of the same trees over time. We shall ignore this for this example but we really should have run a GLMM to take into account all the hierarchy of the data.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = Height ~ age, family = Gamma(link = \"identity\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.32432    0.05556  -23.84   &lt;2e-16 ***\nage          0.88058    0.01246   70.68   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.008982799)\n\n    Null deviance: 57.46161  on 83  degrees of freedom\nResidual deviance:  0.73641  on 82  degrees of freedom\nAIC: 173.7\n\nNumber of Fisher Scoring iterations: 3\n\n\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Height \\;of \\;Pine \\;Tree = Gamma(y',\\alpha)\\]\n\\[y'=y\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Age + Intercept\n\\end{aligned}\n\\]\nAs the Gamma distribution requires two shape parameters (\\(y'\\) and \\(\\alpha\\)), where \\(y'\\) must be above zero, we must convert out linear equation results (\\(y\\)) so that it is positive. This means we use the link function, which for Gamma models is by default a inverse. We can use a different link function if we want, and here we did, we used the identity link function. The identity function does nothing: it just uses the raw data, this is simpler mathematically and computationally but may be the incorrect decision depending on the situation.\n\nglm1$family$link\n\n[1] \"identity\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nAs always we then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just age, the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Height of Pine Tree based on those ages.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\n\nNewData_1&lt;-data.frame(age=seq(min(df$age),max(df$age),length.out=50))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=age,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=age,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Age\",y=\"Response Variable (Height of Pine Tree)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=df,aes(x=age,\n                         y=Height),\n              alpha=0.3,\n             size=0.8,\n             colour=\"darkcyan\")+\n  geom_ribbon(aes(x=age,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=age,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Age\",y=\"Response Variable (Height of Pine Tree)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good. But we have been honest about making sure we model the full hierarchy of our data set, which we did not do here! However, we can see the model got generally the correct pattern with an almost 1:1 relationship with meters and years. We could have seen that from the summary table, where the estimate for age was 0.881 while the estimate for the intercept was -1.324. Fed into our equation that gives use a line of Height = 0.881 * Age + -1.324.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Gamma GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GammaGLMs.html#data-loading-complex---chicks",
    "href": "StatisticsTutorials/GammaGLMs.html#data-loading-complex---chicks",
    "title": "Gamma GLMs",
    "section": "",
    "text": "This dataset is an experimental dataset where weights (g) of chicks were measured from birth until day 21 (Time) based on 4 different dietary regimes. Again as above there is correlation element of each chick being correlated with its previous weight but again as above we will ignore this issue. This type of hierarchy or repeat measurement is highly important and as researchers faced with this structure of data we should really use a GLMM (sometimes called hierarchy model or multilevel model). This should be fine for our example but again: always model the structure you know or understand about your data.\n\n\ndata(\"ChickWeight\")\n\nsummary(ChickWeight)\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n\n\nWe will assess the Weight over time of Chicks depending on different diet types. We will assess if the change in weight over time is different across the different diets.\nThis is a bit more complex model with two interacting fixed effect, and can be written as:\nChick Weight ~ Age*Diet\n\n\n\nAgain, our value is a measurement that is always positive and continuous, thus we will use the Gamma distribution. While, the values in our response variable do not have decimal points the weight of a chick in grams could feasibly have 0.5 of a gram etc. This data not having decimals doesn’t matter and is more related to the measurement style (precision of the scale used), the data still come from a Gamma distribution.\n\n\n\nLets check all our fixed effects.\n\np1&lt;-ggplot(ChickWeight,aes(x=Time))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(ChickWeight,aes(x=Diet))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\n(p1+p2)\n\n\n\n\n\n\n\n\nOur factors are all evenly grouped as this is more of a traditional experimental set up. There are more chicks on diet 1 but still high values for the other diet types so it should be fine. We have less and less chicks at higher times of the experiment, I don’t know why but I would guess at some mortality issues.\n\nglm2&lt;-glm(weight~Time*Diet,data=ChickWeight, family=Gamma())\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs earlier, we see some fairly mixed results here. The normality of residuals is not perfect with many small and high value points not following the line, whereas our homogenerity of variance is pretty good apart from less variation at lower values than larger values.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = weight ~ Time * Diet, family = Gamma(), data = ChickWeight)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.892e-02  4.949e-04  38.234  &lt; 2e-16 ***\nTime        -6.796e-04  2.882e-05 -23.576  &lt; 2e-16 ***\nDiet2       -1.857e-03  7.830e-04  -2.371 0.018065 *  \nDiet3       -2.703e-03  7.457e-04  -3.625 0.000315 ***\nDiet4       -3.463e-03  7.391e-04  -4.685  3.5e-06 ***\nTime:Diet2   4.940e-05  4.484e-05   1.102 0.271037    \nTime:Diet3   4.714e-05  4.208e-05   1.120 0.263050    \nTime:Diet4   1.031e-04  4.249e-05   2.426 0.015570 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.07498304)\n\n    Null deviance: 193.022  on 577  degrees of freedom\nResidual deviance:  42.305  on 570  degrees of freedom\nAIC: 5512.8\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Chick \\;Weight = Gamma(y',\\alpha)\\]\n\\[y'=y^{-1}\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} Time:Diet \\;1 \\;vs \\;4\\\\\n+ \\beta_{2} Time:Diet \\;1 \\;vs \\;3\\\\\n+ \\beta_{3} Time:Diet \\;1 \\;vs \\;2\\\\\n+ \\beta_{4} Diet \\;1 \\;vs \\;4\\\\\n+ \\beta_{5} Diet \\;1 \\;vs \\;3\\\\\n+ \\beta_{6} Diet \\;1 \\;vs \\;2\\\\\n+ \\beta_{7} Time\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Gamma distribution requires two shape parameters (\\(y'\\) and \\(\\alpha\\)), where \\(y'\\) must be above zero, we must convert out linear equation results (\\(y\\)) so that it is positive. This means we use the link function, which for Gamma models is by default a inverse. We can use a different link function if we want, for this example we used this default.\n\nglm2$family$link\n\n[1] \"inverse\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age and diet the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average weight based on those ages and diets.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(Time=seq(min(ChickWeight$Time),max(ChickWeight$Time),length.out=100),\n                       Diet=as.factor(c(1:4)))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Time,ymax=Upr,ymin=Lwr,fill=Diet),\n              alpha=0.6)+\n  geom_line(aes(x=Time,y=response,colour=Diet,linetype=Diet))+\n  labs(x=\"Age\",y=\"Predicted Chick Weight (g)\",\n       fill=\"Diet\",colour=\"Diet\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=ChickWeight,aes(x=Time,y=weight,colour=Diet),\n             alpha=0.5,\n             size=0.6)+\n  geom_ribbon(aes(x=Time,ymax=Upr,ymin=Lwr,fill=Diet),\n              alpha=0.6)+\n  geom_line(aes(x=Time,y=response,colour=Diet,linetype=Diet))+\n  labs(x=\"Age\",y=\"Predicted Chick Weight (g)\",\n       fill=\"Diet\",colour=\"Diet\")+\n  scale_fill_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  scale_colour_manual(values = c(\"darkcyan\",\"darkorange\",\"grey80\",\"forestgreen\"))+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo again our model seems pretty good, with minimal differences between treatments and strong increase in weight with Age of Chick. But some of this isn’t great, such as very low values being over predicted, plus some clear lines of data well outside the models.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Gamma GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/PoissonGLMs.html",
    "href": "StatisticsTutorials/PoissonGLMs.html",
    "title": "Poisson GLMs",
    "section": "",
    "text": "Lets use a real-world dataset. This data set is the number of Plant species on different islands in the Galapagos Islands, how many of those species are endemic, the area of the island, its max elevation, the distance to the nearest island, its distance to santa cruz (the most populace island) and the area of the nearest island.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(faraway)\nlibrary(patchwork)\n\ndata(\"gala\")\n\nglimpse(gala)\n\nRows: 30\nColumns: 7\n$ Species   &lt;dbl&gt; 58, 31, 3, 25, 2, 18, 24, 10, 8, 2, 97, 93, 58, 5, 40, 347, …\n$ Endemics  &lt;dbl&gt; 23, 21, 3, 9, 1, 11, 0, 7, 4, 2, 26, 35, 17, 4, 19, 89, 23, …\n$ Area      &lt;dbl&gt; 25.09, 1.24, 0.21, 0.10, 0.05, 0.34, 0.08, 2.33, 0.03, 0.18,…\n$ Elevation &lt;dbl&gt; 346, 109, 114, 46, 77, 119, 93, 168, 71, 112, 198, 1494, 49,…\n$ Nearest   &lt;dbl&gt; 0.6, 0.6, 2.8, 1.9, 1.9, 8.0, 6.0, 34.1, 0.4, 2.6, 1.1, 4.3,…\n$ Scruz     &lt;dbl&gt; 0.6, 26.3, 58.7, 47.4, 1.9, 8.0, 12.0, 290.2, 0.4, 50.2, 88.…\n$ Adjacent  &lt;dbl&gt; 1.84, 572.33, 0.78, 0.18, 903.82, 1.84, 0.34, 2.85, 17.95, 0…\n\n\n\n\n\n\nAs an archipelago of volcanic islands the Galapagos were formed by geological processes, these geological processes such as tectonic movement and volcanic activity will have implications for the amount of endemic plant species on an island. Therefore, lets explore the relationship of plant endemism and physical features of the islands. For this example we will assess the effect of elevation of an island on the number of endemic species on that island.\nThis is a relatively simple model with just one fixed effect and can be written as:\nCount of Endemic Plants ~ Elevation\n\n\n\nThe number of endemic species is a count response where there is no theoretical limit (although one probably exists). Therefore the values can range from 0 upwards. This tells us that is most likely a Poisson distribution.\n\n\n\nOften with highly variable numeric values, such as Elevation or Area or Population, we might need to transform our fixed effect with a log or a square root. We can assess the distribution of our Elevation variables to see if there is a a lot of variance across our islands.\n\np1&lt;-ggplot(gala,aes(x=Elevation))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(gala,aes(x=log(Elevation)))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np1+p2\n\n\n\n\n\n\n\n\nWe seem to have a good spread of values across its range with some very large values so we will use a log transformation for modelling. We can then convert back to its native scale after modelling. Lets fit the model using the glm function, we add our statistical formula with the log transformed Elevation, our data and then we specify that the family or distribution we want to use is poisson.\n\ndf&lt;-gala %&gt;% \n  mutate(Elevation_log=log(Elevation))\n\nglm1&lt;-glm(Endemics~Elevation_log,data=df, family= \"poisson\")\n\n\n\n\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nWe see some fairly mixed results here. The normality of residuals is good with almost all points following the line, whereas our homogenerity of variance is not amazing, which is likely being driven by a low number of high elevation values. However, the general patterns are quite spread but with very large values showing some overdispersion. This over dispersion might make us want to use a distribution that is more able to deal with over dispersion, such as Negative Binomial. We will accept the amount of over dispersion here as it is quite minor and only at the largest values, which are being driven by our highest value of Elevation and highest value of Endemism: Fernandina and Santa Cruz\n\nsummary(glm1)\n\n\nCall:\nglm(formula = Endemics ~ Elevation_log, family = \"poisson\", data = df)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.32549    0.23027  -5.756  8.6e-09 ***\nElevation_log  0.79088    0.03657  21.626  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 743.55  on 29  degrees of freedom\nResidual deviance: 223.50  on 28  degrees of freedom\nAIC: 360.45\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Count of Endemic Species = Poisson(\\lambda)\\]\n\\[\\lambda=log(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} log Elevation + Intercept\n\\end{aligned}\n\\]\nAs the Poisson distribution requires only one shape parameter (\\(\\lambda\\)) and this must be zero or above, we must convert out linear equation results (\\(y\\)) so that it is non-negative. This means we use the link function, which for poisson models is by default a log. We can use a different link function if we want, or even check to check which link was used.\n\nglm1$family$link\n\n[1] \"log\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just Elevation (on the log scale) the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Count of Endemic Species based on those log Elevations.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-data.frame(Elevation_log=seq(min(df$Elevation_log),max(df$Elevation_log),length.out=50))\n\nPred&lt;-predict(glm1,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         Elevation=exp(Elevation_log))\n\nggplot(NewData)+\n  geom_ribbon(aes(x=Elevation,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=Elevation,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Elevation\",y=\"Response Variable (Count of Endemic Plant Species)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output over the raw values to see how well the model has worked.\n\nggplot(NewData)+\n  geom_point(data=df,aes(x=Elevation,\n                         y=Endemics),\n              alpha=0.3,\n             size=0.8,\n             colour=\"darkcyan\")+\n  geom_ribbon(aes(x=Elevation,\n                    ymax=Upr,\n                    ymin=Lwr),\n              alpha=0.7,\n              fill=\"darkcyan\")+\n  geom_line(aes(x=Elevation,\n                 y=response),\n              colour=\"darkcyan\")+\n  labs(x=\"Elevation\",y=\"Response Variable (Count of Endemic Plant Species)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nNow this looks quite good, with more uncertainty at higher values where there are less values to influence the prediction. This is a very simplified model that is not taking into consideration many different factors. For example, Age of an island is highly influential on its plant communities as well as the volcanic activity. So from a science point of view this is not the whole story, infact the Elevation may be just a proxy for the amount of available habitat space and the potential for habitat niches that are influential on endemism. Other factors such as human occupation and the influence that has caused in Galapagos on the local plant populations should not be ignored: invasive species, agricultural practices etc.\n\n\n\nLets create a more complex poisson model. This data set is the number of epileptic seizures from 59 patients in a clinical trial of a treatment. Patients were given a a treatment of Placebo or a drug called Progabide. There is a base number of seizures for the 8 weeks before the trial and then a seizure rate for every 2 week period (up to 8 weeks) after being given a treatment. Patients Ages are also available. We will summarise all seizures had by a patient in the 4 periods post treatment to make this a simpler assessment. Although, we could have assessed an effect over time post treatment as well. (Another day perhaps)\n\n# install.packages(\"HSAUR2\")\n\nlibrary(HSAUR2)\n\nLoading required package: tools\n\n\n\nAttaching package: 'HSAUR2'\n\n\nThe following objects are masked from 'package:faraway':\n\n    epilepsy, toenail\n\n\nThe following object is masked from 'package:tidyr':\n\n    household\n\ndata(\"epilepsy\")\n\nglimpse(epilepsy)\n\nRows: 236\nColumns: 6\n$ treatment    &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ base         &lt;int&gt; 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 8, 8, 8, 8, 6…\n$ age          &lt;int&gt; 31, 31, 31, 31, 30, 30, 30, 30, 25, 25, 25, 25, 36, 36, 3…\n$ seizure.rate &lt;int&gt; 5, 3, 3, 3, 3, 5, 3, 3, 2, 4, 0, 5, 4, 4, 1, 4, 7, 18, 9,…\n$ period       &lt;ord&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, …\n$ subject      &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, …\n\ndf_epilepsy&lt;-epilepsy %&gt;% \n  group_by(age,base,subject,treatment) %&gt;% \n  summarise(seizures=sum(seizure.rate))\n\n`summarise()` has grouped output by 'age', 'base', 'subject'. You can override\nusing the `.groups` argument.\n\n\n\n\nWe will assess the number of seizures in the 8 weeks after treatment for patients and assess whether this pattern changes with age and the number of seizures they had before treatment.\nThis is a bit more complex model with two interacting fixed effect and one additional fixed effect and can be written as:\nCount of Seizures After Treatment ~ Treatment*Age + Base Number of Seizures\n\n\n\nAgain, the number of seizures can only be a non-negative integer.\n\n\n\nLets check all our fixed effects. For numeric values we can assess their distribution, categorical we can see the number of samples is relatively even.\n\np1&lt;-ggplot(df_epilepsy,aes(x=age))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(df_epilepsy,aes(x=treatment))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np3&lt;-ggplot(df_epilepsy,aes(x=base))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n  \np4&lt;-ggplot(df_epilepsy,aes(x=sqrt(base)))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\nOur factors age and treatment seem fine, but maybe we should square root the base effect so we don’t have really big base values influencing our model as much. We could log here or centre and scale but we shall use square root for now.\n\ndf&lt;-df_epilepsy %&gt;% \n  mutate(base_sqrt=sqrt(base))\n\nglm2&lt;-glm(seizures~treatment*age+base_sqrt,data=df, family= \"poisson\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs earlier, we see some fairly mixed results here. The normality of residuals is good with almost all points following the line, whereas our homogenerity of variance is again not amazing, which is likely being driven by a low number of high base seizure values values. However, the general patterns are quite spread but with very large values showing some overdispersion. This over dispersion might make us want to use a distribution that is more able to deal with over dispersion, such as Negative Binomial. We will accept the amount of over dispersion here as it is quite minor and only at the largest values.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = seizures ~ treatment * age + base_sqrt, family = \"poisson\", \n    data = df)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.483382   0.182272   2.652   0.0080 ** \ntreatmentProgabide      0.317571   0.226665   1.401   0.1612    \nage                     0.029911   0.005555   5.385 7.25e-08 ***\nbase_sqrt               0.357709   0.008901  40.187  &lt; 2e-16 ***\ntreatmentProgabide:age -0.013838   0.007903  -1.751   0.0799 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2122.73  on 58  degrees of freedom\nResidual deviance:  490.77  on 54  degrees of freedom\nAIC: 784.04\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Count of Seizures Post Treatment = Poisson(\\lambda)\\]\n\\[\\lambda=log(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} treatment:age\\\\\n+ \\beta_{2} \\sqrt{base} \\\\\n+ \\beta_{3} age\\\\\n+ \\beta_{4} treatment\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Poisson distribution requires only one shape parameter (\\(\\lambda\\)) and this must be zero or above, we must convert out linear equation results (\\(y\\)) so that it is non-negative. This means we use the link function, which for poisson models is by default a log. We can use a different link function if we want, or even check to check which link was used.\n\nglm2$family$link\n\n[1] \"log\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age, treatment and base level the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nWe will choose a low, middle and high base level\nThe model then predicts the average Count of Seizures based on those ages, treatments and base levels.\nAs we have two continuous fixed effects we could plot as heatmap style if we wanted. But then it is difficult or impossible to display confidence levels well.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(base_sqrt=seq(min(df$base_sqrt),max(df$base_sqrt),length.out=100),\n                       age=seq(min(df$age),max(df$age),length.out=50),\n                       treatment=c(\"placebo\",\"Progabide\"))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         base=base_sqrt^2,\n         treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData)+\n  geom_tile(aes(x=age,y=base,fill=response),\n            alpha=0.9)+\n  scale_y_sqrt()+\n  scale_fill_viridis_c(direction=-1)+\n  facet_wrap(~treatment)+\n  labs(x=\"Age\",y=\"Base Number of Seizures\",fill=\"Predicted Number\\nof Seizures\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\ndf_1&lt;-df %&gt;% \n  mutate(treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData)+\n  geom_tile(aes(x=age,y=base,fill=response),\n            alpha=0.9)+\n  geom_point(data=df_1,aes(x=age,y=base,fill=seizures),shape=21,colour=\"#FFFFFF50\")+\n  scale_y_sqrt()+\n  scale_fill_viridis_c(direction=-1,limits=c(0,470))+\n  scale_colour_viridis_c(direction=-1,limits=c(0,470))+\n  facet_wrap(~treatment)+\n  labs(x=\"Age\",y=\"Base Number of Seizures\",fill=\"Number\\nof Seizures\",colour=\"Number\\nof Seizures\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThe patterns of colour from points to back ground do seem to generalise well. However, from this plot we can see clearly that the model is predicting in the age/base space that is not in the original data, so perhaps it would be better to plot the model assuming an average base level of seizures then compare with the raw data. We can also display the models confidence then too.\n\nNewData_2&lt;-expand.grid(base_sqrt=sqrt(mean(df$base)),\n                       age=seq(min(df$age),max(df$age),length.out=50),\n                       treatment=c(\"placebo\",\"Progabide\"))\n\nPred_3&lt;-predict(glm2,NewData_2,se.fit=TRUE,type=\"response\")\n\nNewData_MeanBase&lt;-NewData_2 %&gt;% \n  mutate(response=Pred_3$fit,\n         se.fit=Pred_3$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         base=base_sqrt^2,\n         treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData_MeanBase)+\n  geom_point(data=df_1,aes(x=age,y=seizures,colour=treatment))+\n   geom_ribbon(aes(x=age,\n                     ymax=Upr,\n                     ymin=Lwr,\n                   fill=treatment),\n               alpha=0.7)+\n   geom_line(aes(x=age,\n                  y=response,\n                   colour=treatment))+\n   scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n   scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n   scale_y_continuous(limits=c(0,100))+\n   labs(x=\"Age\",y=\"Response Variable (Count of Seizures)\",colour=\"Treatment\",fill=\"Treatment\")+\n   theme_classic()\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nI have set the y axis to ignore very large values (above 100) so we can see clearly what the model is telling us.\nThis shows a clearer picture and helps us understand that the model has seen some differences between the treatments and that this difference becomes more distinct with age. Although the differences are very minimal between treatments. But a clear increase in Seizures with increasing age regardless of treatment. This model (which is simplistic and probably not fully adequate at addressing this question) would not give us evidence that the drug is significantly different from placebo but we might infer there is some lessening of the effect of age in the Progabide treatment.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Poisson GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/PoissonGLMs.html#data-loading-complex---epilepsy",
    "href": "StatisticsTutorials/PoissonGLMs.html#data-loading-complex---epilepsy",
    "title": "Poisson GLMs",
    "section": "",
    "text": "Lets create a more complex poisson model. This data set is the number of epileptic seizures from 59 patients in a clinical trial of a treatment. Patients were given a a treatment of Placebo or a drug called Progabide. There is a base number of seizures for the 8 weeks before the trial and then a seizure rate for every 2 week period (up to 8 weeks) after being given a treatment. Patients Ages are also available. We will summarise all seizures had by a patient in the 4 periods post treatment to make this a simpler assessment. Although, we could have assessed an effect over time post treatment as well. (Another day perhaps)\n\n# install.packages(\"HSAUR2\")\n\nlibrary(HSAUR2)\n\nLoading required package: tools\n\n\n\nAttaching package: 'HSAUR2'\n\n\nThe following objects are masked from 'package:faraway':\n\n    epilepsy, toenail\n\n\nThe following object is masked from 'package:tidyr':\n\n    household\n\ndata(\"epilepsy\")\n\nglimpse(epilepsy)\n\nRows: 236\nColumns: 6\n$ treatment    &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ base         &lt;int&gt; 11, 11, 11, 11, 11, 11, 11, 11, 6, 6, 6, 6, 8, 8, 8, 8, 6…\n$ age          &lt;int&gt; 31, 31, 31, 31, 30, 30, 30, 30, 25, 25, 25, 25, 36, 36, 3…\n$ seizure.rate &lt;int&gt; 5, 3, 3, 3, 3, 5, 3, 3, 2, 4, 0, 5, 4, 4, 1, 4, 7, 18, 9,…\n$ period       &lt;ord&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, …\n$ subject      &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, …\n\ndf_epilepsy&lt;-epilepsy %&gt;% \n  group_by(age,base,subject,treatment) %&gt;% \n  summarise(seizures=sum(seizure.rate))\n\n`summarise()` has grouped output by 'age', 'base', 'subject'. You can override\nusing the `.groups` argument.\n\n\n\n\nWe will assess the number of seizures in the 8 weeks after treatment for patients and assess whether this pattern changes with age and the number of seizures they had before treatment.\nThis is a bit more complex model with two interacting fixed effect and one additional fixed effect and can be written as:\nCount of Seizures After Treatment ~ Treatment*Age + Base Number of Seizures\n\n\n\nAgain, the number of seizures can only be a non-negative integer.\n\n\n\nLets check all our fixed effects. For numeric values we can assess their distribution, categorical we can see the number of samples is relatively even.\n\np1&lt;-ggplot(df_epilepsy,aes(x=age))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n\np2&lt;-ggplot(df_epilepsy,aes(x=treatment))+\n  geom_bar(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\np3&lt;-ggplot(df_epilepsy,aes(x=base))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n  \np4&lt;-ggplot(df_epilepsy,aes(x=sqrt(base)))+\n  geom_density(fill=\"darkcyan\",alpha=0.7)+\n  theme_classic()\n\n(p1+p2)/(p3+p4)\n\n\n\n\n\n\n\n\nOur factors age and treatment seem fine, but maybe we should square root the base effect so we don’t have really big base values influencing our model as much. We could log here or centre and scale but we shall use square root for now.\n\ndf&lt;-df_epilepsy %&gt;% \n  mutate(base_sqrt=sqrt(base))\n\nglm2&lt;-glm(seizures~treatment*age+base_sqrt,data=df, family= \"poisson\")\n\n\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs earlier, we see some fairly mixed results here. The normality of residuals is good with almost all points following the line, whereas our homogenerity of variance is again not amazing, which is likely being driven by a low number of high base seizure values values. However, the general patterns are quite spread but with very large values showing some overdispersion. This over dispersion might make us want to use a distribution that is more able to deal with over dispersion, such as Negative Binomial. We will accept the amount of over dispersion here as it is quite minor and only at the largest values.\n\nsummary(glm2)\n\n\nCall:\nglm(formula = seizures ~ treatment * age + base_sqrt, family = \"poisson\", \n    data = df)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.483382   0.182272   2.652   0.0080 ** \ntreatmentProgabide      0.317571   0.226665   1.401   0.1612    \nage                     0.029911   0.005555   5.385 7.25e-08 ***\nbase_sqrt               0.357709   0.008901  40.187  &lt; 2e-16 ***\ntreatmentProgabide:age -0.013838   0.007903  -1.751   0.0799 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2122.73  on 58  degrees of freedom\nResidual deviance:  490.77  on 54  degrees of freedom\nAIC: 784.04\n\nNumber of Fisher Scoring iterations: 5\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nLets write out the equation for the model, then we can use the values from the summary to create an equation for the model (the predict function will do this for us).\nIf we wanted to we could write out our model as this:\n\\[Count of Seizures Post Treatment = Poisson(\\lambda)\\]\n\\[\\lambda=log(y)\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} treatment:age\\\\\n+ \\beta_{2} \\sqrt{base} \\\\\n+ \\beta_{3} age\\\\\n+ \\beta_{4} treatment\\\\\n+ Intercept\n\\end{aligned}\n\\]\nAs the Poisson distribution requires only one shape parameter (\\(\\lambda\\)) and this must be zero or above, we must convert out linear equation results (\\(y\\)) so that it is non-negative. This means we use the link function, which for poisson models is by default a log. We can use a different link function if we want, or even check to check which link was used.\n\nglm2$family$link\n\n[1] \"log\"\n\n\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with age, treatment and base level the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nWe will choose a low, middle and high base level\nThe model then predicts the average Count of Seizures based on those ages, treatments and base levels.\nAs we have two continuous fixed effects we could plot as heatmap style if we wanted. But then it is difficult or impossible to display confidence levels well.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval).\nNote that here I tell Predict that I want the fit to be returned on the response scale and not the link scale.\n\nNewData_1&lt;-expand.grid(base_sqrt=seq(min(df$base_sqrt),max(df$base_sqrt),length.out=100),\n                       age=seq(min(df$age),max(df$age),length.out=50),\n                       treatment=c(\"placebo\",\"Progabide\"))\n\nPred&lt;-predict(glm2,NewData_1,se.fit=TRUE,type=\"response\")\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         base=base_sqrt^2,\n         treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData)+\n  geom_tile(aes(x=age,y=base,fill=response),\n            alpha=0.9)+\n  scale_y_sqrt()+\n  scale_fill_viridis_c(direction=-1)+\n  facet_wrap(~treatment)+\n  labs(x=\"Age\",y=\"Base Number of Seizures\",fill=\"Predicted Number\\nof Seizures\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow lets plot this model output with the raw values to see how well the model has worked.\n\ndf_1&lt;-df %&gt;% \n  mutate(treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData)+\n  geom_tile(aes(x=age,y=base,fill=response),\n            alpha=0.9)+\n  geom_point(data=df_1,aes(x=age,y=base,fill=seizures),shape=21,colour=\"#FFFFFF50\")+\n  scale_y_sqrt()+\n  scale_fill_viridis_c(direction=-1,limits=c(0,470))+\n  scale_colour_viridis_c(direction=-1,limits=c(0,470))+\n  facet_wrap(~treatment)+\n  labs(x=\"Age\",y=\"Base Number of Seizures\",fill=\"Number\\nof Seizures\",colour=\"Number\\nof Seizures\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThe patterns of colour from points to back ground do seem to generalise well. However, from this plot we can see clearly that the model is predicting in the age/base space that is not in the original data, so perhaps it would be better to plot the model assuming an average base level of seizures then compare with the raw data. We can also display the models confidence then too.\n\nNewData_2&lt;-expand.grid(base_sqrt=sqrt(mean(df$base)),\n                       age=seq(min(df$age),max(df$age),length.out=50),\n                       treatment=c(\"placebo\",\"Progabide\"))\n\nPred_3&lt;-predict(glm2,NewData_2,se.fit=TRUE,type=\"response\")\n\nNewData_MeanBase&lt;-NewData_2 %&gt;% \n  mutate(response=Pred_3$fit,\n         se.fit=Pred_3$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96),\n         base=base_sqrt^2,\n         treatment=if_else(treatment==\"placebo\",\"Placebo\",treatment))\n\nggplot(NewData_MeanBase)+\n  geom_point(data=df_1,aes(x=age,y=seizures,colour=treatment))+\n   geom_ribbon(aes(x=age,\n                     ymax=Upr,\n                     ymin=Lwr,\n                   fill=treatment),\n               alpha=0.7)+\n   geom_line(aes(x=age,\n                  y=response,\n                   colour=treatment))+\n   scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n   scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n   scale_y_continuous(limits=c(0,100))+\n   labs(x=\"Age\",y=\"Response Variable (Count of Seizures)\",colour=\"Treatment\",fill=\"Treatment\")+\n   theme_classic()\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nI have set the y axis to ignore very large values (above 100) so we can see clearly what the model is telling us.\nThis shows a clearer picture and helps us understand that the model has seen some differences between the treatments and that this difference becomes more distinct with age. Although the differences are very minimal between treatments. But a clear increase in Seizures with increasing age regardless of treatment. This model (which is simplistic and probably not fully adequate at addressing this question) would not give us evidence that the drug is significantly different from placebo but we might infer there is some lessening of the effect of age in the Progabide treatment.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Poisson GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GaussianGLMs.html",
    "href": "StatisticsTutorials/GaussianGLMs.html",
    "title": "Gaussian GLMs",
    "section": "",
    "text": "Linear Model? or General Linear Model with Gaussian Distribution? or ANOVA? or ANCOVA? There are many names for this type of model, they all effectively mean the same thing. I am going to stick to calling it a Gaussian GLM because then there isn’t a new name for every different test. If you don’t like that, use whatever term you like, but the code and interpretation will be the same.\n\n\nAs before lets use the Palmer penguins dataset and remove the NAs (as before NAs should never be remove without considering why there are NAs but here we will remove them for ease).\n\n#install.packages(\"palmerpenguins\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_noNAs&lt;-penguins%&gt;% \n  drop_na()\n\n\n\n\n\nSo now we will try prove the obvious\nDoes the flipper length of penguins change between species and between sexes?\n\n\nWhether we use an interaction or not depends on if our scientific thought believes the relationship of Species to flipper length is different between sexes (sexual dimorphism may not be consistent across species). We shall use an interaction here as we might expect some sexual dimorphism in some species while less, no or opposite sexual dimorphism in other species. If we had good reason to expect the same sexual dimorphism across these species we would not use an interaction term.\n\n\n\nAs you probably guessed from the title we will be using a Gaussian Distribution. However, flipper length would more technically be a Gamma distribution. It is Numeric, Continuous but cannot be Zero or Negative! Using a Gaussian distribution in this situation is okay as flipper length will not approach 0, so issues of modelling near the zero will not be a problem. This is due to our sampling and just the fact that you won’t ever be able to measure a penguins foot that is 1 mm. or probably less than 25 mm! Our lowest value is 174 in the data. One method could be to centre and scale the flipper_length_mm and model it with a Gamma distribution but this would be a lot of converting back and forth, especially when a Gaussian model will perform equally as well.\n\n\n\nThankfully both sex and species are already factors in the dataset so we don’t have any organising to do!\nSo lets apply our model. lm() is a function in base r that allows us to create a Gaussian GLM object. There is also a glm() function where we would need to define the distribution but lm() is easier to use for gaussian models. We will create a model object then we can inspect and use this model.\n\nlm2.1&lt;-lm(flipper_length_mm~species*sex,data=penguins_noNAs)\n\n\n\n\nWe could apply a linear model to almost all data but often it will not meet our assumptions.\nWe can now check visually the residuals from our model.\nBy using the base plot function in 4 we get 4 plots. The first two are the ones we are most interested in generally. The next two plots are less important generally but can be used to find out what is wrong if the first two plots are not as we want them.\nFor the Residuals vs Fitted plot we want the data to be evenly spread from right to left, meaning the difference between the model and the data (residuals) are not generally larger or smaller at higher or lower values of the model.\nThe next important plot is the qq plot, this is best if the points follow line of x=y, which is the dotted line behind the points.\n\nplot(lm2.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is annoying as we have to press enter in the console to see all the plots.\nWe will code the residuals and qqnorm plots. This is relatively simple code, there are helper functions such as part of the performance package but they are quite unstable from my experience.\n\n#install.packages(\"patchwork\")\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm2.1),\n                  Residuals=resid(lm2.1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs we only have factors in our model we don’t see a ‘cloud’ of points, but the residuals are evenly spread above and below 0 so this is good.\nAs the diagnostics are good we can look at the results\n\nsummary(lm2.1)\n\n\nCall:\nlm(formula = flipper_length_mm ~ species * sex, data = penguins_noNAs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7945  -3.4110   0.0882   3.4590  17.5890 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              187.7945     0.6619 283.721  &lt; 2e-16 ***\nspeciesChinstrap           3.9408     1.1742   3.356 0.000884 ***\nspeciesGentoo             24.9124     0.9947  25.044  &lt; 2e-16 ***\nsexmale                    4.6164     0.9361   4.932  1.3e-06 ***\nspeciesChinstrap:sexmale   3.5600     1.6606   2.144 0.032782 *  \nspeciesGentoo:sexmale      4.2176     1.3971   3.019 0.002737 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.655 on 327 degrees of freedom\nMultiple R-squared:  0.8396,    Adjusted R-squared:  0.8372 \nF-statistic: 342.4 on 5 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nI find the best way to interpret a model output is to plot the model results.\nFirst lets re-plot the raw data, boxplots are probably the best for categorical factors but we could also use a half eye distribution plot. Lets look at both.\nThe best way to do this (in my opinion) is using the patchwork package that can combine the plots\nWe create a plot for each, then plot them side by side. Patchwork is a very simple way of combining ggplots together.\nWe can re-use some of our code from the intro for appearance and colours\n\n#install.packages(\"ggdist\")\nlibrary(ggdist)\n\np_box&lt;-ggplot(penguins_noNAs)+\n  geom_boxplot(aes(x=species,\n                   y=flipper_length_mm,\n                   fill=sex))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\np_cloud&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,colour=sex))+ \n  stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\np_box+p_cloud\n\n\n\n\n\n\n\n\nNow we can also see what the model believes about our data\nThe model has estimated parameters of a linear model.\nIf we wanted to we could write out our model as this:\n\\[FlipperLength = Gaussian(y',\\sigma)\\]\n\\[y'=y\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} sex(female-male):species(Adelie-Chinstrap) \\\\\n+  \\\\\n\\beta_{2} sex(female-male):species(Adelie-Gentoo) \\\\\n+  \\\\\n\\beta_{3} sex(female-male) \\\\\n+  \\\\\n\\beta_{4} species(Adelie-Chinstrap) \\\\\n+  \\\\\n\\beta_{5} species(Adelie-Gentoo) + Intercept\n\\end{aligned}\n\\]\nAs the Gaussian distribution has no data restrictions for the mean value the link function is nothing. Seems silly to include here, and many people wouldn’t, but it will keep consistency for later when we do have a link function.\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just sex and species the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Flipper length in mm based on those species and sexes.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval)\n\nNewData_1&lt;-expand.grid(sex=c(\"female\",\"male\"),\n                     species=c(\"Adelie\",\"Chinstrap\",\"Gentoo\"))\n\nPred&lt;-predict(lm2.1,NewData_1,se.fit=TRUE)\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_point(aes(x=species,\n                 y=response,\n                 colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nLets look at both of these plots next to each other,\nFirst we save both the raw data boxplot as one object and the predicted plot as another then we plot them side by side\n\nPlot1&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,\n                    colour=sex))+ \n  ggdist::stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()+\n  theme(legend.position = \"none\")\n\nPlot2&lt;-ggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Modelled Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\nPlot1+Plot2\n\n\n\n\n\n\n\n\nAt first look this is quite good, but maybe some polishing is needed, mostly the y axis range\nThere are multiple ways to change this for example setting them both to the same range with scale_y_continuous()\n\nPlot1&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,colour=sex))+ \n  ggdist::stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()+\n  scale_y_continuous(limits=c(170,240))+\n  theme(legend.position = \"none\")\n\nPlot2&lt;-ggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Modelled Response Variable (Flipper Length (mm))\")+\n    scale_y_continuous(limits=c(170,240))+\n  theme_classic()\n\nPlot1+Plot2\n\n\n\n\n\n\n\n\nThis is better, although we could actually plot both the modelled and raw data on one plot.\nWe can use a position=position_jitterdodge() to have the raw data not all in one line above their species\n\nggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  geom_point(data=penguins_noNAs,aes(x=species,\n                                      y=flipper_length_mm,\n                                      colour=sex),\n                position=position_jitterdodge(jitter.width = 0.4,\n                                              dodge.width = 0.8),\n              alpha=0.3,\n             size=0.5)+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nOkay that is what we do with linear models of categorical factors.\nBut what if we want to see the relationship between flipper_length_mm and bill_length_mm.\n\n\nWe know there are species differences and sexual differences in flipper length.\nAs males always tend to be larger lets just assess species differences in their flipper to bill relationship.\nflipper_length_mm~species*bill_length_mm\nLets plot the raw data first\n\nggplot(penguins_noNAs)+\n  geom_point(aes(x=bill_length_mm,y=flipper_length_mm,colour=species))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Flipper Length (mm)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can see from the raw data that we will expect to find some interesting relationships\n\n\n\nNothing has changed from above, so lets stick to Gaussian\n\n\n\nThe fixed effects again need little to no prep so lets apply the model.\n\nlm3.1&lt;-lm(flipper_length_mm~species*bill_length_mm,data=penguins_noNAs)\n\n\n\n\nLets check the plots and then the model summary.\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm3.1),\n                  Residuals=resid(lm3.1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\nsummary(lm3.1)\n\n\nCall:\nlm(formula = flipper_length_mm ~ species * bill_length_mm, data = penguins_noNAs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.0561  -3.2927  -0.1646   3.5212  16.2890 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     158.5047     7.0383  22.520  &lt; 2e-16 ***\nspeciesChinstrap                -11.8689    12.5448  -0.946   0.3448    \nspeciesGentoo                    -8.2555    10.8008  -0.764   0.4452    \nbill_length_mm                    0.8139     0.1809   4.500 9.46e-06 ***\nspeciesChinstrap:bill_length_mm   0.1934     0.2788   0.694   0.4884    \nspeciesGentoo:bill_length_mm      0.5943     0.2495   2.382   0.0178 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.799 on 327 degrees of freedom\nMultiple R-squared:  0.8314,    Adjusted R-squared:  0.8288 \nF-statistic: 322.5 on 5 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nAs we hypothesised before modelling that there would be different bill to flipper relationships between species\nThe interaction model follows our scientific assumptions.\nTherefore, it would be incorrect to use lower complexity models (without the interaction for example)\n\n\n\nTo predict again we want to create lines for each species.\nTo do this we want to create fake bill length data over the same range for each species\nHere we will use the seq() function again that creates a sequence of values from your first number to your last number\nAnd you can chose the length of the vector it creates or the distance between each individual value\n\nNewData_&lt;-expand.grid(bill_length_mm=seq(from=min(penguins_noNAs$bill_length_mm),\n                                        to=max(penguins_noNAs$bill_length_mm),\n                                        length.out=1000),\n                     species=c(\"Adelie\",\"Chinstrap\",\"Gentoo\"))\n\nAs the different species won’t be across all of these bill length ranges\nWe should also remove values outside of each species range\nThere would be many ways to do it, here we will use multiple dplyr functions together\nThis is where having the pip function helps keep the order of functions that are applied clear\nFirst we create a df for each Species with their max and min bill lengths\nThen we use case_when (a more sophisticated version of if_else()) to create a new column in our new df that either says Good or it will have NAs\nWe then filter all rows that have NAs in them, thus removing bill lengths outside of each species’ range.\n\nGentoo_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Gentoo\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\nAdelie_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Adelie\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\nChinstrap_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Chinstrap\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\n\nNewData_3&lt;-NewData_ %&gt;% \n  mutate(Range=case_when(species==\"Gentoo\" &\n                              bill_length_mm&gt;=Gentoo_Range$min &\n                              bill_length_mm&lt;=Gentoo_Range$max~\"Good\",\n                         species==\"Adelie\" &\n                           bill_length_mm&gt;=Adelie_Range$min &\n                           bill_length_mm&lt;=Adelie_Range$max~\"Good\",\n                         species==\"Chinstrap\" &\n                           bill_length_mm&gt;=Chinstrap_Range$min &\n                           bill_length_mm&lt;=Chinstrap_Range$max~\"Good\"\n  )) %&gt;% \n  filter(!Range%in%NA) %&gt;% \n  select(-Range)\n\nAfter bad range values are filtered out we use the select function to remove the “Range” column, we do this with the - operator.\n\nPred_2&lt;-predict(lm3.1,NewData_3,se.fit=TRUE)\n\nNewData_2&lt;-NewData_3 %&gt;% \n  mutate(response=Pred_2$fit,\n         se.fit=Pred_2$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nSo now we have many data points that can be used to draw the linear model outputs\n\nggplot()+\n  geom_ribbon(data=NewData_2,mapping=aes(x=bill_length_mm,ymax=Upr,\n                                                 ymin=Lwr,fill=species),\n              alpha=0.4)+\n  geom_line(data=NewData_2,mapping=aes(x=bill_length_mm,y=response,colour=species),\n             alpha=0.4)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks good but lets maybe add the raw data values onto the same figure as the model outputs as before\n\nggplot()+\n  geom_point(data=penguins_noNAs,mapping = aes(x=bill_length_mm,\n                                               y=flipper_length_mm,\n                                               colour=species),\n             alpha=0.4,size=0.8)+\n  geom_ribbon(data=NewData_2,mapping=aes(x=bill_length_mm,ymax=Upr,\n                                         ymin=Lwr,fill=species),\n              alpha=0.4)+\n  geom_line(data=NewData_2,mapping=aes(x=bill_length_mm,y=response,colour=species),\n            alpha=0.4)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Gaussian GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GaussianGLMs.html#data-loading---palmer-penguins",
    "href": "StatisticsTutorials/GaussianGLMs.html#data-loading---palmer-penguins",
    "title": "Gaussian GLMs",
    "section": "",
    "text": "As before lets use the Palmer penguins dataset and remove the NAs (as before NAs should never be remove without considering why there are NAs but here we will remove them for ease).\n\n#install.packages(\"palmerpenguins\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_noNAs&lt;-penguins%&gt;% \n  drop_na()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Gaussian GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GaussianGLMs.html#modelling-with-categorical-predictor-variables",
    "href": "StatisticsTutorials/GaussianGLMs.html#modelling-with-categorical-predictor-variables",
    "title": "Gaussian GLMs",
    "section": "",
    "text": "So now we will try prove the obvious\nDoes the flipper length of penguins change between species and between sexes?\n\n\nWhether we use an interaction or not depends on if our scientific thought believes the relationship of Species to flipper length is different between sexes (sexual dimorphism may not be consistent across species). We shall use an interaction here as we might expect some sexual dimorphism in some species while less, no or opposite sexual dimorphism in other species. If we had good reason to expect the same sexual dimorphism across these species we would not use an interaction term.\n\n\n\nAs you probably guessed from the title we will be using a Gaussian Distribution. However, flipper length would more technically be a Gamma distribution. It is Numeric, Continuous but cannot be Zero or Negative! Using a Gaussian distribution in this situation is okay as flipper length will not approach 0, so issues of modelling near the zero will not be a problem. This is due to our sampling and just the fact that you won’t ever be able to measure a penguins foot that is 1 mm. or probably less than 25 mm! Our lowest value is 174 in the data. One method could be to centre and scale the flipper_length_mm and model it with a Gamma distribution but this would be a lot of converting back and forth, especially when a Gaussian model will perform equally as well.\n\n\n\nThankfully both sex and species are already factors in the dataset so we don’t have any organising to do!\nSo lets apply our model. lm() is a function in base r that allows us to create a Gaussian GLM object. There is also a glm() function where we would need to define the distribution but lm() is easier to use for gaussian models. We will create a model object then we can inspect and use this model.\n\nlm2.1&lt;-lm(flipper_length_mm~species*sex,data=penguins_noNAs)\n\n\n\n\nWe could apply a linear model to almost all data but often it will not meet our assumptions.\nWe can now check visually the residuals from our model.\nBy using the base plot function in 4 we get 4 plots. The first two are the ones we are most interested in generally. The next two plots are less important generally but can be used to find out what is wrong if the first two plots are not as we want them.\nFor the Residuals vs Fitted plot we want the data to be evenly spread from right to left, meaning the difference between the model and the data (residuals) are not generally larger or smaller at higher or lower values of the model.\nThe next important plot is the qq plot, this is best if the points follow line of x=y, which is the dotted line behind the points.\n\nplot(lm2.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is annoying as we have to press enter in the console to see all the plots.\nWe will code the residuals and qqnorm plots. This is relatively simple code, there are helper functions such as part of the performance package but they are quite unstable from my experience.\n\n#install.packages(\"patchwork\")\n\nlibrary(patchwork)\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm2.1),\n                  Residuals=resid(lm2.1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs we only have factors in our model we don’t see a ‘cloud’ of points, but the residuals are evenly spread above and below 0 so this is good.\nAs the diagnostics are good we can look at the results\n\nsummary(lm2.1)\n\n\nCall:\nlm(formula = flipper_length_mm ~ species * sex, data = penguins_noNAs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7945  -3.4110   0.0882   3.4590  17.5890 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              187.7945     0.6619 283.721  &lt; 2e-16 ***\nspeciesChinstrap           3.9408     1.1742   3.356 0.000884 ***\nspeciesGentoo             24.9124     0.9947  25.044  &lt; 2e-16 ***\nsexmale                    4.6164     0.9361   4.932  1.3e-06 ***\nspeciesChinstrap:sexmale   3.5600     1.6606   2.144 0.032782 *  \nspeciesGentoo:sexmale      4.2176     1.3971   3.019 0.002737 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.655 on 327 degrees of freedom\nMultiple R-squared:  0.8396,    Adjusted R-squared:  0.8372 \nF-statistic: 342.4 on 5 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nOkay there are a lot of numbers here but what does it actually mean?\nI find the best way to interpret a model output is to plot the model results.\nFirst lets re-plot the raw data, boxplots are probably the best for categorical factors but we could also use a half eye distribution plot. Lets look at both.\nThe best way to do this (in my opinion) is using the patchwork package that can combine the plots\nWe create a plot for each, then plot them side by side. Patchwork is a very simple way of combining ggplots together.\nWe can re-use some of our code from the intro for appearance and colours\n\n#install.packages(\"ggdist\")\nlibrary(ggdist)\n\np_box&lt;-ggplot(penguins_noNAs)+\n  geom_boxplot(aes(x=species,\n                   y=flipper_length_mm,\n                   fill=sex))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\np_cloud&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,colour=sex))+ \n  stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\np_box+p_cloud\n\n\n\n\n\n\n\n\nNow we can also see what the model believes about our data\nThe model has estimated parameters of a linear model.\nIf we wanted to we could write out our model as this:\n\\[FlipperLength = Gaussian(y',\\sigma)\\]\n\\[y'=y\\]\n\\[\n\\begin{aligned}\ny = \\beta_{1} sex(female-male):species(Adelie-Chinstrap) \\\\\n+  \\\\\n\\beta_{2} sex(female-male):species(Adelie-Gentoo) \\\\\n+  \\\\\n\\beta_{3} sex(female-male) \\\\\n+  \\\\\n\\beta_{4} species(Adelie-Chinstrap) \\\\\n+  \\\\\n\\beta_{5} species(Adelie-Gentoo) + Intercept\n\\end{aligned}\n\\]\nAs the Gaussian distribution has no data restrictions for the mean value the link function is nothing. Seems silly to include here, and many people wouldn’t, but it will keep consistency for later when we do have a link function.\nWhen we plot the estimates into this equation, this should be similar to our raw data but not identical. Remember we are creating a model to Generalise the patterns of the raw data, not copy them!\n\n\n\nThankfully we don’t have to extract each \\(\\beta\\) parameter from the summary table as R has useful functions that can do this for us! To do this we make simulated raw data with the same predictor variables in.\nWe then use the model to predict() the response variable based on those predictor variables.\nTherefore, we make a data set with just sex and species the same as our original data (be careful of spelling and capitalisation, R wants it identical).\nThe model then predicts the average Flipper length in mm based on those species and sexes.\nWe can also tell the predict function to predict error (Standard Error here that we then convert to an approximation of the 95% confidence interval)\n\nNewData_1&lt;-expand.grid(sex=c(\"female\",\"male\"),\n                     species=c(\"Adelie\",\"Chinstrap\",\"Gentoo\"))\n\nPred&lt;-predict(lm2.1,NewData_1,se.fit=TRUE)\n\nNewData&lt;-NewData_1 %&gt;% \n  mutate(response=Pred$fit,\n         se.fit=Pred$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nggplot(NewData)+\n  geom_point(aes(x=species,\n                 y=response,\n                 colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,\n                    ymax=Upr,\n                    ymin=Lwr,\n                    colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nLets look at both of these plots next to each other,\nFirst we save both the raw data boxplot as one object and the predicted plot as another then we plot them side by side\n\nPlot1&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,\n                    colour=sex))+ \n  ggdist::stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()+\n  theme(legend.position = \"none\")\n\nPlot2&lt;-ggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Modelled Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\nPlot1+Plot2\n\n\n\n\n\n\n\n\nAt first look this is quite good, but maybe some polishing is needed, mostly the y axis range\nThere are multiple ways to change this for example setting them both to the same range with scale_y_continuous()\n\nPlot1&lt;-ggplot(penguins_noNAs,\n                aes(x=species,\n                    y=flipper_length_mm,\n                    fill=sex,colour=sex))+ \n  ggdist::stat_halfeye(adjust = .5,\n                       width = .6, \n                       .width = c(.5, .95),\n                       alpha=0.7,\n                       position = position_dodge()) +\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()+\n  scale_y_continuous(limits=c(170,240))+\n  theme(legend.position = \"none\")\n\nPlot2&lt;-ggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Modelled Response Variable (Flipper Length (mm))\")+\n    scale_y_continuous(limits=c(170,240))+\n  theme_classic()\n\nPlot1+Plot2\n\n\n\n\n\n\n\n\nThis is better, although we could actually plot both the modelled and raw data on one plot.\nWe can use a position=position_jitterdodge() to have the raw data not all in one line above their species\n\nggplot(NewData)+\n  geom_point(aes(x=species,y=response,colour=sex),\n             position=position_dodge(0.8))+\n  geom_errorbar(aes(x=species,ymax=Upr,\n                    ymin=Lwr,colour=sex),\n                width=0.1,\n                position=position_dodge(0.8))+\n  geom_point(data=penguins_noNAs,aes(x=species,\n                                      y=flipper_length_mm,\n                                      colour=sex),\n                position=position_jitterdodge(jitter.width = 0.4,\n                                              dodge.width = 0.8),\n              alpha=0.3,\n             size=0.5)+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\"))+\n  labs(x=\"Species\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Gaussian GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/GaussianGLMs.html#modelling-continuous-predictor-variables",
    "href": "StatisticsTutorials/GaussianGLMs.html#modelling-continuous-predictor-variables",
    "title": "Gaussian GLMs",
    "section": "",
    "text": "Okay that is what we do with linear models of categorical factors.\nBut what if we want to see the relationship between flipper_length_mm and bill_length_mm.\n\n\nWe know there are species differences and sexual differences in flipper length.\nAs males always tend to be larger lets just assess species differences in their flipper to bill relationship.\nflipper_length_mm~species*bill_length_mm\nLets plot the raw data first\n\nggplot(penguins_noNAs)+\n  geom_point(aes(x=bill_length_mm,y=flipper_length_mm,colour=species))+\n  scale_colour_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Flipper Length (mm)\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can see from the raw data that we will expect to find some interesting relationships\n\n\n\nNothing has changed from above, so lets stick to Gaussian\n\n\n\nThe fixed effects again need little to no prep so lets apply the model.\n\nlm3.1&lt;-lm(flipper_length_mm~species*bill_length_mm,data=penguins_noNAs)\n\n\n\n\nLets check the plots and then the model summary.\n\nModelOutputs&lt;-data.frame(Fitted=fitted(lm3.1),\n                  Residuals=resid(lm3.1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\nsummary(lm3.1)\n\n\nCall:\nlm(formula = flipper_length_mm ~ species * bill_length_mm, data = penguins_noNAs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.0561  -3.2927  -0.1646   3.5212  16.2890 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     158.5047     7.0383  22.520  &lt; 2e-16 ***\nspeciesChinstrap                -11.8689    12.5448  -0.946   0.3448    \nspeciesGentoo                    -8.2555    10.8008  -0.764   0.4452    \nbill_length_mm                    0.8139     0.1809   4.500 9.46e-06 ***\nspeciesChinstrap:bill_length_mm   0.1934     0.2788   0.694   0.4884    \nspeciesGentoo:bill_length_mm      0.5943     0.2495   2.382   0.0178 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.799 on 327 degrees of freedom\nMultiple R-squared:  0.8314,    Adjusted R-squared:  0.8288 \nF-statistic: 322.5 on 5 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nAs we hypothesised before modelling that there would be different bill to flipper relationships between species\nThe interaction model follows our scientific assumptions.\nTherefore, it would be incorrect to use lower complexity models (without the interaction for example)\n\n\n\nTo predict again we want to create lines for each species.\nTo do this we want to create fake bill length data over the same range for each species\nHere we will use the seq() function again that creates a sequence of values from your first number to your last number\nAnd you can chose the length of the vector it creates or the distance between each individual value\n\nNewData_&lt;-expand.grid(bill_length_mm=seq(from=min(penguins_noNAs$bill_length_mm),\n                                        to=max(penguins_noNAs$bill_length_mm),\n                                        length.out=1000),\n                     species=c(\"Adelie\",\"Chinstrap\",\"Gentoo\"))\n\nAs the different species won’t be across all of these bill length ranges\nWe should also remove values outside of each species range\nThere would be many ways to do it, here we will use multiple dplyr functions together\nThis is where having the pip function helps keep the order of functions that are applied clear\nFirst we create a df for each Species with their max and min bill lengths\nThen we use case_when (a more sophisticated version of if_else()) to create a new column in our new df that either says Good or it will have NAs\nWe then filter all rows that have NAs in them, thus removing bill lengths outside of each species’ range.\n\nGentoo_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Gentoo\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\nAdelie_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Adelie\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\nChinstrap_Range&lt;-penguins_noNAs %&gt;% \n  filter(species==\"Chinstrap\") %&gt;% \n  summarise(min=min(bill_length_mm),\n            max=max(bill_length_mm))\n\n\nNewData_3&lt;-NewData_ %&gt;% \n  mutate(Range=case_when(species==\"Gentoo\" &\n                              bill_length_mm&gt;=Gentoo_Range$min &\n                              bill_length_mm&lt;=Gentoo_Range$max~\"Good\",\n                         species==\"Adelie\" &\n                           bill_length_mm&gt;=Adelie_Range$min &\n                           bill_length_mm&lt;=Adelie_Range$max~\"Good\",\n                         species==\"Chinstrap\" &\n                           bill_length_mm&gt;=Chinstrap_Range$min &\n                           bill_length_mm&lt;=Chinstrap_Range$max~\"Good\"\n  )) %&gt;% \n  filter(!Range%in%NA) %&gt;% \n  select(-Range)\n\nAfter bad range values are filtered out we use the select function to remove the “Range” column, we do this with the - operator.\n\nPred_2&lt;-predict(lm3.1,NewData_3,se.fit=TRUE)\n\nNewData_2&lt;-NewData_3 %&gt;% \n  mutate(response=Pred_2$fit,\n         se.fit=Pred_2$se.fit,\n         Upr=response+(se.fit*1.96),\n         Lwr=response-(se.fit*1.96))\n\nSo now we have many data points that can be used to draw the linear model outputs\n\nggplot()+\n  geom_ribbon(data=NewData_2,mapping=aes(x=bill_length_mm,ymax=Upr,\n                                                 ymin=Lwr,fill=species),\n              alpha=0.4)+\n  geom_line(data=NewData_2,mapping=aes(x=bill_length_mm,y=response,colour=species),\n             alpha=0.4)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks good but lets maybe add the raw data values onto the same figure as the model outputs as before\n\nggplot()+\n  geom_point(data=penguins_noNAs,mapping = aes(x=bill_length_mm,\n                                               y=flipper_length_mm,\n                                               colour=species),\n             alpha=0.4,size=0.8)+\n  geom_ribbon(data=NewData_2,mapping=aes(x=bill_length_mm,ymax=Upr,\n                                         ymin=Lwr,fill=species),\n              alpha=0.4)+\n  geom_line(data=NewData_2,mapping=aes(x=bill_length_mm,y=response,colour=species),\n            alpha=0.4)+\n  scale_color_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  scale_fill_manual(values=c(\"darkcyan\",\"darkorange\",\"grey30\"))+\n  labs(x=\"Bill Length (mm)\",y=\"Response Variable (Flipper Length (mm))\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Gaussian GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ZeroInflatedGLMs.html",
    "href": "StatisticsTutorials/ZeroInflatedGLMs.html",
    "title": "Zero Inflated GLMs",
    "section": "",
    "text": "So in the common issues with GLMs tutorial we talked about zero inflated data and how this type of data will cause a issues, often diagnosed by residuals plots. Here we will have some examples of discovering zero inflated data then how to model more appropriately using zero inflated distributions often called ZIP models. But first a little bit of theory and thought.\n\n\nAs we have mentioned throughout the tutorials, the cause of data allows us best to model it and discover or describe natural phenomena. This structure data allows us to effectively create models that capture the causal links between factors. Likewise, with high levels of zeros we want to know why there are lots of zeros? Are the Zeros True Zeros? or are they false zeros?\n\nA false zero is often called non-detection, as with the experimental fish example, if we have a net with holes bigger than a certain size of fish we will get lots of zeros for that fish, even if they were in the net but just swam through before we counted the sample.\nA true zero would be there were non of that fish there. This is almost impossible to know for sure but if we suspect the zeros are true zeros (we didn’t use a net, we used a bucket and nothing could escape if it was there) then we can model those zeros as well as the counts we do get in a very similar way.\n\nWe need to decide if we want to model the process of the zeros occuring or not. This will be case specific.\n\n\n\nWhat better way to explore poisson models than with fish data. We will use the remotes package to install a package from github called stats4nr. Within this package there is a fishing data set of fish caught in counts with livebait, whether they came in a camper, number of persons in the group and number of children in the group.\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"mbrussell/stats4nr\")\n\nlibrary(stats4nr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\ndata(fishing)\n\nsummary(fishing)\n\n     nofish         livebait         camper         persons     \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :1.000  \n 1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.000   1st Qu.:2.000  \n Median :0.000   Median :1.000   Median :1.000   Median :2.000  \n Mean   :0.296   Mean   :0.864   Mean   :0.588   Mean   :2.528  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:4.000  \n Max.   :1.000   Max.   :1.000   Max.   :1.000   Max.   :4.000  \n     child           count        \n Min.   :0.000   Min.   :  0.000  \n 1st Qu.:0.000   1st Qu.:  0.000  \n Median :0.000   Median :  0.000  \n Mean   :0.684   Mean   :  3.296  \n 3rd Qu.:1.000   3rd Qu.:  2.000  \n Max.   :3.000   Max.   :149.000  \n\nglm1&lt;-glm(count~persons + child + camper,family = \"poisson\", data = fishing)\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs expected this isn’t very good at all, with much of the residuals all around zero, maybe there are too many zeros? Lets look at the distribution of count data.\n\nggplot(fishing)+\n  geom_bar(aes(count),fill=\"darkcyan\")+\n  labs(x=\"Count of Fish Caught\", y=\"Count\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWell there are a lot of zeros there! But remember poisson will break down when the variance isn’t proportional to the expected value, so we cal look at the variance and the mean and see how similar they are.\n\nmean(fishing$count)\n\n[1] 3.296\n\nvar(fishing$count)\n\n[1] 135.3739\n\n\nDefinitely not. This shows extreme over dispersion. We have discussed using a Negative Binomial model for this situation. But as the name of tutorial shows we are looking at Zero-Inflated models. But lets try the negative binomial anyway..\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nglm2&lt;- glm.nb(count ~ persons + child + camper, \n              data = fishing)\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nIt does go some of the way, but there are still issues around the zero values in the Homogeneity of variance plot! Okay lets get on task then, we will use the zeroinfl() function from the pscl package. We will need to extract the fitted and residuals to plot ourselves as the check_model() function doesn’t support this glm type yet.\n\n#install.packages(\"pscl\")\nlibrary(pscl)\n\nClasses and Methods for R originally developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University (2002-2015),\nby and under the direction of Simon Jackman.\nhurdle and zeroinfl functions by Achim Zeileis.\n\nglm3 &lt;- zeroinfl(count ~ persons + child + camper|  persons+child+camper,\n                 data = fishing, dist = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm3),\n                  Residuals=resid(glm3))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nThis is definitely still not great.\n\n\n\nFor a full example we will actually make up some data. Lets pretend we have count of fish inside and outside of a Marine Protected Area (MPA). We will also pretend our MPA is unbelieveablly good at providing protection to the species of fish we are looking at. But we have some unknown situation that means that sometimes we have high levels of zero records.\nWe will first make our factors, Time and MPA, then we will create poisson data where lambda is influenced by both the MPA and Time factors. Lets visualise each factor so we can see how we have created the data and what our data look like. Don’t forget we still need to add in the zero-inflation. We will create lambda from a linear equation using b0, b1, b2 and b3 as the intercept and the effects of Time, MPA, and interaction of Time and MPA.\n\nlibrary(patchwork)\n\nn &lt;- 5000\n\nMPA &lt;- sample(c(0,1), size = n, replace = TRUE)\n\nTime &lt;- c(1:100)\n\nb0&lt;-log(2) # Intercept\n\nb1&lt;-log(1.1) # Effect of Time\n\nb2&lt;-log(1.2) # Effect of MPA\n\nb3&lt;-log(1.3) # Effect of interaction between Time and MPA\n\nlambda&lt;-exp(b0 + b1 * log(Time) + b2 * (MPA==1) + b3 * (MPA==1) * log(Time))\n\ny_sim &lt;- rpois(n = n, lambda = lambda) \n\ndf&lt;-data.frame(MPA=as.factor(MPA),\n               Time=Time,\n               Count=y_sim)\n\np6&lt;-ggplot(df)+\n    geom_bar(aes(x=MPA),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"MPA\")\n\np7&lt;-ggplot(df)+\n    geom_bar(aes(x=Time),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"Time\")\n\np8&lt;-ggplot(df)+\n    geom_bar(aes(x=Count),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"Fish Count\")\n\np6+p7+p8\n\n\n\n\n\n\n\nggplot(df)+\n    geom_point(aes(x=Time,y=Count,colour=MPA))+\n    theme_classic()+\n    scale_colour_manual(values=c(\"darkgoldenrod\",\"darkcyan\"))+\n    labs(y=\"Fish Count\",x=\"Time\")\n\n\n\n\n\n\n\n\nNow lets create a high number of zeros, which are going to more likely to occur (with a probability of 0.1) if it is inside the MPA but not difference with time.\n\ndf1&lt;-df %&gt;% \n  mutate(Count_Zeros=Count*rbinom(MPA, size = 1, prob=0.7))\n  \n\nggplot(df1)+\n    geom_point(aes(x=Time,y=Count_Zeros,colour=MPA))+\n    theme_classic()+\n    scale_colour_manual(values=c(\"darkgoldenrod\",\"darkcyan\"))+\n    labs(y=\"Fish Count\",x=\"Time\")\n\n\n\n\n\n\n\n\nNow lets try model this.\n\nglm4 &lt;- glm(Count_Zeros ~ MPA*Time,\n                 data = df1, family = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm4),\n                  Residuals=resid(glm4))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nWell this model looks pretty bad, not surprisingly.\n\nglm5 &lt;- zeroinfl(Count_Zeros ~ MPA*Time|MPA,\n                 data = df1, dist = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm5),\n                  Residuals=resid(glm5))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nThis looks better, again not perfect but pretty good. A more comprehensive example will come when we do GLMMs.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Zero Inflated GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ZeroInflatedGLMs.html#cause-of-the-zeros",
    "href": "StatisticsTutorials/ZeroInflatedGLMs.html#cause-of-the-zeros",
    "title": "Zero Inflated GLMs",
    "section": "",
    "text": "As we have mentioned throughout the tutorials, the cause of data allows us best to model it and discover or describe natural phenomena. This structure data allows us to effectively create models that capture the causal links between factors. Likewise, with high levels of zeros we want to know why there are lots of zeros? Are the Zeros True Zeros? or are they false zeros?\n\nA false zero is often called non-detection, as with the experimental fish example, if we have a net with holes bigger than a certain size of fish we will get lots of zeros for that fish, even if they were in the net but just swam through before we counted the sample.\nA true zero would be there were non of that fish there. This is almost impossible to know for sure but if we suspect the zeros are true zeros (we didn’t use a net, we used a bucket and nothing could escape if it was there) then we can model those zeros as well as the counts we do get in a very similar way.\n\nWe need to decide if we want to model the process of the zeros occuring or not. This will be case specific.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Zero Inflated GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ZeroInflatedGLMs.html#zero-inflated-poisson-data",
    "href": "StatisticsTutorials/ZeroInflatedGLMs.html#zero-inflated-poisson-data",
    "title": "Zero Inflated GLMs",
    "section": "",
    "text": "What better way to explore poisson models than with fish data. We will use the remotes package to install a package from github called stats4nr. Within this package there is a fishing data set of fish caught in counts with livebait, whether they came in a camper, number of persons in the group and number of children in the group.\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"mbrussell/stats4nr\")\n\nlibrary(stats4nr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\ndata(fishing)\n\nsummary(fishing)\n\n     nofish         livebait         camper         persons     \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :1.000  \n 1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.000   1st Qu.:2.000  \n Median :0.000   Median :1.000   Median :1.000   Median :2.000  \n Mean   :0.296   Mean   :0.864   Mean   :0.588   Mean   :2.528  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:4.000  \n Max.   :1.000   Max.   :1.000   Max.   :1.000   Max.   :4.000  \n     child           count        \n Min.   :0.000   Min.   :  0.000  \n 1st Qu.:0.000   1st Qu.:  0.000  \n Median :0.000   Median :  0.000  \n Mean   :0.684   Mean   :  3.296  \n 3rd Qu.:1.000   3rd Qu.:  2.000  \n Max.   :3.000   Max.   :149.000  \n\nglm1&lt;-glm(count~persons + child + camper,family = \"poisson\", data = fishing)\n\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm1),\n                  Residuals=resid(glm1))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nAs expected this isn’t very good at all, with much of the residuals all around zero, maybe there are too many zeros? Lets look at the distribution of count data.\n\nggplot(fishing)+\n  geom_bar(aes(count),fill=\"darkcyan\")+\n  labs(x=\"Count of Fish Caught\", y=\"Count\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nWell there are a lot of zeros there! But remember poisson will break down when the variance isn’t proportional to the expected value, so we cal look at the variance and the mean and see how similar they are.\n\nmean(fishing$count)\n\n[1] 3.296\n\nvar(fishing$count)\n\n[1] 135.3739\n\n\nDefinitely not. This shows extreme over dispersion. We have discussed using a Negative Binomial model for this situation. But as the name of tutorial shows we are looking at Zero-Inflated models. But lets try the negative binomial anyway..\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nglm2&lt;- glm.nb(count ~ persons + child + camper, \n              data = fishing)\n\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm2),\n                  Residuals=resid(glm2))\n\np3&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np4&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np3+p4\n\n\n\n\n\n\n\n\nIt does go some of the way, but there are still issues around the zero values in the Homogeneity of variance plot! Okay lets get on task then, we will use the zeroinfl() function from the pscl package. We will need to extract the fitted and residuals to plot ourselves as the check_model() function doesn’t support this glm type yet.\n\n#install.packages(\"pscl\")\nlibrary(pscl)\n\nClasses and Methods for R originally developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University (2002-2015),\nby and under the direction of Simon Jackman.\nhurdle and zeroinfl functions by Achim Zeileis.\n\nglm3 &lt;- zeroinfl(count ~ persons + child + camper|  persons+child+camper,\n                 data = fishing, dist = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm3),\n                  Residuals=resid(glm3))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nThis is definitely still not great.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Zero Inflated GLMs"
    ]
  },
  {
    "objectID": "StatisticsTutorials/ZeroInflatedGLMs.html#zero-inflated-simulated-data",
    "href": "StatisticsTutorials/ZeroInflatedGLMs.html#zero-inflated-simulated-data",
    "title": "Zero Inflated GLMs",
    "section": "",
    "text": "For a full example we will actually make up some data. Lets pretend we have count of fish inside and outside of a Marine Protected Area (MPA). We will also pretend our MPA is unbelieveablly good at providing protection to the species of fish we are looking at. But we have some unknown situation that means that sometimes we have high levels of zero records.\nWe will first make our factors, Time and MPA, then we will create poisson data where lambda is influenced by both the MPA and Time factors. Lets visualise each factor so we can see how we have created the data and what our data look like. Don’t forget we still need to add in the zero-inflation. We will create lambda from a linear equation using b0, b1, b2 and b3 as the intercept and the effects of Time, MPA, and interaction of Time and MPA.\n\nlibrary(patchwork)\n\nn &lt;- 5000\n\nMPA &lt;- sample(c(0,1), size = n, replace = TRUE)\n\nTime &lt;- c(1:100)\n\nb0&lt;-log(2) # Intercept\n\nb1&lt;-log(1.1) # Effect of Time\n\nb2&lt;-log(1.2) # Effect of MPA\n\nb3&lt;-log(1.3) # Effect of interaction between Time and MPA\n\nlambda&lt;-exp(b0 + b1 * log(Time) + b2 * (MPA==1) + b3 * (MPA==1) * log(Time))\n\ny_sim &lt;- rpois(n = n, lambda = lambda) \n\ndf&lt;-data.frame(MPA=as.factor(MPA),\n               Time=Time,\n               Count=y_sim)\n\np6&lt;-ggplot(df)+\n    geom_bar(aes(x=MPA),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"MPA\")\n\np7&lt;-ggplot(df)+\n    geom_bar(aes(x=Time),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"Time\")\n\np8&lt;-ggplot(df)+\n    geom_bar(aes(x=Count),\n             fill=\"darkcyan\")+\n    theme_classic()+\n    labs(y=\"Count\",x=\"Fish Count\")\n\np6+p7+p8\n\n\n\n\n\n\n\nggplot(df)+\n    geom_point(aes(x=Time,y=Count,colour=MPA))+\n    theme_classic()+\n    scale_colour_manual(values=c(\"darkgoldenrod\",\"darkcyan\"))+\n    labs(y=\"Fish Count\",x=\"Time\")\n\n\n\n\n\n\n\n\nNow lets create a high number of zeros, which are going to more likely to occur (with a probability of 0.1) if it is inside the MPA but not difference with time.\n\ndf1&lt;-df %&gt;% \n  mutate(Count_Zeros=Count*rbinom(MPA, size = 1, prob=0.7))\n  \n\nggplot(df1)+\n    geom_point(aes(x=Time,y=Count_Zeros,colour=MPA))+\n    theme_classic()+\n    scale_colour_manual(values=c(\"darkgoldenrod\",\"darkcyan\"))+\n    labs(y=\"Fish Count\",x=\"Time\")\n\n\n\n\n\n\n\n\nNow lets try model this.\n\nglm4 &lt;- glm(Count_Zeros ~ MPA*Time,\n                 data = df1, family = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm4),\n                  Residuals=resid(glm4))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nWell this model looks pretty bad, not surprisingly.\n\nglm5 &lt;- zeroinfl(Count_Zeros ~ MPA*Time|MPA,\n                 data = df1, dist = \"poisson\")\n\nModelOutputs&lt;-data.frame(Fitted=fitted(glm5),\n                  Residuals=resid(glm5))\n\np1&lt;-ggplot(ModelOutputs)+\n    geom_point(aes(x=Fitted,y=Residuals))+\n    theme_classic()+\n    labs(y=\"Residuals\",x=\"Fitted Values\")\n\np2&lt;-ggplot(ModelOutputs) +\n    stat_qq(aes(sample=Residuals))+\n    stat_qq_line(aes(sample=Residuals))+\n    theme_classic()+\n    labs(y=\"Sample Quartiles\",x=\"Theoretical Quartiles\")\n\n\np1+p2\n\n\n\n\n\n\n\n\nThis looks better, again not perfect but pretty good. A more comprehensive example will come when we do GLMMs.",
    "crumbs": [
      "Home",
      "Statistics Tutorials",
      "Zero Inflated GLMs"
    ]
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Categories\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBRUVs\n\n\nTUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFunctional Diversity\n\n\nFirst Author\n\n\n\nBede F. R. Davies, Luke Holmes, Anthony Bicknell, Martin J. Attrill & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Sensing\n\n\nSpectral Library\n\n\nSatellite\n\n\nMachine Learning\n\n\nFirst Author\n\n\nEurope\n\n\nNeural Networks\n\n\n\nBede F.R. Davies; Simon Oiry; Philippe Rosa; Maria Laura Zoffoli; Ana I. Sousa; Oliver R. Thomas; Dan A. Smale; Melanie C. Austen; Lauren Biermann; Martin J. Attrill; Alejandro Roman; Gabriel Navarro; Anne-Laure Barillé; Nicolas Harin; Daniel Clewley; Victor Martinez-Vicente; Pierre Gernez & Laurent Barillé\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcoustic\n\n\nBRUVs\n\n\nEcology\n\n\nPAM\n\n\nMarine Diversity\n\n\nFirst Author\n\n\n\nBede F.R. Davies, Martin J. Attrill, Luke Holmes, Adam Rees, Matthew J. Witt & Emma V. Sheehan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHabitat Mapping\n\n\nCarbon Stock\n\n\nJersey\n\n\nFirst Author\n\n\nEurope\n\n\n\nBede Ffinian Rowe Davies, Samantha Blampied, Francis Binney, Graham Epstein, Paul Chambers & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBRUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFirst Author\n\n\n\nBede F.R. Davies, Luke Holmes, Adam Rees, Martin J. Attrill, Amy Y. Cartwright & Emma V. Sheehan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarine Management\n\n\nBRUVs\n\n\nTUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFunctional Diversity\n\n\nFirst Author\n\n\n\nBede F. R. Davies, Luke Holmes, Martin J. Attrill & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Sensing\n\n\nSpectral Library\n\n\nSatellite\n\n\nMachine Learning\n\n\nFirst Author\n\n\nEurope\n\n\nNeural Networks\n\n\n\nBede F.R. Davies; Simon Oiry; Philippe Rosa; Maria Laura Zoffoli; Ana I. Sousa; Oliver R. Thomas; Dan A. Smale; Melanie C. Austen; Lauren Biermann; Martin J. Attrill; Alejandro Roman; Gabriel Navarro; Anne-Laure Barillé; Nicolas Harin; Daniel Clewley; Victor Martinez-Vicente; Pierre Gernez & Laurent Barillé\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Sensing\n\n\nSpectral Library\n\n\nSatellite\n\n\nMachine Learning\n\n\nFirst Author\n\n\n\nBede F. R. Davies, Pierre Gernez, Andréa Geraud, Simon Oiry, Philippe Rosa, Maria Laura Zoffoli & Laurent Barillé\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "About Me",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#first-author-publications",
    "href": "Publications.html#first-author-publications",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Categories\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBRUVs\n\n\nTUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFunctional Diversity\n\n\nFirst Author\n\n\n\nBede F. R. Davies, Luke Holmes, Anthony Bicknell, Martin J. Attrill & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Sensing\n\n\nSpectral Library\n\n\nSatellite\n\n\nMachine Learning\n\n\nFirst Author\n\n\nEurope\n\n\nNeural Networks\n\n\n\nBede F.R. Davies; Simon Oiry; Philippe Rosa; Maria Laura Zoffoli; Ana I. Sousa; Oliver R. Thomas; Dan A. Smale; Melanie C. Austen; Lauren Biermann; Martin J. Attrill; Alejandro Roman; Gabriel Navarro; Anne-Laure Barillé; Nicolas Harin; Daniel Clewley; Victor Martinez-Vicente; Pierre Gernez & Laurent Barillé\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcoustic\n\n\nBRUVs\n\n\nEcology\n\n\nPAM\n\n\nMarine Diversity\n\n\nFirst Author\n\n\n\nBede F.R. Davies, Martin J. Attrill, Luke Holmes, Adam Rees, Matthew J. Witt & Emma V. Sheehan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHabitat Mapping\n\n\nCarbon Stock\n\n\nJersey\n\n\nFirst Author\n\n\nEurope\n\n\n\nBede Ffinian Rowe Davies, Samantha Blampied, Francis Binney, Graham Epstein, Paul Chambers & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBRUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFirst Author\n\n\n\nBede F.R. Davies, Luke Holmes, Adam Rees, Martin J. Attrill, Amy Y. Cartwright & Emma V. Sheehan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarine Management\n\n\nBRUVs\n\n\nTUVs\n\n\nEcology\n\n\nMarine Diversity\n\n\nFunctional Diversity\n\n\nFirst Author\n\n\n\nBede F. R. Davies, Luke Holmes, Martin J. Attrill & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Sensing\n\n\nSpectral Library\n\n\nSatellite\n\n\nMachine Learning\n\n\nFirst Author\n\n\nEurope\n\n\nNeural Networks\n\n\n\nBede F.R. Davies; Simon Oiry; Philippe Rosa; Maria Laura Zoffoli; Ana I. Sousa; Oliver R. Thomas; Dan A. Smale; Melanie C. Austen; Lauren Biermann; Martin J. Attrill; Alejandro Roman; Gabriel Navarro; Anne-Laure Barillé; Nicolas Harin; Daniel Clewley; Victor Martinez-Vicente; Pierre Gernez & Laurent Barillé\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote Sensing\n\n\nSpectral Library\n\n\nSatellite\n\n\nMachine Learning\n\n\nFirst Author\n\n\n\nBede F. R. Davies, Pierre Gernez, Andréa Geraud, Simon Oiry, Philippe Rosa, Maria Laura Zoffoli & Laurent Barillé\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "About Me",
      "Publications"
    ]
  },
  {
    "objectID": "Publications.html#co-author-publications",
    "href": "Publications.html#co-author-publications",
    "title": "Publications",
    "section": "Co-Author Publications:",
    "text": "Co-Author Publications:\n\n\n\n\n\n\n\n\n\n\nDiscriminating Seagrasses from Green Macroalgae in European Intertidal Areas Using High-Resolution Multispectral Drone Imagery\n\n\n\n\n\n\nHabitat Mapping\n\n\nSeagrass\n\n\nDrone\n\n\nCo-Author\n\n\nFrance\n\n\nPortugal\n\n\n\nSimon Oiry, Bede Ffinian Rowe Davies, Ana I. Sousa, Philippe Rosa, Maria Laura Zoﬀoli,Guillaume Brunier, Pierre Gernez and Laurent Barillé\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLessons from Lyme Bay (UK) to inform policy, management, and monitoring of Marine Protected Areas\n\n\n\n\n\n\nMPA\n\n\nEcology\n\n\nMarine Diversity\n\n\nClimatic Events\n\n\nBRUVs\n\n\nSocio-Economic\n\n\nTUVs\n\n\nCo-Author\n\n\n\nChloe Renn, Sian Rees, Adam Rees, Bede F R Davies, Amy Y Cartwright, Sam Fanshawe, Martin J Attrill, Luke A Holmes & Emma V Sheehan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping intertidal microphytobenthic biomass with very high-resolution remote sensing imagery in an estuarine system.\n\n\n\n\n\n\nHabitat Mapping\n\n\nMicrophytobenthos\n\n\nDrone\n\n\nCo-Author\n\n\nSpain\n\n\n\nAlejandro Román, Simon Oiry, Bede F.R. Davies, Philippe Rosa, Pierre Gernez, Antonio Tovar-Sánchez, Gabriel Navarro, Vona Méléder & Laurent Barillé\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping intertidal oyster farms using unmanned aerial vehicles (UAV) high-resolution multispectral data\n\n\n\n\n\n\nDrones\n\n\nEcology\n\n\nOyster\n\n\nMarine Diversity\n\n\nRemote Sensing\n\n\nCo-Author\n\n\n\nAlejandro Román, Hermansyah Prasyad, Simon Oiry, Bede F.R. Davies, Guillaume Brunier & Laurent Barillé́\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReaching New Heights in Plastic Pollution—Preliminary Findings of Microplastics on Mount Everest\n\n\n\n\n\n\nPlastics\n\n\nPollution\n\n\nTerrestrial Pollution\n\n\nCo-Author\n\n\n\nImogen E. Napper, Bede F.R. Davies, Heather Clifford, Sandra Elvin, Heather J.Koldewey, Paul A.Mayewski, Kimberley R.Miner, Mariusz Potocki, Aurora C. Elmore Ananta P. Gajurel & Richard C. Thompson.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemote sensing in seagrass ecology: coupled dynamicsbetween migratory herbivorous birds and intertidalmeadows observed by satellite during four decades\n\n\n\n\n\n\nSatellite\n\n\nEcology\n\n\nSeagrass\n\n\nMarine Diversity\n\n\nClimatic Events\n\n\nRemote Sensing\n\n\nCo-Author\n\n\n\nMaria Laura Zoffoli, Pierre Gernez, Simon Oiry, Laurent Godet, Śebastien Dalloyau, Bede Ffinian Rowe Davies & Laurent Barilĺ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRewilding of Protected Areas Enhances Resilience of Marine Ecosystems to Extreme Climatic Events\n\n\n\n\n\n\nStorms\n\n\nEcology\n\n\nMarine Diversity\n\n\nClimatic Events\n\n\nTUVs\n\n\nCo-Author\n\n\n\nEmma V. Sheehan, Luke A. Holmes, Bede F. R. Davies, Amy Cartwright, Adam Rees & Martin J. Attrill.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Abundance and Characteristics of Microplastics in Surface Water in the Transboundary Ganges River\n\n\n\n\n\n\nPlastics\n\n\nPollution\n\n\nFreshwater Pollution\n\n\nCo-Author\n\n\n\nImogen E. Napper, Anju Baroth, Aaron C. Barrett, Sunanda Bhola. Gawsia W. Chowdhuryd, Bede F.R. Davies, Emily M. Duncan, Sumit Kumar, Sarah E. Nelms, Md Nazmul Hasan Niloy, Bushra Nishat, Taylor Maddalene, Richard C. Thompson & Heather Koldewey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Distribution and Characterisation of Microplastics in Air, Surface Water and Sediment within a Major River System.\n\n\n\n\n\n\nPlastics\n\n\nPollution\n\n\nFreshwater Pollution\n\n\nCo-Author\n\n\n\nImogen E. Napper, Anju Baroth, Aaron C. Barrett, Sunanda Bhola, Gawsia W. Chowdhury, Bede F.R. Davies, Emily M. Duncan, Sumit Kumar, Sarah E. Nelms, Md. Nazmul Hasan Niloy, Bushra Nishat, Taylor Maddalene, Natalie Smith, Richard C. Thompson & Heather Koldewey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe restoration potential of offshore mussel farming on degraded seabed habitat\n\n\n\n\n\n\nEcology\n\n\nMarine Diversity\n\n\nAquaculture\n\n\nTUVs\n\n\nCo-Author\n\n\n\nDanielle Bridger, Martin J. Attrill, Bede F. R. Davies, Luke A. Holmes, Amy Cartwright, Siân E. Rees, Llucia Mascorda Cabre & Emma V. Sheehan\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "About Me",
      "Publications"
    ]
  },
  {
    "objectID": "AdvancedRTutorials/RemoteSensingMapping.html",
    "href": "AdvancedRTutorials/RemoteSensingMapping.html",
    "title": "Making Maps from Satellite Imagery",
    "section": "",
    "text": "So we have learnt how to deal with elements of spatial analysis in R using the sf, terra and tidyterra packages. Now we will look into some more advanced spatial analyses. Generally remote sensing is where satellites circling the globe take measurements of reflected radiation while looking at earth, this could be imagery in visible ranges, non visible ranges, combinations of these and many other options. Remote sensing can rapidly cover huge areas (the globe) with varying levels of spatial resolution. Satellite sensors will also have varying levels of what is called spectral resolution.\n\n\nThe Electro-Magnetic spectrum ranges from wavelengths of 1000s (radio) to 10 \\(^{-12}\\) metres (gamma radiation). Visible light ranges from around 380 to 700 nm (3.8x10\\(^{-7}\\) to 7x10\\(^{-7}\\) metres). Human eyes, with our cones and rods, can register four bands of visible light at different wavelengths: 419, 496, 531 and 559 nm. This means we do not see the whole visible spectrum. Depending on the satellite sensor being used they can measure reflectance of a much wider range of radiation and therefore provide important information about the surface of the planet or its atmosphere. Each sensor (like the cones or rods in our eyes) is often called a band and, depending on how many sensors there are and how much of the EM spectrum they measure, will have a certain spectral resolution. The range of satellites and their applications is massive such as, crop monitoring, sea ice mapping, surface temperature measurement, atmospheric properties and geological information. We will focus in this tutorial on multispectral (from around 10 bands) satellite imagery that is easily accessible, free-to-use and has moderate to high spatial resolution (10 - 30 m).\n\n\n\nWe will access some Landsat data from this UCdavis repo. This will create a new folder in your working directory, download the zip file, unzip it and then delete the raw zip file, and also some tifs we won’t be using to free up space.\n\nif (!file.exists(\"data/rs/samples.rds\")) {\n    dir.create(\"data\", showWarnings = FALSE)\n    download.file(\"https://biogeo.ucdavis.edu/data/rspatial/rs.zip\", dest = \"data/rs.zip\")\n    unzip(\"data/rs.zip\", exdir=\"data\")\n    file.remove(\"data/rs.zip\")\n    file.remove(\"data/rs/LC08_044034_20170614_B8.tif\")\n    file.remove(\"data/rs/LC08_044034_20170614_B9.tif\")\n    file.remove(\"data/rs/LC08_044034_20170614_B10.tif\")\n    file.remove(\"data/rs/LC08_044034_20170614_B11.tif\")\n    file.remove(\"data/rs/centralvalley-2001LE7.tif\")\n    file.remove(\"data/rs/centralvalley-2011LT5.tif\")\n    file.remove(\"data/rs/cropped-landsat.tif\")\n    file.remove(\"data/rs/nlcd-L1.tif\")\n    file.remove(\"data/rs/nlcd-L2.tif\")\n}\n\nWithin this zip file there are all 11 bands of Landsat. Lets bring them into r using terra, inspect and plot them with tidyterra as we have previously plotted other tif files. We can bring in each file separately, or we can use common naming patterns. We see that tidyterra resamples the rasters to a lower resolution to plot, we can set this value to be higher if we want higher resolution plots with maxcell= and some value greater than 5000000. We will leave it as it is for now to stay quick.\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(tidyterra)\n\n# Blue\nb2 &lt;- rast('data/rs/LC08_044034_20170614_B2.tif')\n# Green\nb3 &lt;- rast('data/rs/LC08_044034_20170614_B3.tif')\n# Red\nb4 &lt;- rast('data/rs/LC08_044034_20170614_B4.tif')\n# Near Infrared (NIR)\nb5 &lt;- rast('data/rs/LC08_044034_20170614_B5.tif')\n\ncrs(b2,describe=T)\n\n                   name authority  code\n1 WGS 84 / UTM zone 10N      EPSG 32610\n                                                                                                                                                                                                         area\n1 Between 126°W and 120°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - British Columbia (BC); Northwest Territories (NWT); Nunavut; Yukon. United States (USA) - Alaska (AK)\n             extent\n1 -126, -120, 0, 84\n\nres(b2)\n\n[1] 30 30\n\next(b2)\n\nSpatExtent : 594090, 639000, 4190190, 4227540 (xmin, xmax, ymin, ymax)\n\nfilenames &lt;- list.files('data/rs/', pattern=\"*.tif\",full.names  = T)\n\nCombined_Landsat&lt;-rast(filenames)\n\nggplot()+\n  geom_spatraster(data=Combined_Landsat)+\n  labs(fill=\"\")+\n  facet_wrap(~lyr)+\n  scale_fill_hypso_c(\"colombia\",na.value = NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nWe can combine the bands to make a composite image, these will take different bands for true red green blue imagery, or we can also do a false colour image using the Near Infrared band. We set the maximum value, the default is 255 but our data only range from 0 to around 0.7, but even then the majority is below 0.45 (especially across the b2, b3 and b4) so we shall set 0.45 as the max colour.\n\nlibrary(patchwork)\n\np1&lt;-ggplot()+\n  geom_spatraster_rgb(data=Combined_Landsat,\n                      r=4,g=3,b=2,\n                      max_col_value = 0.45,\n                      interpolate=T)+\n  theme_classic()\n\n\np2&lt;-ggplot()+\n  geom_spatraster_rgb(data=Combined_Landsat,\n                      r=5,g=4,b=3,\n                      max_col_value = 0.45,\n                      interpolate=T)+\n  theme_classic()\n\np1+p2\n\n\n\n\n\n\n\n\n\n\n\nIn the zip folder we downloaded earlier someone has also labelled the landcover of specific places in this image. Using this information we can summarise spectral signatures that are associated with specific land uses. This becomes very important when we want to classify new imagery based on just spectral information. (There will be a tutorial on this using machine learning).\n\nsamp &lt;- readRDS('data/rs/lcsamples.rds')\n\nggplot()+\n  geom_spatraster_rgb(data=Combined_Landsat,\n                      r=4,g=3,b=2,\n                      max_col_value = 0.45,\n                      interpolate=T)+\n  geom_spatvector(data=samp,aes(fill=class))+\n  labs(fill=\"Class\")+\n  theme_classic()\n\n&lt;SpatRaster&gt; resampled to 500520 cells.\n\n\n\n\n\n\n\n\n\nSo we have multiple classes in a shape file and we want to assign all the spectra from the multispectral image to each of those classes. To do this we want to create a look up of which polygon ID is which class, then extract all the spectra for each polygon. We can then assign a class to each spectra.\n\nsamp_df&lt;-as.data.frame(samp) %&gt;% \n  distinct() %&gt;% \n  rename(ID=id)%&gt;% \n  mutate(ID=as.numeric(ID))\n\ndf &lt;- extract(Combined_Landsat, samp) %&gt;% \n  left_join(samp_df,by=\"ID\") %&gt;% \n  drop_na()\n\nhead(df)\n\n  ID LC08_044034_20170614_B1 LC08_044034_20170614_B2 LC08_044034_20170614_B3\n1  1               0.1179525              0.09949739              0.09782753\n2  1               0.1212055              0.10372625              0.10303228\n3  1               0.1157839              0.09604924              0.09422758\n4  1               0.1154803              0.09611430              0.09210231\n5  1               0.1155670              0.09589744              0.09199388\n6  1               0.1153935              0.09542034              0.09236255\n  LC08_044034_20170614_B4 LC08_044034_20170614_B5 LC08_044034_20170614_B6\n1              0.06904963               0.4698801               0.1587231\n2              0.08399158               0.4227555               0.1750746\n3              0.06755326               0.4363746               0.1689807\n4              0.06295574               0.4468708               0.1475329\n5              0.06284730               0.4520105               0.1481184\n6              0.06367139               0.4496250               0.1490943\n  LC08_044034_20170614_B7    class\n1              0.06698941 cropland\n2              0.08737466 cropland\n3              0.08184462 cropland\n4              0.06037505 cropland\n5              0.06067866 cropland\n6              0.06165455 cropland\n\n\nEach row in this data frame is a single pixel. Therefore, we can do some grouping to see what the average spectra of each class looks like and how much variation there is between classes. First, we will convert our data into long format, then add the wavelength information for each band into the data. We can do this easily using the tidyverse functions\n\nWavelengths&lt;-data.frame(Band=c(\"B1\",\n                               \"B2\",\n                               \"B3\",\n                               \"B4\",\n                               \"B5\",\n                               \"B6\",\n                               \"B7\"),\n                        Wavelengths=c(443,\n                                      483,\n                                      563,\n                                      605,\n                                      865,\n                                      1610,\n                                      2200))\n\n\ndf_long&lt;-df %&gt;% \n  rownames_to_column(var=\"PixelID\") %&gt;% \n  pivot_longer(-c(PixelID,ID,class),names_to = \"Band\",values_to = \"Reflectance\") %&gt;% \n  mutate(Band=str_remove_all(Band,\"LC08_044034_20170614_\")) %&gt;% \n  left_join(Wavelengths,by=\"Band\") \n\ndf_long %&gt;% \n  group_by(class,Wavelengths) %&gt;% \n  reframe(Reflectance_mean=mean(Reflectance),\n          Reflectance_sd=sd(Reflectance)) %&gt;% \n  ggplot(aes(x=Wavelengths,y=Reflectance_mean,colour=class,fill=class))+\n  geom_ribbon(aes(ymax=Reflectance_mean+Reflectance_sd,ymin=Reflectance_mean-Reflectance_sd),\n              alpha=0.3)+\n  geom_line()+\n  scale_fill_manual(name=\"Class\",\n                       values = c(\"grey\",\n                                  \"#9fcb41\",\n                                  \"#e86a28\", \n                                  \"#fde825\",\n                                  \"#008B8B\"))+\n  scale_colour_manual(name=\"Class\",\n                       values = c(\"grey\",\n                                  \"#9fcb41\",\n                                  \"#e86a28\", \n                                  \"#fde825\",\n                                  \"#008B8B\"))+\n  labs(y=\"Reflectance\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Maps from Satellite Imagery"
    ]
  },
  {
    "objectID": "AdvancedRTutorials/RemoteSensingMapping.html#spectral-resolution",
    "href": "AdvancedRTutorials/RemoteSensingMapping.html#spectral-resolution",
    "title": "Making Maps from Satellite Imagery",
    "section": "",
    "text": "The Electro-Magnetic spectrum ranges from wavelengths of 1000s (radio) to 10 \\(^{-12}\\) metres (gamma radiation). Visible light ranges from around 380 to 700 nm (3.8x10\\(^{-7}\\) to 7x10\\(^{-7}\\) metres). Human eyes, with our cones and rods, can register four bands of visible light at different wavelengths: 419, 496, 531 and 559 nm. This means we do not see the whole visible spectrum. Depending on the satellite sensor being used they can measure reflectance of a much wider range of radiation and therefore provide important information about the surface of the planet or its atmosphere. Each sensor (like the cones or rods in our eyes) is often called a band and, depending on how many sensors there are and how much of the EM spectrum they measure, will have a certain spectral resolution. The range of satellites and their applications is massive such as, crop monitoring, sea ice mapping, surface temperature measurement, atmospheric properties and geological information. We will focus in this tutorial on multispectral (from around 10 bands) satellite imagery that is easily accessible, free-to-use and has moderate to high spatial resolution (10 - 30 m).",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Maps from Satellite Imagery"
    ]
  },
  {
    "objectID": "AdvancedRTutorials/RemoteSensingMapping.html#bring-in-some-example-data",
    "href": "AdvancedRTutorials/RemoteSensingMapping.html#bring-in-some-example-data",
    "title": "Making Maps from Satellite Imagery",
    "section": "",
    "text": "We will access some Landsat data from this UCdavis repo. This will create a new folder in your working directory, download the zip file, unzip it and then delete the raw zip file, and also some tifs we won’t be using to free up space.\n\nif (!file.exists(\"data/rs/samples.rds\")) {\n    dir.create(\"data\", showWarnings = FALSE)\n    download.file(\"https://biogeo.ucdavis.edu/data/rspatial/rs.zip\", dest = \"data/rs.zip\")\n    unzip(\"data/rs.zip\", exdir=\"data\")\n    file.remove(\"data/rs.zip\")\n    file.remove(\"data/rs/LC08_044034_20170614_B8.tif\")\n    file.remove(\"data/rs/LC08_044034_20170614_B9.tif\")\n    file.remove(\"data/rs/LC08_044034_20170614_B10.tif\")\n    file.remove(\"data/rs/LC08_044034_20170614_B11.tif\")\n    file.remove(\"data/rs/centralvalley-2001LE7.tif\")\n    file.remove(\"data/rs/centralvalley-2011LT5.tif\")\n    file.remove(\"data/rs/cropped-landsat.tif\")\n    file.remove(\"data/rs/nlcd-L1.tif\")\n    file.remove(\"data/rs/nlcd-L2.tif\")\n}\n\nWithin this zip file there are all 11 bands of Landsat. Lets bring them into r using terra, inspect and plot them with tidyterra as we have previously plotted other tif files. We can bring in each file separately, or we can use common naming patterns. We see that tidyterra resamples the rasters to a lower resolution to plot, we can set this value to be higher if we want higher resolution plots with maxcell= and some value greater than 5000000. We will leave it as it is for now to stay quick.\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(tidyterra)\n\n# Blue\nb2 &lt;- rast('data/rs/LC08_044034_20170614_B2.tif')\n# Green\nb3 &lt;- rast('data/rs/LC08_044034_20170614_B3.tif')\n# Red\nb4 &lt;- rast('data/rs/LC08_044034_20170614_B4.tif')\n# Near Infrared (NIR)\nb5 &lt;- rast('data/rs/LC08_044034_20170614_B5.tif')\n\ncrs(b2,describe=T)\n\n                   name authority  code\n1 WGS 84 / UTM zone 10N      EPSG 32610\n                                                                                                                                                                                                         area\n1 Between 126°W and 120°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - British Columbia (BC); Northwest Territories (NWT); Nunavut; Yukon. United States (USA) - Alaska (AK)\n             extent\n1 -126, -120, 0, 84\n\nres(b2)\n\n[1] 30 30\n\next(b2)\n\nSpatExtent : 594090, 639000, 4190190, 4227540 (xmin, xmax, ymin, ymax)\n\nfilenames &lt;- list.files('data/rs/', pattern=\"*.tif\",full.names  = T)\n\nCombined_Landsat&lt;-rast(filenames)\n\nggplot()+\n  geom_spatraster(data=Combined_Landsat)+\n  labs(fill=\"\")+\n  facet_wrap(~lyr)+\n  scale_fill_hypso_c(\"colombia\",na.value = NA)+\n  theme_classic()",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Maps from Satellite Imagery"
    ]
  },
  {
    "objectID": "AdvancedRTutorials/RemoteSensingMapping.html#rgb-plotting",
    "href": "AdvancedRTutorials/RemoteSensingMapping.html#rgb-plotting",
    "title": "Making Maps from Satellite Imagery",
    "section": "",
    "text": "We can combine the bands to make a composite image, these will take different bands for true red green blue imagery, or we can also do a false colour image using the Near Infrared band. We set the maximum value, the default is 255 but our data only range from 0 to around 0.7, but even then the majority is below 0.45 (especially across the b2, b3 and b4) so we shall set 0.45 as the max colour.\n\nlibrary(patchwork)\n\np1&lt;-ggplot()+\n  geom_spatraster_rgb(data=Combined_Landsat,\n                      r=4,g=3,b=2,\n                      max_col_value = 0.45,\n                      interpolate=T)+\n  theme_classic()\n\n\np2&lt;-ggplot()+\n  geom_spatraster_rgb(data=Combined_Landsat,\n                      r=5,g=4,b=3,\n                      max_col_value = 0.45,\n                      interpolate=T)+\n  theme_classic()\n\np1+p2",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Maps from Satellite Imagery"
    ]
  },
  {
    "objectID": "AdvancedRTutorials/RemoteSensingMapping.html#associating-land-use-to-spectral-information",
    "href": "AdvancedRTutorials/RemoteSensingMapping.html#associating-land-use-to-spectral-information",
    "title": "Making Maps from Satellite Imagery",
    "section": "",
    "text": "In the zip folder we downloaded earlier someone has also labelled the landcover of specific places in this image. Using this information we can summarise spectral signatures that are associated with specific land uses. This becomes very important when we want to classify new imagery based on just spectral information. (There will be a tutorial on this using machine learning).\n\nsamp &lt;- readRDS('data/rs/lcsamples.rds')\n\nggplot()+\n  geom_spatraster_rgb(data=Combined_Landsat,\n                      r=4,g=3,b=2,\n                      max_col_value = 0.45,\n                      interpolate=T)+\n  geom_spatvector(data=samp,aes(fill=class))+\n  labs(fill=\"Class\")+\n  theme_classic()\n\n&lt;SpatRaster&gt; resampled to 500520 cells.\n\n\n\n\n\n\n\n\n\nSo we have multiple classes in a shape file and we want to assign all the spectra from the multispectral image to each of those classes. To do this we want to create a look up of which polygon ID is which class, then extract all the spectra for each polygon. We can then assign a class to each spectra.\n\nsamp_df&lt;-as.data.frame(samp) %&gt;% \n  distinct() %&gt;% \n  rename(ID=id)%&gt;% \n  mutate(ID=as.numeric(ID))\n\ndf &lt;- extract(Combined_Landsat, samp) %&gt;% \n  left_join(samp_df,by=\"ID\") %&gt;% \n  drop_na()\n\nhead(df)\n\n  ID LC08_044034_20170614_B1 LC08_044034_20170614_B2 LC08_044034_20170614_B3\n1  1               0.1179525              0.09949739              0.09782753\n2  1               0.1212055              0.10372625              0.10303228\n3  1               0.1157839              0.09604924              0.09422758\n4  1               0.1154803              0.09611430              0.09210231\n5  1               0.1155670              0.09589744              0.09199388\n6  1               0.1153935              0.09542034              0.09236255\n  LC08_044034_20170614_B4 LC08_044034_20170614_B5 LC08_044034_20170614_B6\n1              0.06904963               0.4698801               0.1587231\n2              0.08399158               0.4227555               0.1750746\n3              0.06755326               0.4363746               0.1689807\n4              0.06295574               0.4468708               0.1475329\n5              0.06284730               0.4520105               0.1481184\n6              0.06367139               0.4496250               0.1490943\n  LC08_044034_20170614_B7    class\n1              0.06698941 cropland\n2              0.08737466 cropland\n3              0.08184462 cropland\n4              0.06037505 cropland\n5              0.06067866 cropland\n6              0.06165455 cropland\n\n\nEach row in this data frame is a single pixel. Therefore, we can do some grouping to see what the average spectra of each class looks like and how much variation there is between classes. First, we will convert our data into long format, then add the wavelength information for each band into the data. We can do this easily using the tidyverse functions\n\nWavelengths&lt;-data.frame(Band=c(\"B1\",\n                               \"B2\",\n                               \"B3\",\n                               \"B4\",\n                               \"B5\",\n                               \"B6\",\n                               \"B7\"),\n                        Wavelengths=c(443,\n                                      483,\n                                      563,\n                                      605,\n                                      865,\n                                      1610,\n                                      2200))\n\n\ndf_long&lt;-df %&gt;% \n  rownames_to_column(var=\"PixelID\") %&gt;% \n  pivot_longer(-c(PixelID,ID,class),names_to = \"Band\",values_to = \"Reflectance\") %&gt;% \n  mutate(Band=str_remove_all(Band,\"LC08_044034_20170614_\")) %&gt;% \n  left_join(Wavelengths,by=\"Band\") \n\ndf_long %&gt;% \n  group_by(class,Wavelengths) %&gt;% \n  reframe(Reflectance_mean=mean(Reflectance),\n          Reflectance_sd=sd(Reflectance)) %&gt;% \n  ggplot(aes(x=Wavelengths,y=Reflectance_mean,colour=class,fill=class))+\n  geom_ribbon(aes(ymax=Reflectance_mean+Reflectance_sd,ymin=Reflectance_mean-Reflectance_sd),\n              alpha=0.3)+\n  geom_line()+\n  scale_fill_manual(name=\"Class\",\n                       values = c(\"grey\",\n                                  \"#9fcb41\",\n                                  \"#e86a28\", \n                                  \"#fde825\",\n                                  \"#008B8B\"))+\n  scale_colour_manual(name=\"Class\",\n                       values = c(\"grey\",\n                                  \"#9fcb41\",\n                                  \"#e86a28\", \n                                  \"#fde825\",\n                                  \"#008B8B\"))+\n  labs(y=\"Reflectance\")+\n  theme_classic()",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Maps from Satellite Imagery"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/MakingInteractiveMaps.html",
    "href": "IntermediateRTutorials/MakingInteractiveMaps.html",
    "title": "Making Interactive Maps in R",
    "section": "",
    "text": "One of the main issues with doing mapping in R over other software is our ability to zoom in and out of maps to assess different scales, from whole ocean basin to small harbour? Likewise, when planning fieldwork or even just assessing whether the coordinates you recorded are in the right location with no missing minus signs or wrong decimal location, having a tool you can quickly upload points onto a map that then allows you to zoom in and out on is really handy.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Interactive Maps in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/MakingInteractiveMaps.html#layering",
    "href": "IntermediateRTutorials/MakingInteractiveMaps.html#layering",
    "title": "Making Interactive Maps in R",
    "section": "Layering",
    "text": "Layering\n\nBase Map\nLike with ggplot2 the first blank arguement creates a blank layer, we can then layer preloaded and local data into the map. Unlike ggplot2 we use the pipe ( %&gt;% ) rather than the plus (+).\n\nleaflet() %&gt;% \n  addTiles()\n\n\n\n\n\nThe base map is zoomed to 2 ish globes and with the standard Open Street Map. We can change these element by choosing some other background tile and setting the zoom. You can find a whole list of all the available basemaps here\n\nleaflet() %&gt;%\n  addProviderTiles(\n    \"OpenTopoMap\"\n  )%&gt;%\n    setView(lng = 6.8652, lat = 45.8326, zoom = 6) ## Location of Mont Blanc\n\n\n\n\n\nWe can set one we like, or we can allow our user to choose. Here we will save the map as m but we also want to plot it too so we can put brackets around the whole thing, this means we can just run the line to save and plot.\n\n(m&lt;-leaflet()%&gt;%\n  addProviderTiles(\n    \"OpenTopoMap\"\n  ) )\n\n\n\n\n\n\n\nLocal Data\n\nPoint Data\nSo lets assume we are interested in the tallest mountains in each continent. Lets make a dataframe then add it to the leaflet map.\n\nMountains&lt;-data.frame(\n  Mountain=c(\"Everest\", \"Aconcagua\", \"Denali\", \"Kilimanjaro\", \"Vinson\", \"Mont Blanc\", \"Mount Wilhelm\"),\n  Longitude=c(86.9250,-70.0109,-151.0070, 37.3556,-85.2135, 6.8652,145.0297),\n  Latitude=c(27.9881,-32.6532,63.0692,-3.0674,-78.6341,45.8326,-5.7800),\n  Height=c(8848,6961,6194,5895,4892,4810,4509),\n  Continent=c(\"Asia\",\"South America\",\"North America\",\"Africa\",\"Antarctica\",\"Europe\",\"Oceania\")\n)\n\nm%&gt;% \n  addMarkers(data=Mountains,lng=~Longitude,lat=~Latitude)\n\n\n\n\n\nThe base markers aren’t very nice so lets add some colour and style plus some info.\n\npal1 &lt;- colorFactor(c(\"navy\", \"red\", \"green\"),\n                   domain = unique(Mountains$Continent))\n\nm %&gt;% \n  addCircleMarkers(data=Mountains,\n                    lng=~Longitude,\n                    lat=~Latitude,\n                    popup=~paste0(Mountain,\" - \",Height),\n                    label=~as.character(Mountain),\n                   color=~pal1(Continent),\n                   fillOpacity = 0.8\n                   )\n\n\n\n\n\n\n\nSpatial Data\nLets download and plot some bathymetry data for the Caribbean and Easter Tropical seas. We need to convert our bathy object to be a SpatRaster from the Terra package (also accepts raster objects but the raster package is being deprecated to switch to terra!!!!) Then we can add a colour palette along the bathymetry data we have. This is then just a couple arguments to make an alright legend.\n\nlibrary(marmap)\n\nRegistered S3 methods overwritten by 'adehabitatMA':\n  method                       from\n  print.SpatialPixelsDataFrame sp  \n  print.SpatialPixels          sp  \n\n\n\nAttaching package: 'marmap'\n\n\nThe following object is masked from 'package:grDevices':\n\n    as.raster\n\nlibrary(terra)\n\nterra 1.7.78\n\nlibrary(tidyterra)\n\n\nAttaching package: 'tidyterra'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nbat_panama &lt;- getNOAA.bathy(lon1=-100,lon2=-54,lat1=32,lat2=-10, res = 1)\n\nQuerying NOAA database ...\n\n\nThis may take seconds to minutes, depending on grid size\n\n\nBuilding bathy matrix ...\n\nBathy_panama &lt;- as.xyz(bat_panama) %&gt;% \n  rename(Longitude=V1,Latitude=V2,Depth=V3) %&gt;% \n  filter(Depth&lt;0) %&gt;% \n  rast()\n\ncrs(Bathy_panama)&lt;-\"+proj=longlat\" \n\npal2 &lt;- colorNumeric(c(\"#0C2C84\", \"#41B6C4\", \"#FFFFCC\"), values(Bathy_panama),\n  na.color = \"transparent\")\n\nleaflet()%&gt;%\n  addTiles()%&gt;%\n  addRasterImage(Bathy_panama,colors=pal2,opacity=0.7) %&gt;%\n  addLegend(pal = pal2, values = values(Bathy_panama),\n    title = \"Depth (m)\")",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Making Interactive Maps in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/StarterGIS.html",
    "href": "IntermediateRTutorials/StarterGIS.html",
    "title": "Spatial Data in R",
    "section": "",
    "text": "Generally, when we talk about spatial data the minimum requirements are having some form of spatial information (often called geometry or shape) and some additional information. This spatial information is normally an x and a y coordinate (but may contain z and time).\n\n\nThe most common spatial data is collectively called Vector Data and is generally more discrete spatial, such as individual points, lines or polygons. For example, we might have the location of capital cities (Point) or the route of a river system (Line) or shape of country borders of a country (Polygon).\n\n\n\n\n\n\n\n\n\n\n\n\nAnother common spatial data style is Raster Data, which is more continuous spatially and is sometimes called a spatial field or gridded data. For example bathymetry or elevation of an area will be a grid of x and y spatial cells with a bathymetry or elevation value for each ‘cell’ of the grid.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we see above, we can combine all these data types to make displays of our data. But also we can combine these data types to perform more complex manipulations or analyses. First of all lets bring in some vector data with the sf package. The sf package allows us to bring in our vector data easily and inspect, manipulate and plot it in a very similar way to using a normal dataframe. The only difference is that we have a column called geometry that stores the spatial data for each row. Above in the map of south america with the amazon and capital cities we had a combination of different vector spatial data.\nThe first data type, point data can be created by ourselves in r, without having to read any data in. First we make a data frame with the information of each point (latitude, longitude, name), then we use sf to convert this df to an sf object, telling sf which columns are our geomtry info and what coordinate reference system we are working in. This wasn’t very automatic as I found the values online and copy and pasted them into r.\n\nlibrary(tidyverse)\nlibrary(sf)\n\nPoints_df&lt;-data.frame(\n  City=c(\n    \"Brasilia\", \"Quito\", \"Santiago\", \n    \"Lima\", \"Buenos Aires\", \"La Paz\", \n    \"Bogotá\", \"Asunción\", \"Montevideo\", \n    \"Caracas\", \"Cayenne\", \"Paramaribo\",\n         \"Georgetown\"\n  ) ,   \n  Long=c(-47.9297,-78.5250,-70.6483,-77.0282,\n         -58.4004,-68.1500,-74.0818,-57.6359,\n         -56.1674,-66.8792,-52.3333,-55.1668,-58.1553),                         \n  Lat=c(-15.7797,    -0.2299,    -33.4569, -12.0432,\n        -34.6051, -16.5000,   4.6097, -25.3007,\n        -34.8335, 10.4880, 4.9333, 5.8664, 6.8045)\n  )  \n\nhead(Points_df)\n\n          City     Long      Lat\n1     Brasilia -47.9297 -15.7797\n2        Quito -78.5250  -0.2299\n3     Santiago -70.6483 -33.4569\n4         Lima -77.0282 -12.0432\n5 Buenos Aires -58.4004 -34.6051\n6       La Paz -68.1500 -16.5000\n\nPoints_sf&lt;-Points_df%&gt;% \n  st_as_sf(coords=c(\"Long\",\"Lat\") )  %&gt;% \n  st_set_crs(\"EPSG:4326\") \n\nhead(Points_sf)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -78.525 ymin: -34.6051 xmax: -47.9297 ymax: -0.2299\nGeodetic CRS:  WGS 84\n          City                  geometry\n1     Brasilia POINT (-47.9297 -15.7797)\n2        Quito   POINT (-78.525 -0.2299)\n3     Santiago POINT (-70.6483 -33.4569)\n4         Lima POINT (-77.0282 -12.0432)\n5 Buenos Aires POINT (-58.4004 -34.6051)\n6       La Paz      POINT (-68.15 -16.5)\n\n\nWe can see when we inspect the sf object it has more spatial information that sf has assigned to the df when converting. As r recognises an sf object as being spatial now we can use the base plot function to look at the point locations. This isn’t an amazing plot as there is not much information in the sf object, just city names.\n\nplot(Points_sf)\n\n\n\n\n\n\n\n\nAs it acts just like any other dataframe we can add data, subset, manipulate and join in the same way we would with tidyverse.\n\nFake_ElevationData&lt;-runif(13,min=0,max=3000)\n\nPoints_sf_Elevate&lt;-Points_sf %&gt;% \n  mutate(Elev=Fake_ElevationData)\n\nLocation_Bogota&lt;-Points_sf_Elevate %&gt;% \n  filter(City==\"Bogotá\")\n\n\n\n\nWe can also perform analyses between different spatial objects. So if we get spatial information for boundaries of countries in the world we can use our capital cities to join and subset the dataset. We will use the naturalhires suite of packages to get polygons of world boundaries. Lets bring in the data using a 10 metre resolution, convert it to an sf object and check it has worked well.\n\nlibrary(rnaturalearth) \nlibrary(rnaturalearthdata) \n#remotes::install_github(\"ropensci/rnaturalearthhires\")\nlibrary(rnaturalearthhires) \n\nworld_map &lt;- sovereignty10 %&gt;% \n  st_as_sf() \n\nggplot(world_map)+\n  geom_sf(aes(fill=REGION_UN),alpha=0.7)+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can also inspect the information attached to each row in the sf file for each country. But i will select some specific columns of interest to neaten the dataset up first.\n\nworld_map&lt;-world_map %&gt;% \n  select(SOVEREIGNT,ADM0_A3,CONTINENT,\n         REGION_UN,SUBREGION,POP_EST,\n         POP_RANK, POP_YEAR, GDP_MD, GDP_YEAR)\n\nhead(world_map)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.4537 ymin: -55.9185 xmax: 140.9776 ymax: 7.35578\nGeodetic CRS:  WGS 84\n  SOVEREIGNT ADM0_A3     CONTINENT REGION_UN          SUBREGION   POP_EST\n1  Indonesia     IDN          Asia      Asia South-Eastern Asia 270625568\n2   Malaysia     MYS          Asia      Asia South-Eastern Asia  31949777\n3      Chile     CHL South America  Americas      South America  18952038\n4    Bolivia     BOL South America  Americas      South America  11513100\n5       Peru     PER South America  Americas      South America  32510453\n6  Argentina     ARG South America  Americas      South America  44938712\n  POP_RANK POP_YEAR  GDP_MD GDP_YEAR                       geometry\n1       17     2019 1119190     2019 MULTIPOLYGON (((117.7036 4....\n2       15     2019  364681     2019 MULTIPOLYGON (((117.7036 4....\n3       14     2019  282318     2019 MULTIPOLYGON (((-69.51009 -...\n4       14     2019   40895     2019 MULTIPOLYGON (((-69.51009 -...\n5       15     2019  226848     2019 MULTIPOLYGON (((-69.51009 -...\n6       15     2019  445445     2019 MULTIPOLYGON (((-67.1939 -2...\n\n\nIf we plot our cities on top of the countries we can see it is only a select few cities that we have. Maybe we want to extract only the polygons that one of our cities lands inside of spatially?\n\nggplot(world_map)+\n  geom_sf()+\n  geom_sf(data=Points_sf,fill=\"#dbb13b\",colour=\"white\",shape=23,size=3)+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can use a join to combine the sf objects. To carry out spatial joins we need to tell sf that we want to work in two d space (not a true sphere).\n\nsf_use_s2(FALSE)\n\nSpherical geometry (s2) switched off\n\nSouthAmerica&lt;-world_map %&gt;% \n  st_join(Points_sf,left=F)\n\nalthough coordinates are longitude/latitude, st_intersects assumes that they\nare planar\n\nggplot(SouthAmerica)+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nHmm this is looking a bit weird, we seem to have gained all the French lands. This is because in our world_map sf object, we had multiple polygons per row. Therefore, the row that contained the French Guiana also contained all of Frances territories. So perhaps we want to do a spatial filter. There are many ways to do this, the simplest is to create a bounding box then subset the SouthAmerica sf based on the new spatial object. Lets create a bounding box then plot it over our map to see where it is.\n\nylims &lt;- c(-65, 20)\n\nxlims &lt;- c(-100, -20)\n\nbox_coords &lt;- tibble(x = xlims, y = ylims) %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"),crs=\"EPSG:4326\") %&gt;%\n  st_bbox()%&gt;% \n  st_as_sfc()\n\n\nggplot(SouthAmerica)+\n  geom_sf()+\n  geom_sf(data=box_coords,colour=\"red\",fill=NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks like a good subset for now.\n\nSouthAmerica_subset &lt;- st_intersection(SouthAmerica, box_coords)\n\nalthough coordinates are longitude/latitude, st_intersection assumes that they\nare planar\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nggplot(SouthAmerica_subset)+\n  geom_sf()+\n  geom_sf(data=box_coords,colour=\"red\",fill=NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nMuch Better!\n\n\n\nSo lets get some Vector Data and load it into r from our local system. I downloaded river shape file from: https://datacatalog.worldbank.org/search/dataset/0042032/Major-Rivers-of-the-World\n\nRivers&lt;-st_read(\"majorrivers_0_0/MajorRivers.shp\",quiet=T) \n\nggplot(Rivers)+\n  geom_sf(colour=\"#6cc3d5\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks to be correct, so lets do as before and select the rivers that are in south america. We could select the Amazon by using dplyr::filter(SYSEM==“Amazon”) but lets reuse our bounding box from earlier to select all rivers in south america.\n\nSouthAmerica_rivers &lt;- st_intersection(Rivers, box_coords)\n\nalthough coordinates are longitude/latitude, st_intersection assumes that they\nare planar\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nggplot(SouthAmerica_rivers)+\n  geom_sf(colour=\"#6cc3d5\")+\n  geom_sf(data=box_coords,colour=\"red\",fill=NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nThere are extra bits of information in this sf that we can treat just like a normal df. How long are all the rivers in south america? (In this shape file that is)\n\nsum(SouthAmerica_rivers$KILOMETERS)\n\n[1] 39700.29\n\n\nWhich is the maximum?\n\nSouthAmerica_rivers %&gt;%\n             filter(KILOMETERS == max(KILOMETERS))\n\nSimple feature collection with 1 feature and 4 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -73.5079 ymin: -4.459834 xmax: -52.70839 ymax: -1.583623\nGeodetic CRS:  WGS 84\n    NAME SYSTEM    MILES KILOMETERS                       geometry\n1 Amazon Amazon 1890.428   3042.355 MULTILINESTRING ((-73.5079 ...\n\n\nThe Amazon, surprisingly!\nLets combine all these Vector data into a single plot.\n\nggplot(SouthAmerica_subset)+\n  geom_sf(linewidth=0.1,alpha=0.8,\n          fill=\"palegreen3\",colour=\"grey30\")+\n  geom_sf(data=SouthAmerica_rivers,colour=\"#6cc3d5\")+\n  geom_sf(data=Points_sf,aes(colour=City))+\n  geom_sf(data=box_coords,colour=\"red\",fill=NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLike with all data forms in r, see vector data above, we can create our own version of raster data. The most important information for raster data is the dimensions (x, y, z, etc.), the resolution (how big each pixel is) and its extent (where it is in space). We can make a random noise raster, then inspect it by plotting it. Rasters in r generally are well plotted by the base plot() function. But we can also use the tidyterra package, which adds a new geom_* to ggplot.\n\nlibrary(terra)\nlibrary(tidyterra)\n\nr&lt;-rast(ncol=100,nrow=100,nlyrs=5)\n\nvalues(r[[1]])&lt;-sort(runif(1:ncell(r)))\n\nvalues(r[[2]])&lt;-values(r[[1]])*-1\n\nvalues(r[[3]])&lt;-sort(runif(1:ncell(r)))\n\nvalues(r[[4]])&lt;-values(r[[3]])*0.6\n\nvalues(r[[5]])&lt;-sort(runif(1:ncell(r)))\n\nplot(r)\n\n\n\n\n\n\n\nggplot()+\n  geom_spatraster(data=r)+\n  facet_wrap(~lyr)+\n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo far we haven’t added any information about where this raster is. We can inspect it to find what are its dimensions, resolution and extent.\n\ndim(r)\n\n[1] 100 100   5\n\nres(r)\n\n[1] 3.6 1.8\n\next(r)\n\nSpatExtent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax)\n\n\nWe can see that this raster has 100 rows, 100 columns and 5 layers. Layers are a bit more complicated but they are often visualised like multiple layers of croissant dough (Pâtes feuilletée), with each layer being laid ontop of the other spatially but some containing different information (butter or pastry). It also has a resolution of 3.6 by 1.8 per pixel. And we can see what this information means in relation to its extent. Which is -180 to 180 and -90 to 90, which are the default before we tell raster where the raster is located in space. Once we change the extent we can see that our resolution changes too.\n\next(r)&lt;-c(0,100,0,100)\n\nres(r)\n\n[1] 1 1\n\next(r)&lt;-c(0,50,0,50)\n\nres(r)\n\n[1] 0.5 0.5\n\n\nAs we see the difference in extent is divided by the dimension to give the resolution. If we don’t pay attention to this we can get some weird results when changing dimension, resolution or extent.\n\n\n\nLets look at some real world raster data.\n\nlibrary(geodata)\n\navgtemp &lt;- worldclim_global(var=\"tavg\",res=10,path=tempdir())\n\nplot(avgtemp)\n\n\n\n\n\n\n\nres(avgtemp)\n\n[1] 0.1666667 0.1666667\n\ndim(avgtemp)\n\n[1] 1080 2160   12\n\next(avgtemp)\n\nSpatExtent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax)\n\n\nThis is a big dataset and global, so lets subset it spatially. We can use our bounding box from earlier to do this but first lets convert our sf object to a spatvector object. soatvector and sf objects are very similar, but spatvector plays a bit nicer with spatrasters, whereas sf objects are generally easier to manipulate and analyse using our well practiced dplyr and tidyr skills.\n\nBox_terra&lt;-vect(box_coords)\n\navgtemp_SoAm&lt;-avgtemp %&gt;% \n  crop(Box_terra)\n\nggplot()+\n  geom_spatraster(data=avgtemp_SoAm)+\n  facet_wrap(~lyr)+\n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay we have data for every month, this is interesting but a lot to deal with, lets take the mean across all layers. We could also resample to make computation faster but the raster will become more pixelated.\n\navgtemp_SoAm_mean&lt;-avgtemp_SoAm %&gt;% \n  mean()\n\nggplot()+\n  geom_spatraster(data=avgtemp_SoAm_mean)+\n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\navgtemp_SoAm_mean_agg&lt;-avgtemp_SoAm_mean %&gt;% \n  terra::aggregate(fact=10,fun=\"mean\",na.rm=T)\n\nggplot()+\n  geom_spatraster(data=avgtemp_SoAm_mean_agg)+\n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay lets use this climate data to look at the average temperatures (across the whole year) in our shape file of countries. Lets try two methods, take the centroid of each country then extract the average temperature for that location or we can take the average values within each country then average across its extent.\n\nCountries&lt;-str_replace_all(SouthAmerica_subset$SOVEREIGNT,\"France\",\"French Guiana\") # This is just to make it more correct for plotting later\n\ncountries_vect&lt;-vect(SouthAmerica_subset)\n\ncentroids_SoAm&lt;-countries_vect %&gt;% \n  centroids() \n\nggplot()+\n  geom_spatraster(data=avgtemp_SoAm_mean)+\n  geom_spatvector(data=countries_vect,fill=NA)+\n  geom_spatvector(data=centroids_SoAm)+  \n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\nCentroid_temps&lt;-avgtemp_SoAm_mean %&gt;% \n  extract(centroids_SoAm) %&gt;% \n  rename(Centroid=mean) %&gt;% \n  mutate(Countries=Countries)\n\nCountry_temps&lt;-avgtemp_SoAm_mean %&gt;% \n  extract(countries_vect,mean)%&gt;% \n  rename(Extract=mean) %&gt;% \n  mutate(Countries=Countries)\n\nWe can compare these different methods for finding a countries average temperature.\n\nTemps&lt;-Centroid_temps %&gt;% \n  full_join(Country_temps,by=c(\"ID\",\"Countries\")) %&gt;% \n  pivot_longer(-c(ID,Countries),names_to = \"Method\",values_to = \"Temp\")\n\nTemps_Segment&lt;-Centroid_temps %&gt;% \n  full_join(Country_temps,by=c(\"ID\",\"Countries\")) \n\nggplot(Temps)+\n  geom_segment(data=Temps_Segment,aes(x = Countries, xend = Countries, \n                                     y = Extract, yend = Centroid),\n               colour=\"#f3969a\")+\n  geom_point(aes(x=Countries,y=Temp,fill=Method),size=3,shape=23)+\n  scale_fill_manual(name=\"Method\",values=c(\"#36b779\",\"#fcd32c\"))+\n  labs(y=\"Temperature\")+\n  theme_classic()+\n  coord_flip()\n\n\n\n\n\n\n\n\nWe can see some countries with little variation between the centroid and the overall mean. But some countries have a big difference (Ecuador for example). Obviously, we could do this in a multitude of different ways and it would depend on the objective that we had.\n\n\n\nNow lets load in some local raster data and maybe we can combine all of the above methods together. I went to this website and downloaded a raster of Sea Surface Temperature from 20240207: https://www.cpc.ncep.noaa.gov/products/GIS/GIS_DATA/sst_oiv2/index.php.\n\nSST&lt;-rast(\"sst_io.20240207.tif\")\n\nSST\n\nclass       : SpatRaster \ndimensions  : 181, 361, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -0.5, 360.5, -90.5, 90.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : sst_io.20240207.tif \nname        : sst_io.20240207 \n\nsummary(SST)\n\n sst_io.20240207   \n Min.   :-999.000  \n 1st Qu.:-999.000  \n Median :   1.307  \n Mean   :-325.907  \n 3rd Qu.:  21.861  \n Max.   :  31.270  \n\n\nBy inspecting the tif we can see there are some odd values (-999). This is likely to be a NA value so we should set them as NA. Also, the longitude is 0-360 rather than -180-180. This creates a pacific centric view, but we need to change this so we can extract the values we want.\n\nNAflag(SST)&lt;-c(-999)\n\nsummary(SST)\n\n sst_io.20240207 \n Min.   :-1.800  \n 1st Qu.: 1.369  \n Median :15.748  \n Mean   :14.190  \n 3rd Qu.:26.281  \n Max.   :31.270  \n NA's   :21933   \n\nSST_Atlantic&lt;-rotate(SST)\n\nggplot()+\n  geom_spatraster(data=SST_Atlantic)+\n  scale_fill_terrain_c()+\n  labs(fill=\"Temperature\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nAgain lets crop this to just South America.\n\nSST_SoAm&lt;-SST_Atlantic %&gt;% \n  crop(Box_terra)\n\nggplot()+\n  geom_spatraster(data=SST_SoAm)+\n  scale_fill_terrain_c()+\n  labs(fill=\"Temperature\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks good, now lets bring together all the things we have been looking at to create one map of South America. (all the temperatures are different ranges so don’t read too much into any of it!!!). We will be using multiple scales for the same aesthetic (fill) in this so we will use the ggnewscale package to reset scales.\n\nlibrary(ggnewscale)\n\nggplot()+\n  geom_spatraster(data=SST_SoAm,alpha=0.7)+\n  scale_fill_whitebox_c(\"deep\",name=\"Sea Surface\\nTemperature:\\n 07/02/2024\")+\n  new_scale_fill()+\n  geom_spatraster(data=avgtemp_SoAm_mean)+\n  geom_spatvector(data=countries_vect,fill=NA)+\n  scale_fill_terrain_c(name=\"Average Land\\nTemperature\\nfrom WorldClim\")+\n  new_scale_fill()+\n  geom_sf(data=SouthAmerica_rivers,colour=\"#6cc3d5\")+\n  geom_sf(data=Points_sf,aes(fill=City),shape=23,size=5)+\n  scale_fill_whitebox_d(\"viridi\",name=\"Cities of South America\")+\n  theme_classic()+\n  theme(legend.direction = \"vertical\",\n        legend.position = \"right\")",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Spatial Data in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/StarterGIS.html#data-types",
    "href": "IntermediateRTutorials/StarterGIS.html#data-types",
    "title": "Spatial Data in R",
    "section": "",
    "text": "Generally, when we talk about spatial data the minimum requirements are having some form of spatial information (often called geometry or shape) and some additional information. This spatial information is normally an x and a y coordinate (but may contain z and time).\n\n\nThe most common spatial data is collectively called Vector Data and is generally more discrete spatial, such as individual points, lines or polygons. For example, we might have the location of capital cities (Point) or the route of a river system (Line) or shape of country borders of a country (Polygon).\n\n\n\n\n\n\n\n\n\n\n\n\nAnother common spatial data style is Raster Data, which is more continuous spatially and is sometimes called a spatial field or gridded data. For example bathymetry or elevation of an area will be a grid of x and y spatial cells with a bathymetry or elevation value for each ‘cell’ of the grid.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Spatial Data in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/StarterGIS.html#spatial-basics-vector-data",
    "href": "IntermediateRTutorials/StarterGIS.html#spatial-basics-vector-data",
    "title": "Spatial Data in R",
    "section": "",
    "text": "As we see above, we can combine all these data types to make displays of our data. But also we can combine these data types to perform more complex manipulations or analyses. First of all lets bring in some vector data with the sf package. The sf package allows us to bring in our vector data easily and inspect, manipulate and plot it in a very similar way to using a normal dataframe. The only difference is that we have a column called geometry that stores the spatial data for each row. Above in the map of south america with the amazon and capital cities we had a combination of different vector spatial data.\nThe first data type, point data can be created by ourselves in r, without having to read any data in. First we make a data frame with the information of each point (latitude, longitude, name), then we use sf to convert this df to an sf object, telling sf which columns are our geomtry info and what coordinate reference system we are working in. This wasn’t very automatic as I found the values online and copy and pasted them into r.\n\nlibrary(tidyverse)\nlibrary(sf)\n\nPoints_df&lt;-data.frame(\n  City=c(\n    \"Brasilia\", \"Quito\", \"Santiago\", \n    \"Lima\", \"Buenos Aires\", \"La Paz\", \n    \"Bogotá\", \"Asunción\", \"Montevideo\", \n    \"Caracas\", \"Cayenne\", \"Paramaribo\",\n         \"Georgetown\"\n  ) ,   \n  Long=c(-47.9297,-78.5250,-70.6483,-77.0282,\n         -58.4004,-68.1500,-74.0818,-57.6359,\n         -56.1674,-66.8792,-52.3333,-55.1668,-58.1553),                         \n  Lat=c(-15.7797,    -0.2299,    -33.4569, -12.0432,\n        -34.6051, -16.5000,   4.6097, -25.3007,\n        -34.8335, 10.4880, 4.9333, 5.8664, 6.8045)\n  )  \n\nhead(Points_df)\n\n          City     Long      Lat\n1     Brasilia -47.9297 -15.7797\n2        Quito -78.5250  -0.2299\n3     Santiago -70.6483 -33.4569\n4         Lima -77.0282 -12.0432\n5 Buenos Aires -58.4004 -34.6051\n6       La Paz -68.1500 -16.5000\n\nPoints_sf&lt;-Points_df%&gt;% \n  st_as_sf(coords=c(\"Long\",\"Lat\") )  %&gt;% \n  st_set_crs(\"EPSG:4326\") \n\nhead(Points_sf)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -78.525 ymin: -34.6051 xmax: -47.9297 ymax: -0.2299\nGeodetic CRS:  WGS 84\n          City                  geometry\n1     Brasilia POINT (-47.9297 -15.7797)\n2        Quito   POINT (-78.525 -0.2299)\n3     Santiago POINT (-70.6483 -33.4569)\n4         Lima POINT (-77.0282 -12.0432)\n5 Buenos Aires POINT (-58.4004 -34.6051)\n6       La Paz      POINT (-68.15 -16.5)\n\n\nWe can see when we inspect the sf object it has more spatial information that sf has assigned to the df when converting. As r recognises an sf object as being spatial now we can use the base plot function to look at the point locations. This isn’t an amazing plot as there is not much information in the sf object, just city names.\n\nplot(Points_sf)\n\n\n\n\n\n\n\n\nAs it acts just like any other dataframe we can add data, subset, manipulate and join in the same way we would with tidyverse.\n\nFake_ElevationData&lt;-runif(13,min=0,max=3000)\n\nPoints_sf_Elevate&lt;-Points_sf %&gt;% \n  mutate(Elev=Fake_ElevationData)\n\nLocation_Bogota&lt;-Points_sf_Elevate %&gt;% \n  filter(City==\"Bogotá\")\n\n\n\n\nWe can also perform analyses between different spatial objects. So if we get spatial information for boundaries of countries in the world we can use our capital cities to join and subset the dataset. We will use the naturalhires suite of packages to get polygons of world boundaries. Lets bring in the data using a 10 metre resolution, convert it to an sf object and check it has worked well.\n\nlibrary(rnaturalearth) \nlibrary(rnaturalearthdata) \n#remotes::install_github(\"ropensci/rnaturalearthhires\")\nlibrary(rnaturalearthhires) \n\nworld_map &lt;- sovereignty10 %&gt;% \n  st_as_sf() \n\nggplot(world_map)+\n  geom_sf(aes(fill=REGION_UN),alpha=0.7)+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can also inspect the information attached to each row in the sf file for each country. But i will select some specific columns of interest to neaten the dataset up first.\n\nworld_map&lt;-world_map %&gt;% \n  select(SOVEREIGNT,ADM0_A3,CONTINENT,\n         REGION_UN,SUBREGION,POP_EST,\n         POP_RANK, POP_YEAR, GDP_MD, GDP_YEAR)\n\nhead(world_map)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.4537 ymin: -55.9185 xmax: 140.9776 ymax: 7.35578\nGeodetic CRS:  WGS 84\n  SOVEREIGNT ADM0_A3     CONTINENT REGION_UN          SUBREGION   POP_EST\n1  Indonesia     IDN          Asia      Asia South-Eastern Asia 270625568\n2   Malaysia     MYS          Asia      Asia South-Eastern Asia  31949777\n3      Chile     CHL South America  Americas      South America  18952038\n4    Bolivia     BOL South America  Americas      South America  11513100\n5       Peru     PER South America  Americas      South America  32510453\n6  Argentina     ARG South America  Americas      South America  44938712\n  POP_RANK POP_YEAR  GDP_MD GDP_YEAR                       geometry\n1       17     2019 1119190     2019 MULTIPOLYGON (((117.7036 4....\n2       15     2019  364681     2019 MULTIPOLYGON (((117.7036 4....\n3       14     2019  282318     2019 MULTIPOLYGON (((-69.51009 -...\n4       14     2019   40895     2019 MULTIPOLYGON (((-69.51009 -...\n5       15     2019  226848     2019 MULTIPOLYGON (((-69.51009 -...\n6       15     2019  445445     2019 MULTIPOLYGON (((-67.1939 -2...\n\n\nIf we plot our cities on top of the countries we can see it is only a select few cities that we have. Maybe we want to extract only the polygons that one of our cities lands inside of spatially?\n\nggplot(world_map)+\n  geom_sf()+\n  geom_sf(data=Points_sf,fill=\"#dbb13b\",colour=\"white\",shape=23,size=3)+\n  theme_classic()\n\n\n\n\n\n\n\n\nWe can use a join to combine the sf objects. To carry out spatial joins we need to tell sf that we want to work in two d space (not a true sphere).\n\nsf_use_s2(FALSE)\n\nSpherical geometry (s2) switched off\n\nSouthAmerica&lt;-world_map %&gt;% \n  st_join(Points_sf,left=F)\n\nalthough coordinates are longitude/latitude, st_intersects assumes that they\nare planar\n\nggplot(SouthAmerica)+\n  geom_sf()+\n  theme_classic()\n\n\n\n\n\n\n\n\nHmm this is looking a bit weird, we seem to have gained all the French lands. This is because in our world_map sf object, we had multiple polygons per row. Therefore, the row that contained the French Guiana also contained all of Frances territories. So perhaps we want to do a spatial filter. There are many ways to do this, the simplest is to create a bounding box then subset the SouthAmerica sf based on the new spatial object. Lets create a bounding box then plot it over our map to see where it is.\n\nylims &lt;- c(-65, 20)\n\nxlims &lt;- c(-100, -20)\n\nbox_coords &lt;- tibble(x = xlims, y = ylims) %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"),crs=\"EPSG:4326\") %&gt;%\n  st_bbox()%&gt;% \n  st_as_sfc()\n\n\nggplot(SouthAmerica)+\n  geom_sf()+\n  geom_sf(data=box_coords,colour=\"red\",fill=NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks like a good subset for now.\n\nSouthAmerica_subset &lt;- st_intersection(SouthAmerica, box_coords)\n\nalthough coordinates are longitude/latitude, st_intersection assumes that they\nare planar\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nggplot(SouthAmerica_subset)+\n  geom_sf()+\n  geom_sf(data=box_coords,colour=\"red\",fill=NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nMuch Better!\n\n\n\nSo lets get some Vector Data and load it into r from our local system. I downloaded river shape file from: https://datacatalog.worldbank.org/search/dataset/0042032/Major-Rivers-of-the-World\n\nRivers&lt;-st_read(\"majorrivers_0_0/MajorRivers.shp\",quiet=T) \n\nggplot(Rivers)+\n  geom_sf(colour=\"#6cc3d5\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks to be correct, so lets do as before and select the rivers that are in south america. We could select the Amazon by using dplyr::filter(SYSEM==“Amazon”) but lets reuse our bounding box from earlier to select all rivers in south america.\n\nSouthAmerica_rivers &lt;- st_intersection(Rivers, box_coords)\n\nalthough coordinates are longitude/latitude, st_intersection assumes that they\nare planar\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nggplot(SouthAmerica_rivers)+\n  geom_sf(colour=\"#6cc3d5\")+\n  geom_sf(data=box_coords,colour=\"red\",fill=NA)+\n  theme_classic()\n\n\n\n\n\n\n\n\nThere are extra bits of information in this sf that we can treat just like a normal df. How long are all the rivers in south america? (In this shape file that is)\n\nsum(SouthAmerica_rivers$KILOMETERS)\n\n[1] 39700.29\n\n\nWhich is the maximum?\n\nSouthAmerica_rivers %&gt;%\n             filter(KILOMETERS == max(KILOMETERS))\n\nSimple feature collection with 1 feature and 4 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -73.5079 ymin: -4.459834 xmax: -52.70839 ymax: -1.583623\nGeodetic CRS:  WGS 84\n    NAME SYSTEM    MILES KILOMETERS                       geometry\n1 Amazon Amazon 1890.428   3042.355 MULTILINESTRING ((-73.5079 ...\n\n\nThe Amazon, surprisingly!\nLets combine all these Vector data into a single plot.\n\nggplot(SouthAmerica_subset)+\n  geom_sf(linewidth=0.1,alpha=0.8,\n          fill=\"palegreen3\",colour=\"grey30\")+\n  geom_sf(data=SouthAmerica_rivers,colour=\"#6cc3d5\")+\n  geom_sf(data=Points_sf,aes(colour=City))+\n  geom_sf(data=box_coords,colour=\"red\",fill=NA)+\n  theme_classic()",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Spatial Data in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/StarterGIS.html#spatial-basics-raster-data",
    "href": "IntermediateRTutorials/StarterGIS.html#spatial-basics-raster-data",
    "title": "Spatial Data in R",
    "section": "",
    "text": "Like with all data forms in r, see vector data above, we can create our own version of raster data. The most important information for raster data is the dimensions (x, y, z, etc.), the resolution (how big each pixel is) and its extent (where it is in space). We can make a random noise raster, then inspect it by plotting it. Rasters in r generally are well plotted by the base plot() function. But we can also use the tidyterra package, which adds a new geom_* to ggplot.\n\nlibrary(terra)\nlibrary(tidyterra)\n\nr&lt;-rast(ncol=100,nrow=100,nlyrs=5)\n\nvalues(r[[1]])&lt;-sort(runif(1:ncell(r)))\n\nvalues(r[[2]])&lt;-values(r[[1]])*-1\n\nvalues(r[[3]])&lt;-sort(runif(1:ncell(r)))\n\nvalues(r[[4]])&lt;-values(r[[3]])*0.6\n\nvalues(r[[5]])&lt;-sort(runif(1:ncell(r)))\n\nplot(r)\n\n\n\n\n\n\n\nggplot()+\n  geom_spatraster(data=r)+\n  facet_wrap(~lyr)+\n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\n\nSo far we haven’t added any information about where this raster is. We can inspect it to find what are its dimensions, resolution and extent.\n\ndim(r)\n\n[1] 100 100   5\n\nres(r)\n\n[1] 3.6 1.8\n\next(r)\n\nSpatExtent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax)\n\n\nWe can see that this raster has 100 rows, 100 columns and 5 layers. Layers are a bit more complicated but they are often visualised like multiple layers of croissant dough (Pâtes feuilletée), with each layer being laid ontop of the other spatially but some containing different information (butter or pastry). It also has a resolution of 3.6 by 1.8 per pixel. And we can see what this information means in relation to its extent. Which is -180 to 180 and -90 to 90, which are the default before we tell raster where the raster is located in space. Once we change the extent we can see that our resolution changes too.\n\next(r)&lt;-c(0,100,0,100)\n\nres(r)\n\n[1] 1 1\n\next(r)&lt;-c(0,50,0,50)\n\nres(r)\n\n[1] 0.5 0.5\n\n\nAs we see the difference in extent is divided by the dimension to give the resolution. If we don’t pay attention to this we can get some weird results when changing dimension, resolution or extent.\n\n\n\nLets look at some real world raster data.\n\nlibrary(geodata)\n\navgtemp &lt;- worldclim_global(var=\"tavg\",res=10,path=tempdir())\n\nplot(avgtemp)\n\n\n\n\n\n\n\nres(avgtemp)\n\n[1] 0.1666667 0.1666667\n\ndim(avgtemp)\n\n[1] 1080 2160   12\n\next(avgtemp)\n\nSpatExtent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax)\n\n\nThis is a big dataset and global, so lets subset it spatially. We can use our bounding box from earlier to do this but first lets convert our sf object to a spatvector object. soatvector and sf objects are very similar, but spatvector plays a bit nicer with spatrasters, whereas sf objects are generally easier to manipulate and analyse using our well practiced dplyr and tidyr skills.\n\nBox_terra&lt;-vect(box_coords)\n\navgtemp_SoAm&lt;-avgtemp %&gt;% \n  crop(Box_terra)\n\nggplot()+\n  geom_spatraster(data=avgtemp_SoAm)+\n  facet_wrap(~lyr)+\n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay we have data for every month, this is interesting but a lot to deal with, lets take the mean across all layers. We could also resample to make computation faster but the raster will become more pixelated.\n\navgtemp_SoAm_mean&lt;-avgtemp_SoAm %&gt;% \n  mean()\n\nggplot()+\n  geom_spatraster(data=avgtemp_SoAm_mean)+\n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\navgtemp_SoAm_mean_agg&lt;-avgtemp_SoAm_mean %&gt;% \n  terra::aggregate(fact=10,fun=\"mean\",na.rm=T)\n\nggplot()+\n  geom_spatraster(data=avgtemp_SoAm_mean_agg)+\n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\n\nOkay lets use this climate data to look at the average temperatures (across the whole year) in our shape file of countries. Lets try two methods, take the centroid of each country then extract the average temperature for that location or we can take the average values within each country then average across its extent.\n\nCountries&lt;-str_replace_all(SouthAmerica_subset$SOVEREIGNT,\"France\",\"French Guiana\") # This is just to make it more correct for plotting later\n\ncountries_vect&lt;-vect(SouthAmerica_subset)\n\ncentroids_SoAm&lt;-countries_vect %&gt;% \n  centroids() \n\nggplot()+\n  geom_spatraster(data=avgtemp_SoAm_mean)+\n  geom_spatvector(data=countries_vect,fill=NA)+\n  geom_spatvector(data=centroids_SoAm)+  \n  scale_fill_terrain_c()+\n  theme_classic()\n\n\n\n\n\n\n\nCentroid_temps&lt;-avgtemp_SoAm_mean %&gt;% \n  extract(centroids_SoAm) %&gt;% \n  rename(Centroid=mean) %&gt;% \n  mutate(Countries=Countries)\n\nCountry_temps&lt;-avgtemp_SoAm_mean %&gt;% \n  extract(countries_vect,mean)%&gt;% \n  rename(Extract=mean) %&gt;% \n  mutate(Countries=Countries)\n\nWe can compare these different methods for finding a countries average temperature.\n\nTemps&lt;-Centroid_temps %&gt;% \n  full_join(Country_temps,by=c(\"ID\",\"Countries\")) %&gt;% \n  pivot_longer(-c(ID,Countries),names_to = \"Method\",values_to = \"Temp\")\n\nTemps_Segment&lt;-Centroid_temps %&gt;% \n  full_join(Country_temps,by=c(\"ID\",\"Countries\")) \n\nggplot(Temps)+\n  geom_segment(data=Temps_Segment,aes(x = Countries, xend = Countries, \n                                     y = Extract, yend = Centroid),\n               colour=\"#f3969a\")+\n  geom_point(aes(x=Countries,y=Temp,fill=Method),size=3,shape=23)+\n  scale_fill_manual(name=\"Method\",values=c(\"#36b779\",\"#fcd32c\"))+\n  labs(y=\"Temperature\")+\n  theme_classic()+\n  coord_flip()\n\n\n\n\n\n\n\n\nWe can see some countries with little variation between the centroid and the overall mean. But some countries have a big difference (Ecuador for example). Obviously, we could do this in a multitude of different ways and it would depend on the objective that we had.\n\n\n\nNow lets load in some local raster data and maybe we can combine all of the above methods together. I went to this website and downloaded a raster of Sea Surface Temperature from 20240207: https://www.cpc.ncep.noaa.gov/products/GIS/GIS_DATA/sst_oiv2/index.php.\n\nSST&lt;-rast(\"sst_io.20240207.tif\")\n\nSST\n\nclass       : SpatRaster \ndimensions  : 181, 361, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -0.5, 360.5, -90.5, 90.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : sst_io.20240207.tif \nname        : sst_io.20240207 \n\nsummary(SST)\n\n sst_io.20240207   \n Min.   :-999.000  \n 1st Qu.:-999.000  \n Median :   1.307  \n Mean   :-325.907  \n 3rd Qu.:  21.861  \n Max.   :  31.270  \n\n\nBy inspecting the tif we can see there are some odd values (-999). This is likely to be a NA value so we should set them as NA. Also, the longitude is 0-360 rather than -180-180. This creates a pacific centric view, but we need to change this so we can extract the values we want.\n\nNAflag(SST)&lt;-c(-999)\n\nsummary(SST)\n\n sst_io.20240207 \n Min.   :-1.800  \n 1st Qu.: 1.369  \n Median :15.748  \n Mean   :14.190  \n 3rd Qu.:26.281  \n Max.   :31.270  \n NA's   :21933   \n\nSST_Atlantic&lt;-rotate(SST)\n\nggplot()+\n  geom_spatraster(data=SST_Atlantic)+\n  scale_fill_terrain_c()+\n  labs(fill=\"Temperature\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nAgain lets crop this to just South America.\n\nSST_SoAm&lt;-SST_Atlantic %&gt;% \n  crop(Box_terra)\n\nggplot()+\n  geom_spatraster(data=SST_SoAm)+\n  scale_fill_terrain_c()+\n  labs(fill=\"Temperature\")+\n  theme_classic()\n\n\n\n\n\n\n\n\nThis looks good, now lets bring together all the things we have been looking at to create one map of South America. (all the temperatures are different ranges so don’t read too much into any of it!!!). We will be using multiple scales for the same aesthetic (fill) in this so we will use the ggnewscale package to reset scales.\n\nlibrary(ggnewscale)\n\nggplot()+\n  geom_spatraster(data=SST_SoAm,alpha=0.7)+\n  scale_fill_whitebox_c(\"deep\",name=\"Sea Surface\\nTemperature:\\n 07/02/2024\")+\n  new_scale_fill()+\n  geom_spatraster(data=avgtemp_SoAm_mean)+\n  geom_spatvector(data=countries_vect,fill=NA)+\n  scale_fill_terrain_c(name=\"Average Land\\nTemperature\\nfrom WorldClim\")+\n  new_scale_fill()+\n  geom_sf(data=SouthAmerica_rivers,colour=\"#6cc3d5\")+\n  geom_sf(data=Points_sf,aes(fill=City),shape=23,size=5)+\n  scale_fill_whitebox_d(\"viridi\",name=\"Cities of South America\")+\n  theme_classic()+\n  theme(legend.direction = \"vertical\",\n        legend.position = \"right\")",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Spatial Data in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/CombiningPlots.html",
    "href": "IntermediateRTutorials/CombiningPlots.html",
    "title": "Combining Multiple Plots in R",
    "section": "",
    "text": "We have two options for plotting many plots together. The first would be using facetting, which is where you use the same response variables and split your plots across some grouping factor within your data. This can be very useful but is in specific gridded formats of data with each sub plot being the same size. Sometimes we won’t want that, if we are making a selection of plots and combining with images or maps or maybe just not related plots. To do this we can use a wide selection of packages such as cowplot, ggarrange, grid or Patchwork. My personal favourite is patchwork for its simplicity, integration with ggplot2 and its flexibility.\n\n\nFor facetting we will normally be using at least one of the same axes across the plots. For example, we might look at the height to weight association across Starwars characters.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"starwars\")\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nLet’s assess height against mass of all the characters in this dplyr dataset.\n\nstarwars %&gt;%\n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\nHmmm I wonder who that heavy thing is? Perhaps we want to look at the different species as different colours?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=species))+\n  theme_classic()\n\n\n\n\n\n\n\n\nHmm this is not that easy to see, and even facetting may not be that great but lets see.\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=species))+\n  theme_classic()+\n  facet_wrap(~species)\n\n\n\n\n\n\n\n\nMaybe we could compare hair colour?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color)\n\n\n\n\n\n\n\n\nAlot of characters without hair, okay lets allow each facet (individual subplot) to have a different y scale.\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color,scales = \"free_y\")\n\n\n\n\n\n\n\n\nThat is better but we could also allow different scales for the x axis too?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color,scales = \"free\")\n\n\n\n\n\n\n\n\nWhat does this look like for eye colour?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=eye_color))+\n  theme_classic()+\n  facet_wrap(~eye_color,scales = \"free\")\n\n\n\n\n\n\n\n\nOkay so none of these plots are very nice as the starwars characters are very well spread in their physical characteristics. Maybe we can group some of these lesser filled groups into “Other”? Then we can do a facet grid with Eye (rows) and Hair (columns) Colours grouped.\n\nstarwars %&gt;% \n  mutate(eye_group=case_when(eye_color%in%c(\"black\",\"brown\",\"dark\",\"red\")~\"Dark Eyes\",\n                             eye_color%in%c(\"blue\",\"blue-gray\",\"gold\",\"green, yellow\",\"hazel\",\"orange\",\"pink\",\"red, blue\",\n                                            \"white\",\"yellow\")~\"Light Eyes\",\n                             TRUE~\"Other\"),\n         hair_group=case_when(hair_color%in%c(\"brown\",\"brown, grey\",\"black\")~\"Dark Hair\",\n                             hair_color%in%c(\"blond\",\"auburn, white\", \"auburn, grey\",\n                                             \"white\",\"grey\",\"auburn\",\"blonde\",\"unknown\")~\"Light Hair\",\n                             TRUE~\"Other\")) %&gt;% \n  drop_na()%&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_group,shape=eye_group))+\n  theme_classic()+\n  facet_grid(eye_group~hair_group,scales = \"free\")\n\n\n\n\n\n\n\n\nNot amazingly illuminating but shows the use of facets. When using facet grid it automatically removes repeated axes.\nLets maybe use a slightly different data set, next will be some mpg data from ggplot2. This data is to do with car mile per gallon and different elements of the engine.\nWe can use facet_wrap() or facet_grid(). we have to put a dot after the ‘~’ if we are only facetting by one column. We will look at the type of drive, which is front wheel drive (f), 4x4 (4) or rear-wheel drive (r).\n\ndata(\"mpg\")\n\nmpg2 &lt;- mpg %&gt;% \n  filter(cyl != 5 & class != \"2seater\")\n\n\n mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy, colour=drv)) + \n  geom_point()+\n   facet_grid(~drv)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n\n\n\n\n\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n   facet_grid(drv~.)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n\n\n\n\n\n\n\nLets do some lines on all these points. For this we can use geom_smooth() that creates a loess model around our points. We can also define the number of columns or rows if we use facet_wrap rather than facet_grid().\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,nrow=2)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,ncol=1)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,ncol=3)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nOkay so maybe we want to look at a couple different associations in our data but without having related axes across the plots. To do this with patchwork we can save each plot as an object then print them. We can use + to add other objects to out ‘patchwork’ and build up layouts with () and /, or for more complex layouts we can use a few methods using the function plot_layout() from patchwork.\n\nlibrary(patchwork)\n\n\np1&lt;-mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\np2&lt;-mpg2 %&gt;% \n  ggplot(aes(x=as.factor(year),fill=class)) + \n  geom_bar(position = \"dodge2\")+\n   labs(x=\"Year\",y=\"Number of Models\")+\n   theme_classic()\n\n\n\n\np1+p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\np1/p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can also reuse plots as you like.\n\n(p1+p1+p2)/p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo to make some fairly complicated plot layouts we can use brackets (), slashes / and pluses +. With a slash denoting a new line.\n\n(p1+p2+p2)/p2/(p2+p1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also use some helper functions to tidy our plots up. If for example we have repeated legends across our plots we can collect our legends or ‘guides’.\n\n(p1+p2+p2)/p2/(p2+p1)+plot_layout(guides=\"collect\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs the plots were made with ggplot2 we can also edit the theme of all of them together. To do this we use an ampersand & in our patchwork layout.\n\n(p1+p2+p2)/p2/(p2+p1)+\n  plot_layout(guides=\"collect\")& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use some more advanced layout options for if for example we don’t want to fill the whole grid space. We create a layout object that has a grid spacing, which we can check by plotting, then we apply that layout to a basic patchwork. We need to use the area function for all the plots we want. We will try plot the same lay out as just above, but without stretching plots that are on a row on their own. In the area() function from patchwork (be careful with other packages with the same name function - you can make sure it is correct by using patchwork::area() ) we have four arguments for the top (t), the left (l), the bottom (b) and the right (r). We can put any non-negative numbers in these to create any array of plots.\n\nlayout&lt;-c(\n  area(t=1,l=1,b=1,r=1),\n  area(t=1,l=2,b=1,r=2),\n  area(t=1,l=3,b=1,r=3),\n  area(t=2,l=2,b=2,r=2),\n  area(t=3,l=1,b=3,r=1),\n  area(t=3,l=3,b=3,r=3)\n)\n\nplot(layout)\n\n\n\n\n\n\n\n\nWe can now apply this layout with plot_layout() to a basic list of added up plots.\n\np1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout)& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs patchwork is happy with a ggplot2 object, we could even combine patchworks if we save one patchwork as a global object and added another ggplot to a new patchwork.\n\npatch&lt;-p1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout)& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n\np1/patch\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nUsing layout and its area function can allow us to create more and more complex arrangements, with and without overlaps. We can also add plot labels to help us refer to the plots in the legend. We can use plot_annotation() with tag_levels=“a” for a, b, c etc or “1” for numbers.\n\nlayout2&lt;-c(\n  area(t=1,l=1,b=5,r=5),\n  area(t=1,l=2,b=1,r=2),\n  area(t=1,l=3,b=1,r=3),\n  area(t=2,l=2,b=2,r=2),\n  area(t=3,l=1,b=3,r=1),\n  area(t=3,l=6,b=5,r=9)\n)\n\nplot(layout2)\n\n\n\n\n\n\n\np1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout2)+\n  plot_annotation(tag_levels = \"a\")& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThese don’t look great now but the concept can be really useful for displaying lots of information and especially when making maps or plots where want to zoom in to a certain region to highlight some element of it. Lets get the lakers data from the lubridate package (it is already loaded in tidyverse). We will look at basketball shots on the court with their x and y cordinates, and whether they were missed or made. Let’s make our first plot and build it up slowly.\n\ndata(\"lakers\")\n\n## Default ggplot facet plot\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;% \nggplot()+\n  geom_point(aes(x=x,y=y),\n             alpha=0.4)+\n  facet_wrap(~result)\n\n\n\n\n\n\n\n## Lets fix coordinates and remove axis info\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;% \nggplot()+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  facet_wrap(~result)+\n  coord_fixed()+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Lets add a Hoop and Make the Legend a bit nicer plus different colours\n## We shall also add a square to show where our zoomed in plot will be.\n## To do this we select only the one shots made facet.\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"), ## We can use a dot to show that we are using the data already in the ggplot\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nWarning in geom_rect(data = . %&gt;% filter(result == \"Shot Made\"), aes(xmin = -2, : All aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n## we will also add an arrow into the plot to show where the new zoom plot will be.\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(data=. %&gt;% filter(result==\"Shot Made\"), ## We use the same trick from above to only put arrow on one facet.\n               aes(x = 25, y = 45, xend = 25, yend = 50),\n                  arrow = arrow(length = unit(0.5, \"cm\")),\n               colour=\"darkorange\")+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"),\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nWarning in geom_segment(data = . %&gt;% filter(result == \"Shot Made\"), aes(x = 25, : All aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n## Okay Lets save this one to our global environment\n\nBigPlot&lt;-lakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(data=. %&gt;% filter(result==\"Shot Made\"), \n               aes(x = 25, y = 45, xend = 25, yend = 50),\n                  arrow = arrow(length = unit(0.5, \"cm\")),\n               colour=\"darkorange\")+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"),\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nSo we have our background plot showing all the data and information, now lets zoom in to the area where shots are being made. Lots of points are overlaid here so we could try look at the density of points spatially to see if there was a pattern in the made shots. We use geom_hex() and apply a log transformation to get a heat map.\n\nlakers %&gt;% \n  filter(etype==\"shot\" & result==\"made\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_hex(aes(x=x,y=y,fill = after_stat(log(count))),bins=30)+\n  scale_fill_viridis_c()+\n  labs(fill=\"Number of Made\\nShots: log(count)\")+\n  coord_fixed(ylim=c(0,NA))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Again lets save it as a global object\n\nZoomedInHex&lt;-lakers %&gt;% \n  filter(etype==\"shot\" & result==\"made\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_hex(aes(x=x,y=y,fill = after_stat(log(count))),bins=30)+\n  scale_fill_viridis_c()+\n  labs(fill=\"Number of Made\\nShots: log(count)\")+\n  coord_fixed(ylim=c(-2,NA))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nFinally lets sort out the layout we want to then combine these two saved plots.\n\nlayout3&lt;-c(\n  area(t=1,l=1,b=20,r=20),\n  area(t=2,l=3,b=8,r=9)\n)\n\nplot(layout3)\n\n\n\n\n\n\n\nBigPlot+ZoomedInHex+\n  plot_layout(design = layout3)+\n  plot_annotation(tag_levels = \"a\")\n\nWarning in geom_segment(data = . %&gt;% filter(result == \"Shot Made\"), aes(x = 25, : All aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_rect(data = . %&gt;% filter(result == \"Shot Made\"), aes(xmin = -2, : All aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThis is okay, still not great. We shall see if we can make a nice in the next tutorial.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Combining Multiple Plots in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/CombiningPlots.html#facetting",
    "href": "IntermediateRTutorials/CombiningPlots.html#facetting",
    "title": "Combining Multiple Plots in R",
    "section": "",
    "text": "For facetting we will normally be using at least one of the same axes across the plots. For example, we might look at the height to weight association across Starwars characters.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(\"starwars\")\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nLet’s assess height against mass of all the characters in this dplyr dataset.\n\nstarwars %&gt;%\n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass))+\n  theme_classic()\n\n\n\n\n\n\n\n\n\nHmmm I wonder who that heavy thing is? Perhaps we want to look at the different species as different colours?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=species))+\n  theme_classic()\n\n\n\n\n\n\n\n\nHmm this is not that easy to see, and even facetting may not be that great but lets see.\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=species))+\n  theme_classic()+\n  facet_wrap(~species)\n\n\n\n\n\n\n\n\nMaybe we could compare hair colour?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color)\n\n\n\n\n\n\n\n\nAlot of characters without hair, okay lets allow each facet (individual subplot) to have a different y scale.\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color,scales = \"free_y\")\n\n\n\n\n\n\n\n\nThat is better but we could also allow different scales for the x axis too?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_color))+\n  theme_classic()+\n  facet_wrap(~hair_color,scales = \"free\")\n\n\n\n\n\n\n\n\nWhat does this look like for eye colour?\n\nstarwars %&gt;% \n  drop_na() %&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=eye_color))+\n  theme_classic()+\n  facet_wrap(~eye_color,scales = \"free\")\n\n\n\n\n\n\n\n\nOkay so none of these plots are very nice as the starwars characters are very well spread in their physical characteristics. Maybe we can group some of these lesser filled groups into “Other”? Then we can do a facet grid with Eye (rows) and Hair (columns) Colours grouped.\n\nstarwars %&gt;% \n  mutate(eye_group=case_when(eye_color%in%c(\"black\",\"brown\",\"dark\",\"red\")~\"Dark Eyes\",\n                             eye_color%in%c(\"blue\",\"blue-gray\",\"gold\",\"green, yellow\",\"hazel\",\"orange\",\"pink\",\"red, blue\",\n                                            \"white\",\"yellow\")~\"Light Eyes\",\n                             TRUE~\"Other\"),\n         hair_group=case_when(hair_color%in%c(\"brown\",\"brown, grey\",\"black\")~\"Dark Hair\",\n                             hair_color%in%c(\"blond\",\"auburn, white\", \"auburn, grey\",\n                                             \"white\",\"grey\",\"auburn\",\"blonde\",\"unknown\")~\"Light Hair\",\n                             TRUE~\"Other\")) %&gt;% \n  drop_na()%&gt;% \n  ggplot()+\n  geom_point(aes(x=height,y=mass,colour=hair_group,shape=eye_group))+\n  theme_classic()+\n  facet_grid(eye_group~hair_group,scales = \"free\")\n\n\n\n\n\n\n\n\nNot amazingly illuminating but shows the use of facets. When using facet grid it automatically removes repeated axes.\nLets maybe use a slightly different data set, next will be some mpg data from ggplot2. This data is to do with car mile per gallon and different elements of the engine.\nWe can use facet_wrap() or facet_grid(). we have to put a dot after the ‘~’ if we are only facetting by one column. We will look at the type of drive, which is front wheel drive (f), 4x4 (4) or rear-wheel drive (r).\n\ndata(\"mpg\")\n\nmpg2 &lt;- mpg %&gt;% \n  filter(cyl != 5 & class != \"2seater\")\n\n\n mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy, colour=drv)) + \n  geom_point()+\n   facet_grid(~drv)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n\n\n\n\n\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n   facet_grid(drv~.)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n\n\n\n\n\n\n\nLets do some lines on all these points. For this we can use geom_smooth() that creates a loess model around our points. We can also define the number of columns or rows if we use facet_wrap rather than facet_grid().\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,nrow=2)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,ncol=1)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   facet_wrap(~drv,ncol=3)+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Combining Multiple Plots in R"
    ]
  },
  {
    "objectID": "IntermediateRTutorials/CombiningPlots.html#patchwork",
    "href": "IntermediateRTutorials/CombiningPlots.html#patchwork",
    "title": "Combining Multiple Plots in R",
    "section": "",
    "text": "Okay so maybe we want to look at a couple different associations in our data but without having related axes across the plots. To do this with patchwork we can save each plot as an object then print them. We can use + to add other objects to out ‘patchwork’ and build up layouts with () and /, or for more complex layouts we can use a few methods using the function plot_layout() from patchwork.\n\nlibrary(patchwork)\n\n\np1&lt;-mpg2 %&gt;% \n  ggplot(aes(x=cty, y=hwy,colour=drv)) + \n  geom_point()+\n  geom_smooth()+\n   labs(x=\"Number of Cylinders\",y=\"Highway Miles per Gallon\")+\n   theme_classic()\n\np2&lt;-mpg2 %&gt;% \n  ggplot(aes(x=as.factor(year),fill=class)) + \n  geom_bar(position = \"dodge2\")+\n   labs(x=\"Year\",y=\"Number of Models\")+\n   theme_classic()\n\n\n\n\np1+p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\np1/p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can also reuse plots as you like.\n\n(p1+p1+p2)/p2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo to make some fairly complicated plot layouts we can use brackets (), slashes / and pluses +. With a slash denoting a new line.\n\n(p1+p2+p2)/p2/(p2+p1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also use some helper functions to tidy our plots up. If for example we have repeated legends across our plots we can collect our legends or ‘guides’.\n\n(p1+p2+p2)/p2/(p2+p1)+plot_layout(guides=\"collect\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs the plots were made with ggplot2 we can also edit the theme of all of them together. To do this we use an ampersand & in our patchwork layout.\n\n(p1+p2+p2)/p2/(p2+p1)+\n  plot_layout(guides=\"collect\")& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use some more advanced layout options for if for example we don’t want to fill the whole grid space. We create a layout object that has a grid spacing, which we can check by plotting, then we apply that layout to a basic patchwork. We need to use the area function for all the plots we want. We will try plot the same lay out as just above, but without stretching plots that are on a row on their own. In the area() function from patchwork (be careful with other packages with the same name function - you can make sure it is correct by using patchwork::area() ) we have four arguments for the top (t), the left (l), the bottom (b) and the right (r). We can put any non-negative numbers in these to create any array of plots.\n\nlayout&lt;-c(\n  area(t=1,l=1,b=1,r=1),\n  area(t=1,l=2,b=1,r=2),\n  area(t=1,l=3,b=1,r=3),\n  area(t=2,l=2,b=2,r=2),\n  area(t=3,l=1,b=3,r=1),\n  area(t=3,l=3,b=3,r=3)\n)\n\nplot(layout)\n\n\n\n\n\n\n\n\nWe can now apply this layout with plot_layout() to a basic list of added up plots.\n\np1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout)& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs patchwork is happy with a ggplot2 object, we could even combine patchworks if we save one patchwork as a global object and added another ggplot to a new patchwork.\n\npatch&lt;-p1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout)& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n\np1/patch\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nUsing layout and its area function can allow us to create more and more complex arrangements, with and without overlaps. We can also add plot labels to help us refer to the plots in the legend. We can use plot_annotation() with tag_levels=“a” for a, b, c etc or “1” for numbers.\n\nlayout2&lt;-c(\n  area(t=1,l=1,b=5,r=5),\n  area(t=1,l=2,b=1,r=2),\n  area(t=1,l=3,b=1,r=3),\n  area(t=2,l=2,b=2,r=2),\n  area(t=3,l=1,b=3,r=1),\n  area(t=3,l=6,b=5,r=9)\n)\n\nplot(layout2)\n\n\n\n\n\n\n\np1+p2+p2+p2+p2+p1+\n  plot_layout(guides=\"collect\",design = layout2)+\n  plot_annotation(tag_levels = \"a\")& \n  theme(axis.title.x = element_text(size=20),\n        axis.title.y = element_text(size=6))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThese don’t look great now but the concept can be really useful for displaying lots of information and especially when making maps or plots where want to zoom in to a certain region to highlight some element of it. Lets get the lakers data from the lubridate package (it is already loaded in tidyverse). We will look at basketball shots on the court with their x and y cordinates, and whether they were missed or made. Let’s make our first plot and build it up slowly.\n\ndata(\"lakers\")\n\n## Default ggplot facet plot\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;% \nggplot()+\n  geom_point(aes(x=x,y=y),\n             alpha=0.4)+\n  facet_wrap(~result)\n\n\n\n\n\n\n\n## Lets fix coordinates and remove axis info\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;% \nggplot()+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  facet_wrap(~result)+\n  coord_fixed()+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Lets add a Hoop and Make the Legend a bit nicer plus different colours\n## We shall also add a square to show where our zoomed in plot will be.\n## To do this we select only the one shots made facet.\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"), ## We can use a dot to show that we are using the data already in the ggplot\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nWarning in geom_rect(data = . %&gt;% filter(result == \"Shot Made\"), aes(xmin = -2, : All aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n## we will also add an arrow into the plot to show where the new zoom plot will be.\n\nlakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(data=. %&gt;% filter(result==\"Shot Made\"), ## We use the same trick from above to only put arrow on one facet.\n               aes(x = 25, y = 45, xend = 25, yend = 50),\n                  arrow = arrow(length = unit(0.5, \"cm\")),\n               colour=\"darkorange\")+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"),\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nWarning in geom_segment(data = . %&gt;% filter(result == \"Shot Made\"), aes(x = 25, : All aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n## Okay Lets save this one to our global environment\n\nBigPlot&lt;-lakers %&gt;% \n  filter(etype==\"shot\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0,\n         result=case_when(result==\"made\"~\"Shot Made\",\n                          result==\"missed\"~\"Shot Missed\")) %&gt;% \nggplot()+\n  geom_segment(data=. %&gt;% filter(result==\"Shot Made\"), \n               aes(x = 25, y = 45, xend = 25, yend = 50),\n                  arrow = arrow(length = unit(0.5, \"cm\")),\n               colour=\"darkorange\")+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_point(aes(x=x,y=y,colour=as.factor(points),\n                 shape=as.factor(points)),\n             alpha=0.4)+\n  geom_rect(data=. %&gt;% filter(result==\"Shot Made\"),\n            aes(xmin=-2,xmax=max(x)+2,ymin=-10,ymax=45),colour=\"orange\",fill=NA)+\n  facet_wrap(~result)+\n  labs(shape=\"Points\",colour=\"Points\")+\n  coord_fixed(ylim=c(1,NA))+\n  scale_colour_manual(values=c(\"red\",\"darkcyan\",\"gold\"))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nSo we have our background plot showing all the data and information, now lets zoom in to the area where shots are being made. Lots of points are overlaid here so we could try look at the density of points spatially to see if there was a pattern in the made shots. We use geom_hex() and apply a log transformation to get a heat map.\n\nlakers %&gt;% \n  filter(etype==\"shot\" & result==\"made\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_hex(aes(x=x,y=y,fill = after_stat(log(count))),bins=30)+\n  scale_fill_viridis_c()+\n  labs(fill=\"Number of Made\\nShots: log(count)\")+\n  coord_fixed(ylim=c(0,NA))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n## Again lets save it as a global object\n\nZoomedInHex&lt;-lakers %&gt;% \n  filter(etype==\"shot\" & result==\"made\") %&gt;% \n  drop_na() %&gt;%\n  mutate(hoop_x=25,\n         hoop_y=0) %&gt;% \nggplot()+\n  geom_segment(aes(x=hoop_x,y=hoop_y,xend=hoop_x,yend=-10))+\n  geom_point(aes(x=hoop_x,y=hoop_y),\n             shape=21,size=6,colour=\"darkorange\",fill=\"white\")+\n  geom_hex(aes(x=x,y=y,fill = after_stat(log(count))),bins=30)+\n  scale_fill_viridis_c()+\n  labs(fill=\"Number of Made\\nShots: log(count)\")+\n  coord_fixed(ylim=c(-2,NA))+\n  theme_classic()+\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank())\n\nFinally lets sort out the layout we want to then combine these two saved plots.\n\nlayout3&lt;-c(\n  area(t=1,l=1,b=20,r=20),\n  area(t=2,l=3,b=8,r=9)\n)\n\nplot(layout3)\n\n\n\n\n\n\n\nBigPlot+ZoomedInHex+\n  plot_layout(design = layout3)+\n  plot_annotation(tag_levels = \"a\")\n\nWarning in geom_segment(data = . %&gt;% filter(result == \"Shot Made\"), aes(x = 25, : All aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_rect(data = . %&gt;% filter(result == \"Shot Made\"), aes(xmin = -2, : All aesthetics have length 1, but the data has 6009 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThis is okay, still not great. We shall see if we can make a nice in the next tutorial.",
    "crumbs": [
      "Home",
      "R Tutorials",
      "Combining Multiple Plots in R"
    ]
  },
  {
    "objectID": "Tutorials.html",
    "href": "Tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "All these tutorials have an Ecologist bias, mixed with my own opinions on best practices."
  },
  {
    "objectID": "Tutorials.html#r-basics",
    "href": "Tutorials.html#r-basics",
    "title": "Tutorials",
    "section": "R Basics:",
    "text": "R Basics:\n\n\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\n\nAn Introduction to R for Research Scientists: From Installation to Reading and Writing Data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Manipulation in R\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\n\nAn Introduction to Data Manipulation in R for Research Scientists: From Data Creation to Data Wrangling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualisation in R\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\n\nAn Introduction to Data Visualisation in R for Research Scientists: From Base Scatter Plots to Facetting GGPlots.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tutorials.html#r-intermediate",
    "href": "Tutorials.html#r-intermediate",
    "title": "Tutorials",
    "section": "R Intermediate:",
    "text": "R Intermediate:\n\n\n\n\n\n\n\n\n\n\nCombining Multiple Plots in R\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\npatchwork\n\n\n\nA Introduction to Combining Plots in R for Research Scientists: From Facetting to Patchworks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Static Maps in R\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nsf\n\n\nMapping\n\n\nGIS\n\n\n\nA Rapid Introduction to Mapping in R for Research Scientists: From Reading in Shape files and Rasters to plotting Shape files alongside Rasters in different Projections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Interactive Maps in R\n\n\n\n\n\n\nIntroduction\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nleaflet\n\n\nMapping\n\n\nGIS\n\n\n\nA Rapid Introduction to Interactive Mapping in R for Research Scientists: All/some things Leaflet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data in R\n\n\n\n\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nterra\n\n\nsf\n\n\nMapping\n\n\nGIS\n\n\n\nAn Introduction look at spatial analyses, plotting, manipulation and combination.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplex GIS in R\n\n\n\n\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nterra\n\n\nsf\n\n\nMapping\n\n\nGIS\n\n\n\nAn in-depth look at spatial dataset analyses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoops in R\n\n\n\n\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nforloop\n\n\nmap\n\n\npurrrr\n\n\n\nAn Introduction look at loops/ifelse/apply/mapping.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tutorials.html#r-advanced",
    "href": "Tutorials.html#r-advanced",
    "title": "Tutorials",
    "section": "R Advanced:",
    "text": "R Advanced:\n\n\n\n\n\n\n\n\n\n\nMaking Maps from Satellite Imagery\n\n\n\n\n\n\nTidyverse\n\n\nggplot2\n\n\nR\n\n\nterra\n\n\nsf\n\n\nMapping\n\n\nGIS\n\n\n\nIntroduction to using Satellite Imagery for Mapping.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tutorials.html#general-linear-models-glms",
    "href": "Tutorials.html#general-linear-models-glms",
    "title": "Tutorials",
    "section": "General Linear Models (GLMs):",
    "text": "General Linear Models (GLMs):\n\n\n\n\n\n\n\n\n\n\nIntroduction GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\n\nAn Introduction to General Linear Models (GLMs) in R.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nGaussian\n\n\n\nExamples of Gaussian GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nPoisson\n\n\n\nExamples of Poisson GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nBinomial\n\n\n\nExamples of Binomial GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGamma GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nGamma\n\n\n\nExamples of Gamma GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeta GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nBeta\n\n\n\nExamples of Beta GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZero Inflated GLMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMs\n\n\nStats\n\n\nZero Inflation\n\n\n\nExamples of Zero Inflated GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommon GLM Problems\n\n\n\n\n\n\nProblems\n\n\nR\n\n\nGLMs\n\n\n\nUnderstanding and Solving Common Problems of General Linear Models (GLMs) in R.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tutorials.html#general-linear-mixed-effect-models-glmms",
    "href": "Tutorials.html#general-linear-mixed-effect-models-glmms",
    "title": "Tutorials",
    "section": "General Linear Mixed Effect Models (GLMMs):",
    "text": "General Linear Mixed Effect Models (GLMMs):\n\n\n\n\n\n\n\n\n\n\nIntroduction to GLMMs\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGLMMs\n\n\n\nAn Introduction to General Linear Mixed Effect Models (GLMMs) in R.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tutorials.html#machine-learning",
    "href": "Tutorials.html#machine-learning",
    "title": "Tutorials",
    "section": "Machine Learning:",
    "text": "Machine Learning:\n\n\n\n\n\n\n\n\n\n\nIntroduction to Machine Learning\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nMachine Learning\n\n\nStats\n\n\nRandom Forest\n\n\n\nBrief Introduction to Machine Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing and Ensembling Machine Learning Models\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nMachine Learning\n\n\nStats\n\n\nRandom Forest\n\n\nXGBoost\n\n\nSVM\n\n\nKNN\n\n\n\nBuilding different Models and Intercomparing, and then Combining into an Ensemble\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Machine Learning with Satellite Imagery\n\n\n\n\n\n\nIntroduction\n\n\nR\n\n\nGIS\n\n\nMachine Learning\n\n\nRemote Sensing\n\n\nMapping\n\n\nStats\n\n\nRandom Forest\n\n\n\nBuilding a Prediction Model for Satellite Imagery\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts1/Acoustics.html",
    "href": "posts1/Acoustics.html",
    "title": "Acoustic Complexity Index to Assess Benthic Biodiversity of a Partially Protected Area in the Southwest of the UK",
    "section": "",
    "text": "Link here: Davies et al., 2020\nThe soundscape of the marine environment is a relatively understudied area of ecology that has the potential to provide large amounts of information on biodiversity, reproductive behaviour, habitat selection, spawning and predator–prey interactions. Biodiversity is often visually assessed and used as a proxy for ecosystem health. Visual assessment using divers or remote video methods can be expensive, and limited to times of good weather and water visibility. Previous studies have concluded that acoustic measures, such as the Acoustic Complexity Index (ACI), correlate with visual biodiversity estimates and offer an alternative to assess ecosystem health. Here, the ACI measured over 5 years in a Marine Protected Area (MPA) in the UK, Lyme Bay, was analysed alongside another monitoring method, Baited Remote Underwater Video Systems (BRUVs). Two treatments were sampled annually in the summer from 2014 until 2018 with sites inside the MPA, as well as Open Control sites outside of the MPA. Year by year correlations, which have been used elsewhere to test ACI, showed significant correlations with Number of Species and ACI. However, the sign of these correlations changed almost yearly, showing that more in-depth analyses are needed. Multivariate analysis of the benthic assemblage composition (from BRUVs) was carried out by Permutational Multivariate Analysis of Variance (PERMANOVA) using Distance Matrices. Although not consistently correlating with univariate measures, the ACI was significantly interacting with the changing benthic assemblage composition, as it changed over time and protection (Inside vs Outside the MPA). ACI showed potential to allude to shifting benthic communities, yet with no consistency when used alongside univariate measures of diversity. Although it is not without its own disadvantages, and thus should be developed further before implementation, the ACI could potentially reflect more complex changes to the benthos than simply the overall diversity."
  },
  {
    "objectID": "posts1/SAC.html",
    "href": "posts1/SAC.html",
    "title": "Ecosystem benefits of adopting a whole‐site approach to MPA management.",
    "section": "",
    "text": "Link here: Davies et al., 2022\n\nAbstract\nGlobally, nations are designating Marine Protected Areas to recover and protect habitats and species. With targets to protect 30% of marine areas by 2030, effectiveness of MPAs to protect designated space is important. In Lyme Bay (South West UK) two co-located MPAs have each adopted different management styles to exclude mobile demersal fishing; a Special Area of Conservation (SAC) protecting the known extent of sensitive reef habitat and an area including a mosaic of reef and sedimentary habitats where the whole-site is protected from mobile demersal fishing under a Statutory Instrument (SI). Underwater videography, both towed (individuals m\\(^{-2}\\)) and baited (MaxN), was used to enumerate change over time of reef species (Number of Taxa, Total Abundance, Functional Richness and Functional Redundancy) in the MPAs and nearby control areas (2008-2019). Total abundance and functional redundancy of sessile taxa and functional richness of mobile taxa increased, while the number of sessile or mobile taxa, functional richness of sessile taxa, total abundance of mobile taxa or functional redundancy of mobile taxa did not differ from nearby control sites. Over time, both management styles did result in increases in sessile and sedentary taxa diversity relative to open controls, with increases in total abundance of 15% and 95% in the ‘feature based’ and whole-site MPAs respectively alongside increases in the number of sessile taxa of 44% over time in the ‘feature based’ MPA. However, the mobile taxa in the whole-site MPA showed levels of functional redundancy 7% higher than the ‘feature based’ MPA, indicative of a higher community resilience inside the whole-site MPA to perturbations, such as storms or biological invasions. Increases seen in the diversity of sessile taxa we expected only in areas where mobile demersal fishing was excluded (~46.8% of its’ areas). Therefore, if the whole ‘feature based’ MPA was consistently protected, we expected to see similar levels of increase in functional extent of reef. While the ‘feature based’ MPA showed similar results over time to that of the ‘whole site’, the ‘whole site’ showed higher levels of diversity, both taxonomical and functional."
  },
  {
    "objectID": "posts1/JerseyCarbon.html",
    "href": "posts1/JerseyCarbon.html",
    "title": "Carbon stocks in marine habitats across Jersey’s territorial waters.",
    "section": "",
    "text": "Link here: Davies et al., 2024\n\nAbstract\nMarine areas have been shown to store varying amounts of carbon. The densities of these stores varies considerably across environmental settings, including the biotic and physical habitat type, the productivity of these habitats, the ephemeral or permanent nature of the habitats as well as the hydrological energy of the area, its depth and its exposure to anthropogenic activities. To appropriately incorporate carbon storage into management plans, baseline information on the carbon held within marine areas is required. Here, an inventory of carbon was carried out for marine habitats found within the territorial waters of Jersey, one of the Channel Islands situated between England and France. Across the habitats surveyed, the biogenic habitats Seagrass and Sand Mason Worm Dominated Sands contained high densities of Organic Carbon with 0.25 ± 0.047 (Seagrass) and 0.17 ± 0.048 Mg Ha-1 (Sand Mason Worms) respectively, but due to their small spatial area this only accounted for 110 ± 0.211 (Seagrass) and 359 ± 1.02 Tonnes (Sand Mason Worms) across Jersey’s entire territorial waters (cumulatively 1.11 % by area). Less carbon dense habitats, such as Coarse Sediments with 0.12 ± 0.031 Mg Ha-1 covered much larger areas and accounted for a much larger proportion of organic carbon stocks (8450 ± 21.9 Tonnes; 30.9 % of Jersey’s Territorial waters by area). As nations aim to quantify their biosphere carbon stocks to best inform future management, the need for extensive knowledge of the quantities of carbon found within and across marine habitats is highly important. This work provides a baseline assessment of carbon found within marine habitats in Jersey’s territorial waters. The work highlights the importance of not only understanding the density of carbon within specific habitats but also the necessity of accurate spatial information characterising the extents of these habitats."
  },
  {
    "objectID": "posts1/PhenologyICECREAMS.html",
    "href": "posts1/PhenologyICECREAMS.html",
    "title": "A Sentinel Watching over Intertidal Seagrass Phenology across Western Europe and North Africa.",
    "section": "",
    "text": "Link here: Davies et al., 2024\n\nAbstract\nSeagrasses are marine flowering plants that form extensive meadows from the inter-tidal zone up to ~50 m depth. As biological and ecological Essential Biodiversity Variables, seagrass cover and composition provide a wide range of ecosystem services. Inter-tidal seagrass meadows provide services to many ecosystems, so monitoring their occurrence, extent, condition and diversity can be used to indicate the biodiversity and health of local ecosystems. Current global estimates of seagrass extent and recent reviews either do not mention inter-tidal seagrasses and their seasonal variation, or combine them with sub-tidal seagrasses. Here, using high-spatial and high-temporal resolution satellite data (Sentinel-2), we demonstrate a method for consistently mapping inter-tidal seagrass meadows and their phenology at a continental scale. We were able to highlight varying seasonal patterns that are observable across a 23° latitudinal range. Timings of peaks in seagrass extent varied by up to 5 months, rather than the previously assumed marginal to non-existent variation in peak timing. These results will aid management by providing high-resolution spatio-temporal monitoring data to better inform seagrass conservation and restoration. They also highlight the high level of seasonal variability in inter-tidal seagrass, meaning combination with sub-tidal seagrass for global assessments will likely produce misleading or incorrect estimates."
  },
  {
    "objectID": "consultancy_Mobile.html",
    "href": "consultancy_Mobile.html",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\n\n\n\n\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects."
  },
  {
    "objectID": "consultancy_Mobile.html#statistical-experience",
    "href": "consultancy_Mobile.html#statistical-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "",
    "text": "My statistical experiences have ranged from:\n\nmodelling decadal oceanographic changes over ocean basin scales to predict marine invasive species\nassessing changes in acoustic complexity index (a univariate measure of ecosystem noise), to assemblage level changes (multivariate abundance) to predictive spatial modelling of macroalgal species from satelite imagery\nabundance and diversity of taxa of importance, to whole ecosystem functional trait analysis.\n\n\n\n\n\n\n\n\n\n\nThe methods used have mostly been applied within the coding language {r}, with models being applied of varying complexity, in both frequentist and bayesian frameworks as well as Machine Learning algorithms, from one or two fixed effects to more complex mixed effects, hierarchical regression, General Additive, Random Forest, XGBoost, Neural Network classification models. These models, like the data, have been varying in distributions and characteristics with models using:\n\ngaussian distributions\npoisson distributions\nbeta distributions\ngamma distributions\nbernoulli distributions\nbinomial distributions\nzero-inflated/one-inflated/zero-altered/one-altered alternatives.\n\n\n\n\n\n\n\n\n\n\nThese analysis methods have been used to provide visiualisations of predicted scenarios across many of the projects."
  },
  {
    "objectID": "consultancy_Mobile.html#consultancy-fieldwork-experience",
    "href": "consultancy_Mobile.html#consultancy-fieldwork-experience",
    "title": "Ecological and Statistical Consultancy",
    "section": "Consultancy Fieldwork Experience",
    "text": "Consultancy Fieldwork Experience\nI have assisted in Fisheries Assessment cruises as a Benthic Taxonomist across the Greenlandic and Icelandic Seas.\nDuring this cruise on the RV Tarajoq my main duties were to:\n\nIdentify\nQuantify\nRecord\n\nAll Benthic taxa caught as bycatch during fisheries assessments of Alantic Cod and Shrimp."
  }
]